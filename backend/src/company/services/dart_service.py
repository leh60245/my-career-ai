import contextlib
import logging
import re
import time
from datetime import datetime, timedelta
from io import StringIO
from typing import Any

import dart_fss as dart
import pandas as pd
from bs4 import BeautifulSoup
from src.common.config import CHUNK_CONFIG, DART_CONFIG, TARGET_SECTIONS


logger = logging.getLogger(__name__)


class DartService:
    """
    DART Ï†ÑÏûêÍ≥µÏãú ÏãúÏä§ÌÖú Ïó∞Îèô ÏÑúÎπÑÏä§
    Ïó≠Ìï†: 1. API ÌÜµÏã†, 2. HTML Îã§Ïö¥Î°úÎìú, 3. ÌååÏã± (HTML -> Structured Dict)
    """

    def __init__(self):
        self.api_key = DART_CONFIG.get("api_key")
        if not self.api_key:
            logger.warning("‚ö†Ô∏è DART_API_KEY is missing.")
        else:
            dart.set_api_key(api_key=self.api_key)

        self._corp_list = None

    # ==================== 1. Optimized Core Data Access ====================

    @property
    def corp_list(self):
        if self._corp_list is None:
            logger.info("üîÑ Loading DART Corp List (Heavy Operation)...")
            try:
                self._corp_list = dart.get_corp_list()
                logger.info(f"‚úÖ Loaded {len(self._corp_list)} corporations.")
            except Exception as e:
                logger.error(f"Failed to load corp list: {e}")
                self._corp_list = None
        return self._corp_list

    def get_corp_by_code(self, corp_code: str) -> Any | None:
        """
        [O(1) Search] Í≥†Ïú†Î≤àÌò∏(corp_code)Î°ú Corp Í∞ùÏ≤¥ Ï∞æÍ∏∞
        """
        if not self.corp_list:
            return None
        return self.corp_list.find_by_corp_code(corp_code)

    def get_corp_by_stock_code(self, stock_code: str) -> Any | None:
        """
        [O(1) Search] Ï¢ÖÎ™©ÏΩîÎìú(stock_code)Î°ú Corp Í∞ùÏ≤¥ Ï∞æÍ∏∞
        ÎùºÏù¥Î∏åÎü¨Î¶¨ ÎÇ¥Î∂Ä Ìï¥ÏãúÎßµ(_stock_codes) ÌôúÏö©
        """
        if not self.corp_list:
            return None
        return self.corp_list.find_by_stock_code(stock_code)

    # ==================== 2. API Fetch Logic ====================

    def search_all_reports(self, bgn_de: str | None = None, end_de: str | None = None) -> list[Any]:
        """
        Í∏∞Í∞Ñ ÎÇ¥ Ï†úÏ∂úÎêú Î™®Îì† ÏÇ¨ÏóÖÎ≥¥Í≥†ÏÑúÎ•º Í≤ÄÏÉâ (Efficient Mode)
        """
        if not end_de:
            end_de = datetime.now().strftime("%Y%m%d")

        # 3Í∞úÏõî Ï†úÌïú Î°úÏßÅ (Safety Clamp)
        if not bgn_de:
            bgn_dt = datetime.strptime(end_de, "%Y%m%d") - timedelta(days=90)
            bgn_de = bgn_dt.strftime("%Y%m%d")

        logger.info(f"üîç Searching all reports: {bgn_de} ~ {end_de}")

        all_reports = []
        page_no = 1

        while True:
            try:
                report_type = DART_CONFIG.get("report_type_code", "a001")
                if isinstance(report_type, str):
                    report_type = [report_type]

                # dart.search Î™®Îìà Ìï®Ïàò ÏÇ¨Ïö© (Ï†ÑÏ≤¥ Í≤ÄÏÉâÏö©)
                res = dart.search(
                    bgn_de=bgn_de,
                    end_de=end_de,
                    pblntf_detail_ty=report_type,
                    last_reprt_at="Y",  # [ÌïÑÏàò] ÏµúÏ¢ÖÎ≥∏Îßå
                    page_no=page_no,
                    page_count=100,
                )

                # SearchResults Í∞ùÏ≤¥Ïùò Î¶¨Ïä§Ìä∏ Ï∂îÏ∂ú
                current_list = getattr(res, "report_list", []) if hasattr(res, "report_list") else res
                if not current_list:
                    break

                all_reports.extend(current_list)

                total_page = getattr(res, "total_page", 1)
                if page_no >= total_page:
                    break
                page_no += 1
                time.sleep(0.5)  # Rate Limit Ï§ÄÏàò

            except Exception as e:
                logger.error(f"Search failed at page {page_no}: {e}")
                break

        return all_reports

    def get_corps_with_reports(self, bgn_de: str | None = None) -> list[Any]:
        """
        ÏµúÍ∑º Î≥¥Í≥†ÏÑúÍ∞Ä ÏûàÎäî Í∏∞ÏóÖÏùò 'Corp Í∞ùÏ≤¥' Î¶¨Ïä§Ìä∏ Î∞òÌôò
        """
        all_reports = self.search_all_reports(bgn_de=bgn_de)
        if not all_reports:
            return []

        unique_codes = {r.corp_code for r in all_reports if hasattr(r, "corp_code")}

        targets = []
        for code in unique_codes:
            corp = self.get_corp_by_code(code)
            if corp:
                targets.append(corp)

        return targets

    def get_annual_report(self, corp_code: str, days: int = 365) -> Any:
        """ÌäπÏ†ï Í∏∞ÏóÖÏùò ÏµúÏã† ÏÇ¨ÏóÖÎ≥¥Í≥†ÏÑú 1Í±¥ Ï°∞Ìöå"""
        try:
            end_de = datetime.now().strftime("%Y%m%d")
            start_dt = datetime.now() - timedelta(days=days)
            bgn_de = start_dt.strftime("%Y%m%d")

            report_type = DART_CONFIG.get("report_type_code", "a001")
            if isinstance(report_type, str):
                report_type = [report_type]

            search_results = dart.search(
                corp_code=corp_code,
                bgn_de=bgn_de,
                end_de=end_de,
                pblntf_detail_ty=report_type,
                last_reprt_at="Y",
            )
            return search_results[0] if search_results else None

        except Exception as e:
            logger.error(f"Failed to search report for {corp_code}: {e}")
            return None

    def extract_company_info(self, corp_obj) -> dict:
        """
        Í∏∞ÏóÖ Í∏∞Î≥∏ Ï†ïÎ≥¥ Ï∂îÏ∂ú (DB Ï†ÄÏû•Ïö©)
        """
        return {
            "corp_code": getattr(corp_obj, "corp_code", None),
            "company_name": getattr(corp_obj, "corp_name", None),
            "stock_code": getattr(corp_obj, "stock_code", None),
            # dart-fss Corp Í∞ùÏ≤¥Îäî sector, product ÏÜçÏÑ±ÏùÑ Í∞ÄÏßê
            "sector": getattr(corp_obj, "sector", None),
            "product": getattr(corp_obj, "product", None),
            # industry_codeÎäî Î≥¥ÌÜµ corp_info ÏÉÅÏÑ∏ Ï°∞Ìöå(API Call)Î•º Ìï¥Ïïº ÎÇòÏò§ÎØÄÎ°ú
            # Î¶¨Ïä§Ìä∏ Ï°∞Ìöå Îã®Í≥ÑÏóêÏÑúÎäî NoneÏúºÎ°ú Îë† (ÎÇòÏ§ëÏóê ÌïÑÏöîÌïòÎ©¥ Ï±ÑÏõÄ)
            "industry_code": None,
        }

    def extract_report_metadata(self, report, corp_obj) -> dict[str, Any]:
        """
        Í∏∞ÏóÖ Î≥¥Í≥†ÏÑú Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú (DB Ï†ÄÏû•Ïö©)
        """
        return {
            "title": getattr(report, "report_nm", "No Title"),
            "rcept_no": getattr(report, "rcept_no", None),
            "rcept_dt": getattr(report, "rcept_dt", None),
            # Corp Í∞ùÏ≤¥ Í∏∞Ï§Ä Ï†ïÎ≥¥ÎèÑ ÏùºÎ∂Ä Ìè¨Ìï®
            "corp_code": getattr(corp_obj, "corp_code", None),
            "corp_name": getattr(corp_obj, "corp_name", None),
            "stock_code": getattr(corp_obj, "stock_code", None),
            "report_type": "annual",
        }

    # ==================== 3. Parsing Logic (HTML -> Chunks) ====================

    def parse_report_sections(self, report) -> list[dict[str, Any]]:
        """HTML ÌååÏã± Î©îÏù∏ Î°úÏßÅ"""
        all_raw_chunks = []
        global_sequence = 0

        if not report:
            return []

        rcept_no = getattr(report, "rcept_no", getattr(report, "rcp_no", "Unknown"))
        logger.info(f"   üìÇ Parsing Report: {getattr(report, 'report_nm', 'No Title')} ({rcept_no})")

        # ÏÑπÏÖòÎ≥Ñ ÏàúÌöå
        # DART API ÌäπÏÑ±ÏÉÅ 'Ï≤®Î∂Ä' Î¨∏ÏÑúÏóêÏÑú Î≥∏Î¨∏ÏùÑ Ï∞æÏïÑÏïº Ìï† ÏàòÎèÑ ÏûàÏùå
        # dart-fssÎäî extract_text()ÎÇò pages ÏÜçÏÑ±ÏùÑ Ï†úÍ≥µÌï®

        try:
            # pagesÍ∞Ä Î°úÎìúÎêòÏßÄ ÏïäÏïòÎã§Î©¥ Î°úÎìú ÏãúÎèÑ
            if not hasattr(report, "pages") or not report.pages:
                with contextlib.suppress(BaseException):
                    report.extract_pages()

            for section_name in TARGET_SECTIONS:
                # ÏÑπÏÖò Ïù¥Î¶ÑÏúºÎ°ú ÌéòÏù¥ÏßÄ Ï∞æÍ∏∞ (Ïòà: "ÏÇ¨ÏóÖÏùò ÎÇ¥Ïö©", "II. ÏÇ¨ÏóÖÏùò ÎÇ¥Ïö©" Îì±)
                found_pages = []

                # 1. Exact Match ÏãúÎèÑ
                if hasattr(report, "pages"):
                    for page in report.pages:
                        if section_name in page.title:
                            found_pages.append(page)

                # 2. sub_docs Í≤ÄÏÉâ ÏãúÎèÑ (legacy method)
                if not found_pages and hasattr(report, "sub_docs"):
                    for title, url in report.sub_docs.items():
                        if section_name in title:
                            # Ïù¥ Í≤ΩÏö∞ Î≥ÑÎèÑ Ï≤òÎ¶¨Í∞Ä ÌïÑÏöîÌïòÏßÄÎßå dart-fss ÏµúÏã† Î≤ÑÏ†ÑÏùÄ pagesÎ°ú ÌÜµÌï©Îê®
                            pass

                if not found_pages:
                    continue

                logger.info(f"   üìñ Found Section '{section_name}' ({len(found_pages)} pages)")

                for page in found_pages:
                    html_content = page.html
                    if not html_content:
                        continue

                    soup = BeautifulSoup(html_content, "html.parser")
                    # ÏÑπÏÖò Ìó§Îçî Îì± Î∂àÌïÑÏöîÌïú ÌÉúÍ∑∏ Ï†úÍ±∞ Î°úÏßÅ Ï∂îÍ∞Ä Í∞ÄÎä•

                    chunks = self._parse_html_to_chunks(soup, section_name, global_sequence)
                    if chunks:
                        global_sequence += len(chunks)
                        all_raw_chunks.extend(chunks)

        except Exception as e:
            logger.error(f"Parsing Error: {e}")

        return all_raw_chunks

    def _parse_html_to_chunks(self, soup, section_path: str, start_seq: int) -> list[dict[str, Any]]:
        """HTML DOM ÏàúÌöå Î∞è Ï≤≠ÌÅ¨ ÏÉùÏÑ±"""
        blocks = []
        current_seq = start_seq
        text_buffer = []

        def flush_buffer():
            nonlocal current_seq
            if not text_buffer:
                return

            # [1] _clean_text ÏÇ¨Ïö© (ÌÖçÏä§Ìä∏ Ï†ïÏ†ú)
            raw_text = "\n".join(text_buffer)
            clean_content = self._clean_text(raw_text)  # <-- Ìò∏Ï∂ú

            if not clean_content:
                text_buffer.clear()
                return

            # [2] _chunk_text ÏÇ¨Ïö© (ÌÖçÏä§Ìä∏ Î∂ÑÌï†)
            # ÌÖçÏä§Ìä∏Í∞Ä Í∏∏Î©¥ Ïó¨Îü¨ Í∞úÏùò Ï≤≠ÌÅ¨Î°ú Ï™ºÍ∞úÏßê
            chunks = self._chunk_text(clean_content)  # <-- Ìò∏Ï∂ú

            for chunk in chunks:
                blocks.append(
                    {
                        "chunk_type": "text",
                        "raw_content": chunk,
                        "section_path": section_path,
                        "sequence_order": current_seq,
                        "meta_info": {},
                    }
                )
                current_seq += 1

            text_buffer.clear()

        # ... (DOM ÏàúÌöå Î°úÏßÅ) ...
        for elem in soup.recursiveChildGenerator():
            if isinstance(elem, str):
                text_content = elem.strip()
                if text_content:
                    text_buffer.append(text_content)

            elif elem.name == "table":
                flush_buffer()

                md, meta = self._table_to_markdown(elem)
                if md:
                    blocks.append(
                        {
                            "chunk_type": "table",
                            "raw_content": md,
                            "section_path": section_path,
                            "sequence_order": current_seq,
                            "table_metadata": meta,
                            "meta_info": {},
                        }
                    )
                    current_seq += 1
            elif elem.name in ["br", "p", "div", "li", "tr"]:
                text_buffer.append("\n")

        flush_buffer()
        return blocks

    # ==================== 4. Helper Methods ====================

    def _clean_text(self, text: str) -> str:
        text = re.sub(r"\n{3,}", "\n\n", text)
        text = re.sub(r" {2,}", " ", text)
        return text.replace("\xa0", " ").replace("\r", "").strip()

    def _chunk_text(self, text: str) -> list[str]:
        chunk_size = CHUNK_CONFIG.get("max_chunk_size", 1000)
        overlap = CHUNK_CONFIG.get("overlap", 100)
        min_size = CHUNK_CONFIG.get("min_chunk_size", 50)

        if len(text) <= chunk_size:
            return [text] if len(text) >= min_size else []

        chunks = []
        start = 0
        while start < len(text):
            end = start + chunk_size
            if end < len(text):
                for sep in ["\n\n", "\n", ". "]:
                    last_sep = text[start:end].rfind(sep)
                    if last_sep > chunk_size // 2:
                        end = start + last_sep + len(sep)
                        break

            chunk = text[start:end].strip()
            if len(chunk) >= min_size:
                chunks.append(chunk)

            start = end - overlap
        return chunks

    def _table_to_markdown(self, table_element) -> tuple[str, dict]:
        try:
            dfs = pd.read_html(StringIO(str(table_element)), flavor="bs4")
            if not dfs:
                return "", {}
            df = dfs[0].fillna("")

            meta = {
                "rows": len(df),
                "cols": len(df.columns),
                "columns": [str(c) for c in df.columns],
            }
            caption = table_element.find("caption")
            if caption:
                meta["title"] = caption.get_text(strip=True)

            return df.to_markdown(index=False), meta
        except Exception as e:
            text = table_element.get_text(separator=" ", strip=True)
            return f"[Ìëú Îç∞Ïù¥ÌÑ∞]\n{text}", {"error": str(e)}
