import logging
import re
from collections.abc import Sequence
from typing import Any

from src.common.services.embedding import Embedding
from src.company.models.source_material import SourceMaterial
from src.company.repositories.source_material_repository import SourceMaterialRepository


logger = logging.getLogger(__name__)


class IngestionService:
    """
    ë°ì´í„° ì ì¬ ë° ì „ì²˜ë¦¬ ì„œë¹„ìŠ¤ (Shift-Left Strategy ì ìš©)

    ì—­í• :
    1. Raw Chunks ì „ì²˜ë¦¬ (ë…¸ì´ì¦ˆ ë³‘í•©, ê³ ì•„ ë…¸ì´ì¦ˆ ì œê±°)
    2. Context-Aware ì„ë² ë”© ìƒì„± (Text -> Table ë¬¸ë§¥ ì£¼ì…)
    3. DB Bulk Insert
    """

    # ë…¸ì´ì¦ˆ í…Œì´ë¸” íŒë³„ì„ ìœ„í•œ í‚¤ì›Œë“œ
    NOISE_KEYWORDS = [
        "ë‹¨ìœ„",
        "Unit",
        "ë²”ë¡€",
        "ì°¸ì¡°",
        "â€»",
        "ì£¼)",
        "(ì£¼)",
        "ì›",
        "ì²œì›",
        "ë°±ë§Œì›",
        "ì–µì›",
        "ì£¼1)",
        "ì£¼2)",
        "(ë‹¨ìœ„",
    ]
    NOISE_TABLE_MAX_ROWS = 2

    def __init__(self, source_repo: SourceMaterialRepository, embedding: Embedding):
        self.source_repo = source_repo
        self.embedding = embedding

    async def save_chunks(self, analysis_report_id: int, chunks: list[dict[str, Any]]) -> Sequence[SourceMaterial]:
        """
        [Main Pipeline] ì „ì²˜ë¦¬ -> ì„ë² ë”© -> ì €ì¥

        Note: Idempotency(ë©±ë“±ì„±)ë¥¼ ë³´ì¥í•˜ê¸° ìœ„í•´, ì €ì¥ ì „ í•´ë‹¹ ë¦¬í¬íŠ¸ì˜ ê¸°ì¡´ ì²­í¬ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.
        """
        if not chunks:
            return []

        logger.info(f"   âš™ï¸ Processing {len(chunks)} chunks for Report ID {analysis_report_id}...")

        # 1. [Clean Slate] ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ì¤‘ë³µ ë°©ì§€)
        await self.delete_report_chunks(analysis_report_id)

        # 2. [ì „ì²˜ë¦¬] ë…¸ì´ì¦ˆ ë³‘í•© ë° ì •ì œ (Shift Left)
        clean_chunks = self._preprocess_and_merge(chunks)

        logger.debug(f"      Noise filtering: {len(chunks)} -> {len(clean_chunks)} chunks")

        # 3. [ì„ë² ë”©] ë¬¸ë§¥ ì£¼ì… (Context Injection) ë° ë²¡í„° ìƒì„±
        await self._generate_embeddings(clean_chunks)

        # 4. [ì €ì¥] DB Bulk Insert
        # Repositoryê°€ ID ì£¼ì…ì„ ë‹´ë‹¹í•˜ë¯€ë¡œ IDì™€ ì²­í¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ë„˜ê¹€
        return await self.source_repo.create_bulk(analysis_report_id, clean_chunks)

    async def delete_report_chunks(self, analysis_report_id: int) -> None:
        """
        íŠ¹ì • ë¦¬í¬íŠ¸ì˜ ëª¨ë“  ì²­í¬ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤. (ì¬ì ì¬ ì „ ì´ˆê¸°í™”)
        """
        count = await self.source_repo.delete_by_analysis_report_id(analysis_report_id)
        if count > 0:
            logger.info(f"   ğŸ—‘ï¸ Deleted {count} old chunks for Report ID {analysis_report_id}")

    # =========================================================================
    #  Internal Logic (Preprocessing & Embedding)
    # =========================================================================

    def _is_noise_table(self, content: str) -> bool:
        """í‘œê°€ ë‹¨ìˆœ ë‹¨ìœ„/ë²”ë¡€ í‘œ(Noise)ì¸ì§€ íŒë³„"""
        if not content:
            return False

        lines = content.strip().split("\n")
        # íŒŒì´í”„(|)ë¡œ ì‹œì‘í•˜ëŠ” ë¼ì¸ ì¤‘ êµ¬ë¶„ì„ ì´ ì•„ë‹Œ ë°ì´í„° í–‰ ì¹´ìš´íŠ¸
        data_rows = [line for line in lines if "|" in line and not re.match(r"^\|[\s\-:]+\|$", line.strip())]

        if len(data_rows) <= self.NOISE_TABLE_MAX_ROWS:
            for k in self.NOISE_KEYWORDS:
                if k in content:
                    return True
        return False

    def _preprocess_and_merge(self, chunks: list[dict[str, Any]]) -> list[dict[str, Any]]:
        """
        [í•µì‹¬ ë¡œì§] Forward Merge Strategy
        ë…¸ì´ì¦ˆ(ë‹¨ìœ„ í‘œ)ë¥¼ ë°œê²¬í•˜ë©´ ë‹¤ìŒ í‘œì˜ í—¤ë”ë¡œ ë³‘í•©í•˜ê³ , ê³ ì•„ ë…¸ì´ì¦ˆëŠ” ì œê±°í•©ë‹ˆë‹¤.
        """
        n = len(chunks)
        merge_flags = [False] * n

        for i in range(n):
            if merge_flags[i]:
                continue

            curr = chunks[i]
            curr_type = curr.get("chunk_type", "text")
            curr_content = curr.get("raw_content", "")

            # [Noise Check]
            if curr_type == "table" and self._is_noise_table(curr_content):
                # Forward Lookahead
                if i + 1 < n and chunks[i + 1].get("chunk_type") == "table":
                    next_chunk = chunks[i + 1]

                    # [Merge] ë‹¨ìœ„ ì •ë³´ë¥¼ ë‹¤ìŒ í‘œì˜ ìƒë‹¨ì— ë¶™ì„
                    next_chunk["raw_content"] = f"{curr_content}\n\n{next_chunk['raw_content']}"

                    # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
                    meta = next_chunk.get("meta_info", {}) if next_chunk.get("meta_info") else {}
                    meta["has_merged_meta"] = True
                    next_chunk["meta_info"] = meta

                    # í˜„ì¬ ì²­í¬ ì‚­ì œ í‘œì‹œ
                    merge_flags[i] = True
                else:
                    # [Drop] ê³ ì•„ ë…¸ì´ì¦ˆ
                    merge_flags[i] = True

        valid_chunks = [chunks[i] for i in range(n) if not merge_flags[i]]
        return valid_chunks

    async def _generate_embeddings(self, chunks: list[dict[str, Any]]) -> None:
        """
        ì²­í¬ ë¦¬ìŠ¤íŠ¸ì— ëŒ€í•´ ì„ë² ë”©ì„ ìƒì„±í•˜ì—¬ ì£¼ì…í•©ë‹ˆë‹¤.
        * ìµœì í™”: í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš°ë§Œ API í˜¸ì¶œ
        * ë¬¸ë§¥ ì£¼ì…: Tableì€ ì§ì „ Textì˜ ë‚´ìš©ì„ ì„ë² ë”© í”„ë¡¬í”„íŠ¸ì— í¬í•¨
        """
        texts_to_embed = []
        indices_to_embed = []

        for i, chunk in enumerate(chunks):
            raw_content = chunk.get("raw_content", "")
            if not raw_content.strip():
                continue

            # [Context Injection Logic]
            embedding_text = raw_content
            context_injected = False

            # í˜„ì¬ê°€ Tableì´ê³ , ì§ì „ì´ Textì´ë©°, ê°™ì€ ì„¹ì…˜ì¸ ê²½ìš° -> ë¬¸ë§¥ ì£¼ì…
            if chunk.get("chunk_type") == "table" and i > 0:
                prev = chunks[i - 1]
                if prev.get("chunk_type") == "text" and prev.get("section_path") == chunk.get("section_path"):
                    prev_text = prev.get("raw_content", "")
                    # ë„ˆë¬´ ê¸¸ë©´ ë’¤ìª½ 500ìë§Œ ì‚¬ìš©
                    ctx = prev_text[-500:] if len(prev_text) > 500 else prev_text

                    path = chunk.get("section_path", "N/A")
                    embedding_text = f"ë¬¸ì„œ ê²½ë¡œ: {path}\n[ë¬¸ë§¥ ì„¤ëª…: {ctx}]\n[í‘œ ë°ì´í„°]\n{raw_content}"
                    context_injected = True

            # ì¼ë°˜ í…ìŠ¤íŠ¸ì˜ ê²½ìš° ê²½ë¡œ ì •ë³´ë§Œì´ë¼ë„ ì¶”ê°€í•˜ë©´ ì¢‹ìŒ (ì„ íƒ ì‚¬í•­)
            elif chunk.get("chunk_type") == "text":
                path = chunk.get("section_path", "")
                embedding_text = f"{path}\n{raw_content}"

            texts_to_embed.append(embedding_text)
            indices_to_embed.append((i, context_injected))

        if not texts_to_embed:
            return

        # Batch Embedding Call (ë¹„ë™ê¸°)
        embeddings = await self.embedding.get_embeddings(texts_to_embed)

        # ê²°ê³¼ ë§¤í•‘
        for (idx, has_ctx), vec in zip(indices_to_embed, embeddings):
            chunks[idx]["embedding"] = vec

            # ë©”íƒ€ ì •ë³´ ì—…ë°ì´íŠ¸
            meta = chunks[idx].get("meta_info", {}) if chunks[idx].get("meta_info") else {}
            meta["has_embedding"] = True
            if has_ctx:
                meta["context_injected"] = True
            chunks[idx]["meta_info"] = meta
