import asyncio
import logging
import os
import re
import traceback

from backend.src.common.config import AI_CONFIG
from backend.src.common.database.connection import AsyncDatabaseEngine
from backend.src.common.enums import ReportJobStatus
from backend.src.company.repositories.company_repository import CompanyRepository
from backend.src.company.repositories.report_job_repository import ReportJobRepository
from backend.src.company.services.quality_inspector import evaluate_report_quality
from backend.src.company.services.report_job_service import ReportJobService
from knowledge_storm import STORMWikiRunner, STORMWikiRunnerArguments

from .adapter import save_storm_result_to_db
from .builder import build_hybrid_rm, build_lm_configs
from .io import create_run_directory, find_topic_directory, write_run_metadata


logger = logging.getLogger(__name__)


async def run_storm_pipeline(
    job_id: str,
    company_name: str,
    topic: str,
    jobs_dict: dict,  # ë©”ëª¨ë¦¬ ê¸°ë°˜ ìƒíƒœ ê´€ë¦¬ (Optional)
    model_provider: str = "openai",
):
    logger.info(f"[{job_id}] ğŸš€ Starting STORM Pipeline for {company_name}")

    # [ë©”ëª¨ë¦¬ ìƒíƒœ ì—…ë°ì´íŠ¸] - UI pollingìš©
    jobs_dict[job_id]["status"] = ReportJobStatus.PROCESSING.value
    jobs_dict[job_id]["progress"] = 10

    db_engine = AsyncDatabaseEngine()

    # ----------------------------------------------------------------
    # Phase 1: ì‘ì—… ì‹œì‘ ìƒíƒœ ê¸°ë¡ (DB)
    # ----------------------------------------------------------------
    try:
        async with db_engine.get_session() as session:
            # Service ì¡°ë¦½ (On-demand)
            job_repo = ReportJobRepository(session)
            job_service = ReportJobService(job_repo)

            # DB ìƒíƒœ ì—…ë°ì´íŠ¸: PROCESSING
            await job_service.start_job(job_id)

            # Company ID ì¡°íšŒ (Engine ì‹¤í–‰ì— í•„ìš”)
            company_repo = CompanyRepository(session)
            company = await company_repo.get_by_company_name(company_name)
            if not company:
                raise ValueError(f"Company '{company_name}' not found")
            company_id = company.id

    except Exception as e:
        logger.error(f"[{job_id}] Failed during initialization: {e}")
        # ì´ˆê¸°í™” ì‹¤íŒ¨ëŠ” ì¦‰ì‹œ ì¢…ë£Œ
        jobs_dict[job_id]["status"] = ReportJobStatus.FAILED.value
        jobs_dict[job_id]["message"] = str(e)
        return

    # ----------------------------------------------------------------
    # Phase 2: STORM ì—”ì§„ ì‹¤í–‰ (Long-Running Task)
    # ì£¼ì˜: ì´ êµ¬ê°„ì—ì„œëŠ” DB ì„¸ì…˜ì„ ë“¤ê³  ìˆìœ¼ë©´ ì•ˆ ë©ë‹ˆë‹¤. (Timeout ìœ„í—˜)
    # ----------------------------------------------------------------
    rm = None
    try:
        jobs_dict[job_id]["progress"] = 20

        # 1. Engine Build (Builder í™œìš© - ê°„ì†Œí™”ë¨)
        lm_configs = build_lm_configs(model_provider)
        rm = build_hybrid_rm(company_name=company_name, top_k=10)

        jobs_dict[job_id]["progress"] = 30

        # 2. IO & Runner ì„¤ì •
        base_output_dir = os.path.join("results", "enterprise")
        output_dir = create_run_directory(base_output_dir, company_id, company_name, job_id)

        engine_args = STORMWikiRunnerArguments(
            output_dir=output_dir,
            max_conv_turn=3,
            max_perspective=3,
            search_top_k=10,
            max_thread_num=AI_CONFIG.get("storm_max_thread_num", 1),
        )

        runner = STORMWikiRunner(engine_args, lm_configs, rm)

        # 3. Blocking Run (ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰í•˜ì—¬ FastAPI ë¸”ë¡œí‚¹ ë°©ì§€)
        logger.info(f"[{job_id}] Running STORM core...")
        from datetime import date

        today_str = date.today().strftime("%Y-%m-%d")
        full_topic = f"{company_name} {topic} (ê¸°ì¤€ì¼: {today_str})"
        # Windows íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ í—ˆìš©ë˜ì§€ ì•ŠëŠ” ë¬¸ì ì œê±°
        safe_topic = re.sub(r"[\\/:*?\"<>|]", " ", full_topic).strip()
        safe_topic = re.sub(r"\s+", " ", safe_topic)

        loop = asyncio.get_running_loop()

        # ë©”íƒ€ë°ì´í„° ê¸°ë¡
        write_run_metadata(output_dir, {"job_id": job_id, "topic": topic})

        # ì‹¤ì œ ì‹¤í–‰ (CPU Bound)
        await loop.run_in_executor(
            None,
            lambda: runner.run(
                topic=safe_topic,
                do_research=True,
                do_generate_outline=True,
                do_generate_article=True,
                do_polish_article=True,
            ),
        )

        # ë§ˆë¬´ë¦¬ ì‘ì—…
        runner.post_run()
        runner.summary()

        jobs_dict[job_id]["progress"] = 80

        # ----------------------------------------------------------------
        # Phase 2.5: í’ˆì§ˆ ê²€ìˆ˜ (Quality Inspection)
        # ----------------------------------------------------------------
        quality_result = None
        try:
            topic_dir = find_topic_directory(output_dir)
            if topic_dir:
                article_path = os.path.join(topic_dir, "storm_gen_article_polished.txt")
                if os.path.exists(article_path):
                    with open(article_path, encoding="utf-8") as f:
                        article_text = f.read()
                    if article_text.strip():
                        logger.info(f"[{job_id}] Running quality inspection...")
                        quality_result = evaluate_report_quality(article_text)
                        logger.info(f"[{job_id}] Quality grade: {quality_result.get('overall_grade', 'N/A')}")
                        jobs_dict[job_id]["quality_grade"] = quality_result.get("overall_grade", "N/A")
        except Exception as qe:
            logger.warning(f"[{job_id}] Quality inspection failed (non-blocking): {qe}")

        # ----------------------------------------------------------------
        # Phase 3: ê²°ê³¼ ì €ì¥ ë° ì¢…ë£Œ ì²˜ë¦¬ (DB)
        # ----------------------------------------------------------------
        async with db_engine.get_session() as session:
            # Service ë‹¤ì‹œ ì¡°ë¦½ (ìƒˆ ì„¸ì…˜)
            job_repo = ReportJobRepository(session)
            job_service = ReportJobService(job_repo)

            # [Adapter] ê²°ê³¼ ì €ì¥ (Adapter ë‚´ë¶€ì—ì„œë„ ì„¸ì…˜ ê´€ë¦¬ê°€ í•„ìš”í•  ìˆ˜ ìˆìŒ)
            # ì—¬ê¸°ì„œëŠ” Adapterê°€ sessionì„ ë°›ë„ë¡ ë¦¬íŒ©í† ë§í•œë‹¤ê³  ê°€ì •í•˜ê±°ë‚˜,
            # Adapterê°€ ë‚´ë¶€ì—ì„œ í•´ê²°í•˜ë„ë¡ í•´ì•¼ í•¨.
            # (ë‹¤ìŒ ë‹¨ê³„ ë¦¬íŒ©í† ë§ ëŒ€ìƒ)
            report_id = await save_storm_result_to_db(
                session=session,  # ì„¸ì…˜ ì£¼ì… ë°©ì‹ìœ¼ë¡œ ë³€ê²½ ì˜ˆì •
                company_name=company_name,
                topic=topic,
                output_dir=output_dir,
                model_name=model_provider,
                meta_info={"job_id": job_id, "quality": quality_result},
            )
            if report_id is None:
                raise RuntimeError(f"Report DB ì €ì¥ ì‹¤íŒ¨: output_dir={output_dir}")
            # ì„±ê³µ ì²˜ë¦¬
            await job_service.complete_job(job_id)

            # ë©”ëª¨ë¦¬ ìƒíƒœ ì—…ë°ì´íŠ¸
            jobs_dict[job_id]["status"] = ReportJobStatus.COMPLETED.value
            jobs_dict[job_id]["report_id"] = report_id
            jobs_dict[job_id]["progress"] = 100
            jobs_dict[job_id]["message"] = "ì™„ë£Œ"

    except Exception as e:
        logger.error(f"[{job_id}] Pipeline Runtime Error: {e}")
        traceback.print_exc()

        # ì—ëŸ¬ ë°œìƒ ì‹œ DBì— ê¸°ë¡ (Phase 3ì˜ ì„¸ì…˜ ì—°ê²° ì‹œë„)
        try:
            async with db_engine.get_session() as session:
                job_repo = ReportJobRepository(session)
                job_service = ReportJobService(job_repo)
                await job_service.fail_job(job_id, str(e))
        except Exception as db_e:
            logger.critical(f"Failed to log error to DB: {db_e}")

        jobs_dict[job_id]["status"] = ReportJobStatus.FAILED.value
        jobs_dict[job_id]["message"] = str(e)
        jobs_dict[job_id]["progress"] = 0
    finally:
        if rm and hasattr(rm, "aclose"):
            try:
                await rm.aclose()
            except Exception as close_error:
                logger.warning(f"[{job_id}] Failed to close HybridRM: {close_error}")
