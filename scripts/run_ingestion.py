#!/usr/bin/env python
"""
Îç∞Ïù¥ÌÑ∞ ÏàòÏßë ÌååÏù¥ÌîÑÎùºÏù∏ Ïã§Ìñâ Ïä§ÌÅ¨Î¶ΩÌä∏ (run_ingestion_v3.py)

PHASE 3.5: Legacy Migration Complete
- Refactored to call Async methods directly (No nested asyncio.run)
- Implements DB Reset using AsyncDatabaseEngine
- Orchestrates DART Agent -> DataPipeline -> EmbeddingWorker
"""

import argparse
import asyncio
import logging
import os
import sys

from sqlalchemy import func, select

# NEW: Service Layer & Database Engine
# Ïó¨Í∏∞Ïóê # noqa: E402Î•º Î∂ôÏó¨ÏÑú Í≤ΩÍ≥†Î•º Î¨¥ÏãúÌï©ÎãàÎã§.
from src.database import AsyncDatabaseEngine
from src.database.models import Base
from src.database.repositories import (
    AnalysisReportRepository,
    CompanyRepository,
    SourceMaterialRepository,
)

# Refactored Modules
from src.ingestion.embedding_worker import ContextLookbackEmbeddingWorker  # noqa: E402
from src.ingestion.pipeline import DataPipeline  # noqa: E402

# ÌîÑÎ°úÏ†ùÌä∏ Î£®Ìä∏Î•º pathÏóê Ï∂îÍ∞Ä
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)


# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger(__name__)


# ============================================================
# Helper Functions
# ============================================================


async def reset_database():
    """DB Ï¥àÍ∏∞Ìôî: Î™®Îì† ÌÖåÏù¥Î∏î ÏÇ≠Ï†ú ÌõÑ Ïû¨ÏÉùÏÑ±"""
    logger.warning("‚ö†Ô∏è RESETTING DATABASE: All data will be lost!")

    db_engine = AsyncDatabaseEngine()
    await db_engine.initialize()

    async with db_engine.engine.begin() as conn:
        # ÏùòÏ°¥ÏÑ± ÏàúÏÑúÏóê Îî∞Îùº Drop (Î∞òÎåÄ ÏàúÏÑú ÏïÑÎãò, cascadeÍ∞Ä ÏóÜÏúºÎ©¥ ÏàúÏÑú Ï§ëÏöî)
        # Base.metadata.drop_allÏùÄ ÏàúÏÑúÎ•º ÏïåÏïÑÏÑú Ï≤òÎ¶¨Ìï®
        await conn.run_sync(Base.metadata.drop_all)
        logger.info("‚úÖ All tables dropped.")

        await conn.run_sync(Base.metadata.create_all)
        logger.info("‚úÖ All tables recreated.")

    await db_engine.dispose()


# ============================================================
# Async Execution Functions
# ============================================================


async def run_efficient_mode_async(
    reset_db: bool = False,
    limit: int | None = None,
    bgn_de: str | None = None,
    end_de: str | None = None,
):
    """
    Ìö®Ïú® Î™®Îìú: ÏµúÍ∑º ÏÇ¨ÏóÖÎ≥¥Í≥†ÏÑúÍ∞Ä ÏûàÎäî Í∏∞ÏóÖÎßå ÏÑ†Î≥ÑÌïòÏó¨ ÏàòÏßëÌï©ÎãàÎã§.
    """
    if reset_db:
        await reset_database()

    logger.info("üîÑ Efficient Mode: Searching for targets...")

    pipeline = DataPipeline()

    # 1. ÎåÄÏÉÅ Í∏∞ÏóÖ Í≤ÄÏÉâ (Sync Agent call - It's okay in script level)
    # pipeline.run_efficient()Îäî ÎÇ¥Î∂ÄÏóêÏÑú asyncio.runÏùÑ Ïì∞ÎØÄÎ°ú ÏÇ¨Ïö© Í∏àÏßÄ
    # ÏßÅÏ†ë AgentÎ•º ÌÜµÌï¥ ÌÉÄÍ≤üÏùÑ Í∞ÄÏ†∏ÏòµÎãàÎã§.
    corps_with_reports = pipeline.agent.get_corps_with_reports(
        bgn_de=bgn_de, end_de=end_de
    )

    if limit:
        corps_with_reports = corps_with_reports[:limit]

    # (Corp, Report) ÌäúÌîåÏóêÏÑú Corp Í∞ùÏ≤¥Îßå Ï∂îÏ∂ú
    targets = [item[0] for item in corps_with_reports]

    logger.info(f"üìã Found {len(targets)} targets with reports.")

    # 2. Async ÌååÏù¥ÌîÑÎùºÏù∏ Ïã§Ìñâ
    await pipeline.run_pipeline_async(targets)

    logger.info("‚úÖ Efficient mode complete")


async def run_custom_mode_async(stock_codes: list, reset_db: bool = False):
    """
    Ïª§Ïä§ÌÖÄ Î™®Îìú: ÏßÄÏ†ïÌïú Ï¢ÖÎ™©ÏΩîÎìú Î¶¨Ïä§Ìä∏Ïóê ÎåÄÌï¥ÏÑúÎßå ÏàòÏßëÏùÑ ÏàòÌñâÌï©ÎãàÎã§.
    """
    if reset_db:
        await reset_database()

    logger.info(f"üîÑ Custom Mode: Processing stock codes: {stock_codes}")

    pipeline = DataPipeline()

    # 1. Ï¢ÖÎ™©ÏΩîÎìúÎ°ú Corp Í∞ùÏ≤¥ Î≥ÄÌôò
    targets = []
    for code in stock_codes:
        corp = pipeline.agent.get_corp_by_stock_code(code)
        if corp:
            targets.append(corp)
        else:
            logger.warning(f"‚ö†Ô∏è Stock code not found: {code}")

    if not targets:
        logger.error("‚ùå No valid targets found.")
        return

    # 2. Async ÌååÏù¥ÌîÑÎùºÏù∏ Ïã§Ìñâ
    await pipeline.run_pipeline_async(targets)

    logger.info("‚úÖ Custom mode complete")


async def run_embed_mode_async(
    batch_size: int = 32, limit: int | None = None, force: bool = False
):
    """
    ÏûÑÎ≤†Îî© ÏÉùÏÑ± Î™®Îìú: ÏàòÏßëÎêú ÌÖçÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞Ïóê ÎåÄÌï¥ Î≤°ÌÑ∞ ÏûÑÎ≤†Îî©ÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.
    """
    logger.info("üîÑ Embedding Mode: Generating embeddings with context look-back...")

    worker = ContextLookbackEmbeddingWorker(batch_size=batch_size)

    # Async Run Ìò∏Ï∂ú
    await worker.run_async(limit=limit, force=force)

    logger.info("‚úÖ Embedding mode complete")


async def run_stats_mode_async():
    """
    DB ÌÜµÍ≥Ñ Ï°∞Ìöå: ÌòÑÏû¨ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Ïùò Ï†ÅÏû¨ ÌòÑÌô©ÏùÑ Î≥¥Ïó¨Ï§çÎãàÎã§.
    """
    logger.info("\n[STATS] DB Statistics")
    logger.info("=" * 40)

    db_engine = AsyncDatabaseEngine()

    async with db_engine.get_session() as session:
        company_repo = CompanyRepository(session)
        analysis_repo = AnalysisReportRepository(session)
        source_repo = SourceMaterialRepository(session)

        # 1. Í∏∞Î≥∏ Î†àÏΩîÎìú Ïπ¥Ïö¥Ìä∏
        companies_count = (await company_repo.count()) or 0
        reports_count = (await analysis_repo.count()) or 0

        # Source Material Ï†ÑÏ≤¥ Ïπ¥Ïö¥Ìä∏
        # repo.count()Îäî ÌïÑÌÑ∞ ÏóÜÏù¥ Ï†ÑÏ≤¥ Í∞úÏàò
        materials_count = (await source_repo.count()) or 0

        # 2. ÏûÑÎ≤†Îî© ÏôÑÎ£åÎêú Ï≤≠ÌÅ¨ Ïπ¥Ïö¥Ìä∏
        # ORMÏúºÎ°ú Ïπ¥Ïö¥Ìä∏ Ï°∞Ìöå
        stmt = select(func.count(source_repo.model.id)).where(
            source_repo.model.embedding.is_not(None)
        )
        result = await session.execute(stmt)
        embedded_count = result.scalar() or 0

        # 3. Í≤∞Í≥º Ï∂úÎ†•
        logger.info(f"   Companies       : {companies_count:,}")
        logger.info(f"   Reports         : {reports_count:,}")
        logger.info(f"   Source Materials: {materials_count:,}")
        logger.info(f"   Embedded chunks : {embedded_count:,}")

        if materials_count > 0:
            embed_rate = (embedded_count / materials_count) * 100
            logger.info(f"   Embedding Rate  : {embed_rate:.1f}%")
        else:
            logger.info("   Embedding Rate  : 0.0% (No materials)")

    await db_engine.dispose()
    logger.info("=" * 40)


# ============================================================
# Main Entry Point
# ============================================================


def main():
    parser = argparse.ArgumentParser(
        description="Enterprise STORM Data Ingestion Pipeline (v3.5 Async)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    # Efficient mode (companies with reports)
    python -m scripts.run_ingestion_v3 --efficient
    
    # Specific companies
    python -m scripts.run_ingestion_v3 --codes 005930,000660
    
    # Generate embeddings
    python -m scripts.run_ingestion_v3 --embed --batch-size 64
    
    # DB statistics
    python -m scripts.run_ingestion_v3 --stats
""",
    )

    # Ïã§Ìñâ Î™®Îìú
    mode_group = parser.add_mutually_exclusive_group(required=True)
    mode_group.add_argument(
        "--efficient",
        action="store_true",
        help="Efficient mode (companies with reports)",
    )
    mode_group.add_argument("--codes", type=str, help="Stock codes (comma separated)")
    mode_group.add_argument(
        "--embed", action="store_true", help="Embedding generation mode"
    )
    mode_group.add_argument("--stats", action="store_true", help="DB statistics")

    # Í≥µÌÜµ ÏòµÏÖò
    parser.add_argument(
        "--reset-db",
        action="store_true",
        help="Reset DB before execution (WARNING: Deletes all data)",
    )
    parser.add_argument("--limit", type=int, help="Max companies/items to process")
    parser.add_argument("--bgn-de", type=str, help="Search start date (YYYYMMDD)")
    parser.add_argument("--end-de", type=str, help="Search end date (YYYYMMDD)")

    # ÏûÑÎ≤†Îî© ÏòµÏÖò
    parser.add_argument(
        "--batch-size", type=int, default=32, help="Embedding batch size"
    )
    parser.add_argument(
        "--force", action="store_true", help="Regenerate existing embeddings"
    )

    args = parser.parse_args()

    # Asyncio Ïã§Ìñâ ÎûòÌçº
    if args.efficient:
        asyncio.run(
            run_efficient_mode_async(
                reset_db=args.reset_db,
                limit=args.limit,
                bgn_de=args.bgn_de,
                end_de=args.end_de,
            )
        )
    elif args.codes:
        stock_codes = [code.strip() for code in args.codes.split(",")]
        asyncio.run(run_custom_mode_async(stock_codes, reset_db=args.reset_db))
    elif args.embed:
        asyncio.run(
            run_embed_mode_async(
                batch_size=args.batch_size, limit=args.limit, force=args.force
            )
        )
    elif args.stats:
        asyncio.run(run_stats_mode_async())


if __name__ == "__main__":
    main()
