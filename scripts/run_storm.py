#!/usr/bin/env python
"""
Enterprise STORM Pipeline - ê¸°ì—… ë¶„ì„ ë¦¬í¬íŠ¸ ì¼ê´„ ìƒì„± (v3.0 - COMPLETE)

PHASE 3: Service Layer Integration
âœ… **COMPLETE**: All functions from run_storm.py successfully migrated
- Replaced save_report_to_db() with GenerationService.save_generated_report()
- Full async/await support
- Uses AsyncDatabaseEngine
- All 9 functions from original run_storm.py: âœ… complete

PostgreSQL ë‚´ë¶€ DBë¥¼ í™œìš©í•œ ê¸°ì—… ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.
ì™¸ë¶€ ê²€ìƒ‰ ì—”ì§„ ëŒ€ì‹  PostgresRMì„ ì‚¬ìš©í•˜ì—¬ DART ë³´ê³ ì„œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.

í†µí•© ì•„í‚¤í…ì²˜:
    - src.common.config: í†µí•© ì„¤ì • (DB, AI, Embedding)
    - src.common.embedding: í†µí•© ì„ë² ë”© ì„œë¹„ìŠ¤ (ì°¨ì› ê²€ì¦ í¬í•¨)
    - knowledge_storm: STORM ì—”ì§„ (PostgresRM ì‚¬ìš©)
    - src.services: GenerationService (DB ì €ì¥)

Required Environment Variables:
    - OPENAI_API_KEY: OpenAI API key
    - GOOGLE_API_KEY: Google Gemini API key (--model-provider gemini ì‚¬ìš© ì‹œ)
    - EMBEDDING_PROVIDER: 'huggingface' ë˜ëŠ” 'openai' (DBì™€ ì¼ì¹˜ í•„ìˆ˜!)
    - PG_HOST, PG_PORT, PG_USER, PG_PASSWORD, PG_DATABASE: PostgreSQL ì ‘ì† ì •ë³´

âš ï¸ ì¤‘ìš”: EMBEDDING_PROVIDERëŠ” DBì— ì €ì¥ëœ ë²¡í„° ì°¨ì›ê³¼ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤!
    - HuggingFace: 768ì°¨ì›
    - OpenAI: 1536ì°¨ì›

Output Structure:
    results/
        topic_name/
            conversation_log.json
            raw_search_results.json
            storm_gen_outline.txt
            url_to_info.json
            storm_gen_article.txt
            storm_gen_article_polished.txt

Usage:
    python -m scripts.run_storm --topic "ì‚¼ì„±ì „ì SWOT ë¶„ì„"
    python -m scripts.run_storm --batch  # ë°°ì¹˜ ëª¨ë“œ (ANALYSIS_TARGETS ì‚¬ìš©)

Author: Enterprise STORM Team
Updated: 2026-01-21 - Phase 3 Service Layer Integration
"""

import asyncio
import json
import logging
import os
import re
import sys
from argparse import ArgumentParser
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from knowledge_storm import STORMWikiLMConfigs, STORMWikiRunner, STORMWikiRunnerArguments
from knowledge_storm.lm import AzureOpenAIModel, GoogleModel, OpenAIModel
from knowledge_storm.rm import SerperRM
from knowledge_storm.utils import load_api_key

# NEW: Service Layer & Database Engine
from src.common import AI_CONFIG, DB_CONFIG, TOPICS
from src.database import AsyncDatabaseEngine
from src.engine import HybridRM, PostgresRM
from src.repositories import CompanyRepository, GeneratedReportRepository
from src.services import GenerationService

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger(__name__)


def select_company_and_topic() -> tuple[int, str, str]:
    """
    CLI ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ: ê¸°ì—… ë° ì£¼ì œ ì„ íƒ

    DBì—ì„œ ê¸°ì—… ëª©ë¡ì„ ì¡°íšŒí•˜ì—¬ ë²ˆí˜¸ ë©”ë‰´ë¡œ ì¶œë ¥í•˜ê³ ,
    ì‚¬ìš©ìê°€ ì„ íƒí•œ ê¸°ì—…ëª…ê³¼ ë¶„ì„ ì£¼ì œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    Returns:
        tuple[int, str, str]: (ê¸°ì—…ID, ê¸°ì—…ëª…, ë¶„ì„ ì£¼ì œ)

    Raises:
        SystemExit: DBì—ì„œ ê¸°ì—… ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨ ì‹œ
    """
    # 1. ê¸°ì—… ì„ íƒ
    companies = get_available_companies()
    if not companies:
        print("âŒ [Error] DBì—ì„œ ì¡°íšŒëœ ê¸°ì—…ì´ ì—†ìŠµë‹ˆë‹¤. DB ì—°ê²°ì„ í™•ì¸í•˜ì„¸ìš”.")
        sys.exit(1)

    print("\n" + "=" * 50)
    print("        [ Enterprise STORM ë¶„ì„ê¸° ]")
    print("=" * 50)
    print("\nğŸ¢ ë¶„ì„í•  ê¸°ì—…ì„ ì„ íƒí•˜ì„¸ìš”:")

    for company_id, company_name in companies:
        print(f"  [{company_id}] {company_name}")

    target_company = (0, "")
    while True:
        try:
            sel = input("\nğŸ‘‰ ê¸°ì—… ë²ˆí˜¸ ì…ë ¥: ").strip()
            company_id = int(sel)
            if any(cid == company_id for cid, _ in companies):
                target_company = next((cid, name) for cid, name in companies if cid == company_id)
                break
            else:
                print("âš ï¸ ì˜¬ë°”ë¥¸ ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        except ValueError:
            print("âš ï¸ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    # 2. ì£¼ì œ ì„ íƒ
    topics = list()
    for topic in TOPICS:
        topics.append(topic["label"])

    print(f"\nğŸ“ [{target_company[1]}] ê´€ë ¨ ë¶„ì„ ì£¼ì œë¥¼ ì„ íƒí•˜ì„¸ìš”:")
    for idx, topic in enumerate(topics):
        print(f"  [{idx + 1}] {topic}")

    target_topic = ""
    while True:
        try:
            sel = input("\nğŸ‘‰ ì£¼ì œ ë²ˆí˜¸ ì…ë ¥: ").strip()
            idx = int(sel) - 1
            if 0 <= idx < len(topics):
                if idx == len(topics) - 1:  # ììœ  ì£¼ì œ
                    target_topic = input("   âœï¸  ì§ˆë¬¸í•  ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
                    if not target_topic:
                        print("âš ï¸ ì£¼ì œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                        continue
                else:
                    target_topic = topics[idx]
                break
            else:
                print("âš ï¸ ì˜¬ë°”ë¥¸ ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        except ValueError:
            print("âš ï¸ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    print(f"\nâœ… ë¶„ì„ ì‹œì‘: {target_company[1]} - {target_topic}")
    return target_company[0], target_company[1], target_topic


def _safe_dir_component(name: str, fallback: str = "unknown") -> str:
    """ë””ë ‰í† ë¦¬ ê²½ë¡œ ì»´í¬ë„ŒíŠ¸ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜í•©ë‹ˆë‹¤ (Windows ê¸ˆì§€ë¬¸ì ì œê±°, ê³µë°±->ì–¸ë”ìŠ¤ì½”ì–´)."""
    if not name:
        return fallback
    safe = name.replace(" ", "_")
    safe = safe.replace("/", "_").replace("\\", "_")
    safe = re.sub(r'[:*?"<>|]', "", safe)
    safe = safe.strip(". ")
    return safe or fallback


def build_run_output_dir(base_output_dir: str, company_id: int, company_name: str = "NONAME") -> str:
    """
    ì‹¤í–‰ë³„ ê²°ê³¼ í´ë”ë¥¼ `base/YYYYMMDD_HHMMSS_company_id/` í˜•íƒœë¡œ ìƒì„±í•©ë‹ˆë‹¤.

    Flat structureë¡œ íƒ€ì„ìŠ¤íƒ¬í”„ + company_idë¡œ ê³ ìœ ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.
    ì´ë¥¼ í†µí•´ ê²½ë¡œ ê¸¸ì´ ì œí•œ ë¬¸ì œë¥¼ íšŒí”¼í•˜ê³  ë””ë²„ê¹…ì„ ìš©ì´í•˜ê²Œ í•©ë‹ˆë‹¤.

    Args:
        base_output_dir: ê¸°ë³¸ ì¶œë ¥ ë””ë ‰í† ë¦¬
        company_id: ê¸°ì—… ID (ê³ ìœ ì„± ë³´ì¥ìš©)
        company_name: ê¸°ì—…ëª… (ë””ë ‰í† ë¦¬ ëª…ì— í¬í•¨í•  ìˆ˜ ìˆìŒ, ì„ íƒì‚¬í•­)

    Returns:
        ìƒì„±ëœ ê²°ê³¼ í´ë” ê²½ë¡œ
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    if company_name:
        company_suffix = _safe_dir_component(company_name, fallback="company")
        dir_name = f"{timestamp}_{company_id}_{company_suffix}"
    else:
        dir_name = f"{timestamp}_{company_id}"

    run_dir = os.path.join(base_output_dir, dir_name)

    # ê°™ì€ ì´ˆì— ì¬ì‹¤í–‰/ë³‘ë ¬ ì‹¤í–‰ ì‹œ ì¶©ëŒ ë°©ì§€
    suffix = 1
    candidate = run_dir
    while os.path.exists(candidate):
        suffix += 1
        candidate = f"{run_dir}_{suffix}"

    os.makedirs(candidate, exist_ok=True)
    return candidate


def write_run_args_json(
    run_output_dir: str,
    *,
    topic: str,
    company_id: int,
    company_name: str,
    args,
    model_name: str,
):
    """ì‹¤í–‰ í´ë”ì— ìŠ¤í¬ë¦½íŠ¸ ë ˆë²¨ ì„¤ì •ì„ JSONìœ¼ë¡œ ê¸°ë¡í•©ë‹ˆë‹¤."""
    payload = {
        "created_at": datetime.now().isoformat(timespec="seconds"),
        "topic": topic,
        "company_id": company_id,
        "company_name": company_name,
        "model_provider": getattr(args, "model_provider", None),
        "model_name": model_name,
        "output_dir": run_output_dir,
        "storm_args": {
            "max_conv_turn": getattr(args, "max_conv_turn", None),
            "max_perspective": getattr(args, "max_perspective", None),
            "search_top_k": getattr(args, "search_top_k", None),
            "min_score": getattr(args, "min_score", None),
            "max_thread_num": getattr(args, "max_thread_num", None),
            "do_research": getattr(args, "do_research", None),
            "do_generate_outline": getattr(args, "do_generate_outline", None),
            "do_generate_article": getattr(args, "do_generate_article", None),
            "do_polish_article": getattr(args, "do_polish_article", None),
        },
        "env": {
            "OPENAI_API_TYPE": getattr(args, "model_provider", None),
            "EMBEDDING_PROVIDER": ACTIVE_EMBEDDING_PROVIDER,
            "PG_HOST": DB_CONFIG.get("host"),
            "PG_PORT": DB_CONFIG.get("port"),
            "PG_DATABASE": DB_CONFIG.get("database"),
        },
    }

    path = os.path.join(run_output_dir, "run_args.json")
    with open(path, "w", encoding="utf-8") as f:
        json.dump(payload, f, indent=2, ensure_ascii=False)


async def save_report_to_db_async(
    ai_query: str,
    output_dir: str,
    model_name: str,
    company_id: int,
    company_name: str,
    analysis_topic: str,
) -> bool:
    """
    STORM ì‹¤í–‰ ê²°ê³¼ë¥¼ PostgreSQLì˜ Generated_Reports í…Œì´ë¸”ì— ì ì¬í•©ë‹ˆë‹¤ (NEW: Service Layer).

    í´ë” êµ¬ì¡°:
        base/YYYYMMDD_HHMMSS_company_id_company_name/
            {ai_query}/  â† STORM runnerê°€ ìƒì„±í•˜ëŠ” í´ë”
                conversation_log.json
                storm_gen_outline.txt
                storm_gen_article_polished.txt
                url_to_info.json
                raw_search_results.json
                ...

    Args:
        ai_query: LLMì—ê²Œ ì…ë ¥ëœ ì‹¤ì œ ì§ˆë¬¸/í”„ë¡¬í”„íŠ¸ (í´ë”ëª…ìœ¼ë¡œë„ ì‚¬ìš©ë¨)
        output_dir: STORM ì‹¤í–‰ ê²°ê³¼ ê¸°ë³¸ ë””ë ‰í† ë¦¬ (= run_output_dir)
        model_name: ì‚¬ìš©í•œ ëª¨ë¸ëª… ('openai' ë˜ëŠ” 'gemini')
        company_id: Companies tableì˜ ID (í•„ìˆ˜, FK)
        company_name: ê¸°ì—…ëª…
        analysis_topic: ë¶„ì„ ì£¼ì œ (DBì— ì €ì¥í•  topic í•„ë“œ)

    Returns:
        bool: ì €ì¥ ì„±ê³µ ì—¬ë¶€
    """

    # ========================================
    # Step 1: íŒŒì¼ ê²½ë¡œ êµ¬ì„±
    # ========================================
    safe_topic_dir = _safe_dir_component(ai_query)
    topic_output_dir = os.path.join(output_dir, safe_topic_dir)

    logger.info(f"Reading STORM output from: {topic_output_dir}")

    # ========================================
    # Step 2: í•„ìˆ˜ íŒŒì¼ ì½ê¸°
    # ========================================
    polished_article_path = os.path.join(topic_output_dir, "storm_gen_article_polished.txt")
    if not os.path.exists(polished_article_path):
        logger.error(f"Required file not found: {polished_article_path}")
        return False

    with open(polished_article_path, encoding="utf-8") as f:
        report_content = f.read()

    url_to_info_path = os.path.join(topic_output_dir, "url_to_info.json")
    if not os.path.exists(url_to_info_path):
        logger.error(f"Required file not found: {url_to_info_path}")
        return False

    with open(url_to_info_path, encoding="utf-8") as f:
        references_data = json.load(f)

    # ========================================
    # Step 3: ì„ íƒ íŒŒì¼ ì½ê¸°
    # ========================================
    toc_text = None
    outline_path = os.path.join(topic_output_dir, "storm_gen_outline.txt")
    if os.path.exists(outline_path):
        with open(outline_path, encoding="utf-8") as f:
            toc_text = f.read()

    conversation_log = None
    conv_log_path = os.path.join(topic_output_dir, "conversation_log.json")
    if os.path.exists(conv_log_path):
        with open(conv_log_path, encoding="utf-8") as f:
            conversation_log = json.load(f)

    run_config_data = None
    config_path = os.path.join(topic_output_dir, "run_config.json")
    if os.path.exists(config_path):
        with open(config_path, encoding="utf-8") as f:
            run_config_data = json.load(f)

    raw_search_results_data = None
    search_results_path = os.path.join(topic_output_dir, "raw_search_results.json")
    if os.path.exists(search_results_path):
        with open(search_results_path, encoding="utf-8") as f:
            raw_search_results_data = json.load(f)

    # ========================================
    # Step 4: meta_info ìƒì„±
    # ========================================
    meta_info = {"config": run_config_data, "search_results": raw_search_results_data}

    # ========================================
    # Step 5: DBì— ì €ì¥ (Refactored)
    # ========================================
    try:
        db_engine = AsyncDatabaseEngine()
        # Ensure engine is initialized
        # Note: If it's a singleton, this might be idempotent or handled inside get_session.

        async with db_engine.get_session() as session:
            # 1. Initialize Repositories with session
            generated_repo = GeneratedReportRepository(session)
            company_repo = CompanyRepository(session)

            # 2. Initialize Service with Repos (Injected)
            generation_service = GenerationService(generated_repo, company_repo)

            # 3. Call Service Method (NO session arg)
            report = await generation_service.save_generated_report(
                company_name=company_name,
                company_id=company_id,
                topic=analysis_topic,
                report_content=report_content,
                model_name=model_name,
                toc_text=toc_text,
                references_data=references_data,
                conversation_log=conversation_log,
                meta_info=meta_info,
            )

            # 4. Commit Transaction
            # Service layer generally delegates UoW/Commit to the caller in this pattern.
            await session.commit()

            logger.info(f"âœ“ Report saved to DB: {analysis_topic} (ID: {report.id}, company_id={company_id})")
            return True

    except Exception as e:
        logger.error(f"âœ— Failed to save report to DB: {e}", exc_info=True)
        return False


def save_report_to_db(
    ai_query: str,
    output_dir: str,
    secrets_path: str,
    model_name: str,
    company_id: int,
    company_name: str,
    analysis_topic: str,
) -> bool:
    """Sync wrapper for save_report_to_db_async (for backward compatibility)."""
    return asyncio.run(
        save_report_to_db_async(ai_query, output_dir, model_name, company_id, company_name, analysis_topic)
    )


# ============================================================
# Phase 3 Migration Notes
# ============================================================
"""
âœ… Completed:
- Replaced save_report_to_db() with GenerationService.save_generated_report()
- Async/await pattern for DB operations
- Uses AsyncDatabaseEngine for connection pooling

â³ TODO (Phase 3.5):
- Refactor main analysis loop to use async
- Add error recovery and retry logic
- Implement progress tracking in JOBS dictionary (for backend API)

ğŸ“‹ Legacy Components Still Used:
- STORM runner logic (unchanged)
- PostgresRM (unchanged, already optimized)
- These will be maintained as stable components
"""


def setup_lm_configs(provider: str = "openai") -> STORMWikiLMConfigs:
    """
    LLM ì„¤ì •ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤ (Service Layer ë²„ì „).

    Args:
        provider: LLM ê³µê¸‰ì ('openai' ë˜ëŠ” 'gemini')

    Returns:
        STORMWikiLMConfigs: ì„¤ì •ëœ LM êµ¬ì„± ê°ì²´
    """
    lm_configs = STORMWikiLMConfigs()

    if provider == "gemini":
        # Google Gemini ëª¨ë¸ ì„¤ì •
        gemini_kwargs = {
            "temperature": 1.0,
            "top_p": 0.9,
        }

        # Gemini ëª¨ë¸ëª… ì„¤ì • (2026ë…„ ìµœì‹  í˜•ì‹: models/ ì ‘ë‘ì‚¬ ì—†ì´ ì‚¬ìš©)
        gemini_flash_model = "gemini-2.0-flash"
        gemini_pro_model = "gemini-2.0-flash"

        conv_simulator_lm = GoogleModel(model=gemini_flash_model, max_tokens=2048, **gemini_kwargs)
        question_asker_lm = GoogleModel(model=gemini_flash_model, max_tokens=2048, **gemini_kwargs)
        outline_gen_lm = GoogleModel(model=gemini_pro_model, max_tokens=4096, **gemini_kwargs)
        article_gen_lm = GoogleModel(model=gemini_pro_model, max_tokens=8192, **gemini_kwargs)
        article_polish_lm = GoogleModel(model=gemini_pro_model, max_tokens=8192, **gemini_kwargs)

        logger.info(f"âœ“ Using Gemini models: {gemini_flash_model} (fast), {gemini_pro_model} (pro)")

    else:
        # OpenAI ëª¨ë¸ ì„¤ì • (ê¸°ë³¸ê°’)
        openai_kwargs = {
            "api_key": os.getenv("OPENAI_API_KEY"),
            "temperature": 1.0,
            "top_p": 0.9,
        }

        # API íƒ€ì…ì— ë”°ë¥¸ ëª¨ë¸ í´ë˜ìŠ¤ ì„ íƒ
        api_type = os.getenv("OPENAI_API_TYPE", "openai")
        ModelClass = OpenAIModel if api_type == "openai" else AzureOpenAIModel

        # ëª¨ë¸ëª… ì„¤ì •
        gpt_large_model = "gpt-4o-mini"
        gpt_fast_model = "gpt-4o"

        # Azure ì„¤ì • (í•„ìš”ì‹œ)
        if api_type == "azure":
            openai_kwargs["api_base"] = os.getenv("AZURE_API_BASE")
            openai_kwargs["api_version"] = os.getenv("AZURE_API_VERSION")

        conv_simulator_lm = ModelClass(model=gpt_large_model, max_tokens=500, **openai_kwargs)
        question_asker_lm = ModelClass(model=gpt_large_model, max_tokens=500, **openai_kwargs)
        outline_gen_lm = ModelClass(model=gpt_fast_model, max_tokens=400, **openai_kwargs)
        article_gen_lm = ModelClass(model=gpt_fast_model, max_tokens=700, **openai_kwargs)
        article_polish_lm = ModelClass(model=gpt_fast_model, max_tokens=700, **openai_kwargs)

        logger.info(f"âœ“ Using OpenAI models: {gpt_large_model} (large), {gpt_fast_model} (fast)")

    # LM ì„¤ì •ì— ëª¨ë¸ í• ë‹¹
    lm_configs.conv_simulator_lm = conv_simulator_lm
    lm_configs.question_asker_lm = question_asker_lm
    lm_configs.outline_gen_lm = outline_gen_lm
    lm_configs.article_gen_lm = article_gen_lm
    lm_configs.article_polish_lm = article_polish_lm

    return lm_configs


def fix_topic_json_encoding(ai_query: str, output_dir: str):
    """
    STORM runnerì˜ JSON íŒŒì¼ë“¤ì´ ì œëŒ€ë¡œ UTF-8 ì¸ì½”ë”©ë˜ì–´ ì €ì¥ë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    Windowsì—ì„œ ê¸°ë³¸ ì¸ì½”ë”©ì´ cp949ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ëª…ì‹œì ìœ¼ë¡œ UTF-8ë¡œ ì¬ì €ì¥í•©ë‹ˆë‹¤.

    Args:
        ai_query: STORM runnerê°€ ìƒì„±í•œ í´ë” ì´ë¦„
        output_dir: ê¸°ë³¸ ì¶œë ¥ ë””ë ‰í† ë¦¬
    """
    import json

    safe_topic_dir = _safe_dir_component(ai_query)
    topic_output_dir = os.path.join(output_dir, safe_topic_dir)
    json_files = [
        "url_to_info.json",
        "conversation_log.json",
        "raw_search_results.json",
        "run_config.json",
    ]

    for filename in json_files:
        filepath = os.path.join(topic_output_dir, filename)
        if os.path.exists(filepath):
            try:
                with open(filepath, encoding="utf-8") as f:
                    data = json.load(f)
                with open(filepath, "w", encoding="utf-8") as f:
                    json.dump(data, f, indent=2, ensure_ascii=False)
            except Exception as e:
                logger.warning(f"âš ï¸ Could not verify {filename}: {e}")


def run_batch_analysis(args):
    """
    ë°°ì¹˜ ë¶„ì„ì„ ì‹¤í–‰í•©ë‹ˆë‹¤ (Service Layer ë²„ì „).

    Args:
        args: ArgumentParserì—ì„œ íŒŒì‹±ëœ ì¸ì
    """

    # .env íŒŒì¼ë¡œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
    env_path = os.path.join(os.path.dirname(__file__), "..", ".env")
    if os.path.exists(env_path):
        from dotenv import load_dotenv

        load_dotenv(dotenv_path=env_path)
        logger.info(f"âœ“ Loaded environment variables from: {env_path}")

    logger.info("Initializing LM configurations...")
    lm_configs = setup_lm_configs(args.model_provider)
    current_model_name = "gemini" if args.model_provider == "gemini" else "openai"

    logger.info("Initializing HybridRM (Internal DB + External Search)...")
    internal_rm = PostgresRM(k=args.search_top_k, min_score=args.min_score)
    logger.info(f"âœ“ Internal RM (PostgresRM) initialized with k={args.search_top_k}")

    serper_api_key = AI_CONFIG.get("serper_api_key") or load_api_key("SERPER_API_KEY")
    external_rm = SerperRM(serper_search_api_key=serper_api_key, k=args.search_top_k)
    logger.info(f"âœ“ External RM (SerperRM) initialized with k={args.search_top_k}")

    rm = HybridRM(internal_rm, external_rm, internal_k=3, external_k=7)
    logger.info("âœ“ HybridRM initialized with internal_k=3, external_k=7 (3:7 ratio)")

    # ì»¤ë§¨ë“œë¼ì¸ì—ì„œ ì§€ì •ëœ ì •ë³´ ì‚¬ìš©
    company_id = args.company_id
    company_name = args.company_name
    analysis_topic = args.analysis_topic  # UIì—ì„œ ì„ íƒëœ ë¶„ì„ ì£¼ì œ ì¹´í…Œê³ ë¦¬
    ai_query = f"{company_name} {analysis_topic}"  # LLMì—ê²Œ ì…ë ¥ë˜ëŠ” ì‹¤ì œ ì§ˆë¬¸

    logger.info("=" * 60)
    logger.info("Starting Enterprise STORM Batch Analysis")
    logger.info(f"Model provider: {args.model_provider} ({current_model_name})")
    logger.info(f"Company: {company_name} (ID: {company_id})")
    logger.info("=" * 60)

    successful = True
    topic_start_time = datetime.now()

    try:
        if not company_id or not company_name:
            raise ValueError("Company ID and name are required")

        # ì‹¤í–‰ë³„ë¡œ ë³„ë„ í´ë” êµ¬ì„±: base/YYYYMMDD_HHMMSS_company_id_company_name/
        run_output_dir = build_run_output_dir(args.output_dir, company_id, company_name)
        logger.info(f"ğŸ“ Run output directory: {run_output_dir}")

        # Engine Arguments ì„¤ì • (output_dirì„ run_output_dirë¡œ ì§€ì •)
        engine_args = STORMWikiRunnerArguments(
            output_dir=run_output_dir,
            max_conv_turn=args.max_conv_turn,
            max_perspective=args.max_perspective,
            search_top_k=args.search_top_k,
            max_thread_num=args.max_thread_num,
        )

        # Runner ìƒì„±
        runner = STORMWikiRunner(engine_args, lm_configs, rm)

        # STORM íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        runner.run(
            topic=ai_query,
            do_research=args.do_research,
            do_generate_outline=args.do_generate_outline,
            do_generate_article=args.do_generate_article,
            do_polish_article=args.do_polish_article,
        )
        runner.post_run()
        runner.summary()

        # ìŠ¤í¬ë¦½íŠ¸ ë ˆë²¨ ì‹¤í–‰ ì„¤ì • ì €ì¥
        write_run_args_json(
            run_output_dir,
            topic=analysis_topic,
            company_id=company_id,
            company_name=company_name,
            args=args,
            model_name=current_model_name,
        )

        # DB ì €ì¥ ì „ì— 'ë°©ê¸ˆ ë§Œë“  í´ë”'ë§Œ ì¸ì½”ë”© ë³´ì • ìˆ˜í–‰
        fix_topic_json_encoding(ai_query, run_output_dir)

        # DBì— ê²°ê³¼ ì €ì¥
        save_report_to_db(
            ai_query,
            run_output_dir,
            "secrets_path",
            model_name=current_model_name,
            company_id=company_id,
            company_name=company_name,
            analysis_topic=analysis_topic,
        )
        elapsed = datetime.now() - topic_start_time
        logger.info(f"âœ“ Completed '{ai_query}' in {elapsed.total_seconds():.1f}s")

    except Exception as e:
        elapsed = datetime.now() - topic_start_time
        logger.error(f"âœ— Failed '{ai_query}' after {elapsed.total_seconds():.1f}s")
        logger.error(f"  Error: {e}")
        import traceback

        logger.error("  Full traceback:")
        logger.error(traceback.format_exc())
        successful = False
        if args.stop_on_error:
            raise

    finally:
        # PostgresRM ì—°ê²° ì¢…ë£Œ
        rm.close()

    logger.info("=" * 60)
    logger.info(f"Batch Analysis {'Successful' if successful else 'Failed'}!")
    logger.info(f"Output directory: {args.output_dir}")
    logger.info("=" * 60)


def main():
    """CLI ì§„ì…ì : ê¸°ì—…/ì£¼ì œ ì„ íƒ í›„ ë°°ì¹˜ ë¶„ì„ ì‹¤í–‰."""
    parser = ArgumentParser(description="Enterprise STORM - ê¸°ì—… ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ë„êµ¬ (Service Layer v3)")

    # ì‹¤í–‰ ëª¨ë“œ
    parser.add_argument(
        "--batch",
        action="store_true",
        help="ë°°ì¹˜ ëª¨ë“œë¡œ ì‹¤í–‰ (ANALYSIS_TARGETS ë¦¬ìŠ¤íŠ¸ ì¼ê´„ ì²˜ë¦¬). ë¯¸ì§€ì • ì‹œ ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ.",
    )

    # ì¶œë ¥ ì„¤ì •
    parser.add_argument(
        "--output-dir",
        type=str,
        default="./results/enterprise",
        help="ê²°ê³¼ë¬¼ ì €ì¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: ./results/enterprise)",
    )

    # ëª¨ë¸ ê³µê¸‰ì ì„ íƒ
    parser.add_argument(
        "--model-provider",
        type=str,
        choices=["openai", "gemini"],
        default="openai",
        help="ì‚¬ìš©í•  LLM ê³µê¸‰ì ì„ íƒ (openai ë˜ëŠ” gemini, ê¸°ë³¸ê°’: openai)",
    )

    # PostgresRM ì„¤ì •
    parser.add_argument(
        "--search-top-k",
        type=int,
        default=10,
        help="ê²€ìƒ‰ ê²°ê³¼ ìƒìœ„ kê°œ (ê¸°ë³¸ê°’: 10)",
    )
    parser.add_argument(
        "--min-score",
        type=float,
        default=0.5,
        help="ìµœì†Œ ìœ ì‚¬ë„ ì ìˆ˜ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.5)",
    )

    # STORM ì—”ì§„ ì„¤ì •
    parser.add_argument(
        "--max-conv-turn",
        type=int,
        default=3,
        help="ìµœëŒ€ ëŒ€í™” í„´ ìˆ˜ (ê¸°ë³¸ê°’: 3)",
    )
    parser.add_argument(
        "--max-perspective",
        type=int,
        default=3,
        help="ìµœëŒ€ ê´€ì  ìˆ˜ (ê¸°ë³¸ê°’: 3)",
    )
    parser.add_argument(
        "--max-thread-num",
        type=int,
        default=3,
        help="ìµœëŒ€ ìŠ¤ë ˆë“œ ìˆ˜ (ê¸°ë³¸ê°’: 3)",
    )

    # íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ ì„¤ì •
    parser.add_argument(
        "--do-research",
        action="store_true",
        default=True,
        help="ë¦¬ì„œì¹˜ ë‹¨ê³„ ì‹¤í–‰ (ê¸°ë³¸ê°’: True)",
    )
    parser.add_argument(
        "--do-generate-outline",
        action="store_true",
        default=True,
        help="ì•„ì›ƒë¼ì¸ ìƒì„± ë‹¨ê³„ ì‹¤í–‰ (ê¸°ë³¸ê°’: True)",
    )
    parser.add_argument(
        "--do-generate-article",
        action="store_true",
        default=True,
        help="ì•„í‹°í´ ìƒì„± ë‹¨ê³„ ì‹¤í–‰ (ê¸°ë³¸ê°’: True)",
    )
    parser.add_argument(
        "--do-polish-article",
        action="store_true",
        default=True,
        help="ì•„í‹°í´ ë‹¤ë“¬ê¸° ë‹¨ê³„ ì‹¤í–‰ (ê¸°ë³¸ê°’: True)",
    )

    # ì—ëŸ¬ ì²˜ë¦¬
    parser.add_argument(
        "--stop-on-error",
        action="store_true",
        help="ì—ëŸ¬ ë°œìƒ ì‹œ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ë‹¨",
    )

    args = parser.parse_args()

    # CLIì—ì„œ ê¸°ì—…/ì£¼ì œ ì„ íƒ (ë‹¨ê±´ ì‹¤í–‰)
    # ë°°ì¹˜ ëª¨ë“œ í”Œë˜ê·¸ê°€ ìˆì–´ë„ í˜„ì¬ ë¡œì§ìƒ ë‹¨ê±´ ì„ íƒ í›„ ì‹¤í–‰ë¨ (run_batch_analysis ì´ë¦„ë§Œ batch)
    # ì‹¤ì œ ì—¬ëŸ¬ ê±´ ë°°ì¹˜ëŠ” ANALYSIS_TARGETS ë“± ì™¸ë¶€ ì„¤ì • ì—°ë™ í•„ìš”í•˜ë‚˜ ì—¬ê¸°ì„œëŠ” ë‹¨ê±´ ë¡œì§ ìœ ì§€
    args.company_id, args.company_name, args.analysis_topic = select_company_and_topic()

    run_batch_analysis(args)


if __name__ == "__main__":
    main()
