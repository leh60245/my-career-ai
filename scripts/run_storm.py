#!/usr/bin/env python
"""
Enterprise STORM Pipeline - ê¸°ì—… ë¶„ì„ ë¦¬í¬íŠ¸ ì¼ê´„ ìƒì„±

PostgreSQL ë‚´ë¶€ DBë¥¼ í™œìš©í•œ ê¸°ì—… ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.
ì™¸ë¶€ ê²€ìƒ‰ ì—”ì§„ ëŒ€ì‹  PostgresRMì„ ì‚¬ìš©í•˜ì—¬ DART ë³´ê³ ì„œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.

í†µí•© ì•„í‚¤í…ì²˜:
    - src.common.config: í†µí•© ì„¤ì • (DB, AI, Embedding)
    - src.common.embedding: í†µí•© ì„ë² ë”© ì„œë¹„ìŠ¤ (ì°¨ì› ê²€ì¦ í¬í•¨)
    - knowledge_storm: STORM ì—”ì§„ (PostgresRM ì‚¬ìš©)

Required Environment Variables:
    - OPENAI_API_KEY: OpenAI API key
    - GOOGLE_API_KEY: Google Gemini API key (--model-provider gemini ì‚¬ìš© ì‹œ)
    - EMBEDDING_PROVIDER: 'huggingface' ë˜ëŠ” 'openai' (DBì™€ ì¼ì¹˜ í•„ìˆ˜!)
    - PG_HOST, PG_PORT, PG_USER, PG_PASSWORD, PG_DATABASE: PostgreSQL ì ‘ì† ì •ë³´

âš ï¸ ì¤‘ìš”: EMBEDDING_PROVIDERëŠ” DBì— ì €ì¥ëœ ë²¡í„° ì°¨ì›ê³¼ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤!
    - HuggingFace: 768ì°¨ì›
    - OpenAI: 1536ì°¨ì›

Output Structure:
    results/
        topic_name/
            conversation_log.json
            raw_search_results.json
            storm_gen_outline.txt
            url_to_info.json
            storm_gen_article.txt
            storm_gen_article_polished.txt

Usage:
    python -m scripts.run_storm --topic "ì‚¼ì„±ì „ì SWOT ë¶„ì„"
    python -m scripts.run_storm --batch  # ë°°ì¹˜ ëª¨ë“œ (ANALYSIS_TARGETS ì‚¬ìš©)

Author: Enterprise STORM Team
Updated: 2026-01-11 - Unified Architecture with Dimension Validation
"""

import os
import sys
import re
import json
import logging
from datetime import datetime
from argparse import ArgumentParser

import psycopg2
from psycopg2.extras import Json

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.common.db_utils import get_available_companies

from knowledge_storm import (
    STORMWikiRunnerArguments,
    STORMWikiRunner,
    STORMWikiLMConfigs,
)
from knowledge_storm.lm import OpenAIModel, AzureOpenAIModel, GoogleModel
from knowledge_storm.rm import PostgresRM
from knowledge_storm.utils import load_api_key

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# ============================================================
# ë¶„ì„ íƒ€ê²Ÿ ë¦¬ìŠ¤íŠ¸ (Batch Processing Targets)
# ============================================================
ANALYSIS_TARGETS = [
    "ì‚¼ì„±ì „ì ê¸°ì—… ê°œìš” ë° ì£¼ìš” ì‚¬ì—…ì˜ ë‚´ìš©"
    # "ì‚¼ì„±ì „ì ìµœê·¼ 3ê°œë…„ ìš”ì•½ ì¬ë¬´ì œí‘œ ë° ì¬ë¬´ ìƒíƒœ ë¶„ì„"
    # "ì‚¼ì„±ì „ì SWOT ë¶„ì„ (ê°•ì , ì•½ì , ê¸°íšŒ, ìœ„í˜‘)"
    # "ì‚¼ì„±ì „ì 3C ë¶„ì„ (ìì‚¬, ê²½ìŸì‚¬, ê³ ê°)"
    # "ì‚¼ì„±ì „ì ì±„ìš© ê³µê³  ë° ì¸ì¬ìƒ ë¶„ì„"
]


def select_company_and_topic() -> tuple[str, str]:
    """
    CLI ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ: ê¸°ì—… ë° ì£¼ì œ ì„ íƒ

    DBì—ì„œ ê¸°ì—… ëª©ë¡ì„ ì¡°íšŒí•˜ì—¬ ë²ˆí˜¸ ë©”ë‰´ë¡œ ì¶œë ¥í•˜ê³ ,
    ì‚¬ìš©ìê°€ ì„ íƒí•œ ê¸°ì—…ëª…ê³¼ ë¶„ì„ ì£¼ì œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    Returns:
        tuple[str, str]: (ê¸°ì—…ëª…, ë¶„ì„ ì£¼ì œ)

    Raises:
        SystemExit: DBì—ì„œ ê¸°ì—… ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨ ì‹œ
    """
    # 1. ê¸°ì—… ì„ íƒ
    companies = get_available_companies()
    if not companies:
        print("âŒ [Error] DBì—ì„œ ì¡°íšŒëœ ê¸°ì—…ì´ ì—†ìŠµë‹ˆë‹¤. DB ì—°ê²°ì„ í™•ì¸í•˜ì„¸ìš”.")
        sys.exit(1)

    print("\n" + "=" * 50)
    print("        [ Enterprise STORM ë¶„ì„ê¸° ]")
    print("=" * 50)
    print("\nğŸ¢ ë¶„ì„í•  ê¸°ì—…ì„ ì„ íƒí•˜ì„¸ìš”:")

    for idx, name in enumerate(companies):
        print(f"  [{idx + 1}] {name}")

    target_company = ""
    while True:
        try:
            sel = input("\nğŸ‘‰ ê¸°ì—… ë²ˆí˜¸ ì…ë ¥: ").strip()
            idx = int(sel) - 1
            if 0 <= idx < len(companies):
                target_company = companies[idx]
                break
            else:
                print("âš ï¸ ì˜¬ë°”ë¥¸ ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        except ValueError:
            print("âš ï¸ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    # 2. ì£¼ì œ ì„ íƒ
    topics = [
        "ê¸°ì—… ê°œìš” ë° ì£¼ìš” ì‚¬ì—… ë‚´ìš©",
        "ìµœê·¼ 3ê°œë…„ ì¬ë¬´ì œí‘œ ë° ì¬ë¬´ ìƒíƒœ ë¶„ì„",
        "SWOT ë¶„ì„ (ê°•ì , ì•½ì , ê¸°íšŒ, ìœ„í˜‘)",
        "3C ë¶„ì„ (ìì‚¬, ê²½ìŸì‚¬, ê³ ê°)",
        "ì±„ìš© ê³µê³  ë° ì¸ì¬ìƒ ë¶„ì„",
        "ììœ  ì£¼ì œ (ì§ì ‘ ì…ë ¥)"
    ]

    print(f"\nğŸ“ [{target_company}] ê´€ë ¨ ë¶„ì„ ì£¼ì œë¥¼ ì„ íƒí•˜ì„¸ìš”:")
    for idx, topic in enumerate(topics):
        print(f"  [{idx + 1}] {topic}")

    target_topic = ""
    while True:
        try:
            sel = input("\nğŸ‘‰ ì£¼ì œ ë²ˆí˜¸ ì…ë ¥: ").strip()
            idx = int(sel) - 1
            if 0 <= idx < len(topics):
                if idx == len(topics) - 1:  # ììœ  ì£¼ì œ
                    target_topic = input("   âœï¸  ì§ˆë¬¸í•  ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
                    if not target_topic:
                        print("âš ï¸ ì£¼ì œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                        continue
                else:
                    target_topic = topics[idx]
                break
            else:
                print("âš ï¸ ì˜¬ë°”ë¥¸ ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        except ValueError:
            print("âš ï¸ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    print(f"\nâœ… ë¶„ì„ ì‹œì‘: {target_company} - {target_topic}")
    return target_company, target_topic


def _extract_company_from_topic(topic: str, default_company: str | None) -> str:
    """
    í† í”½ ë¬¸ìì—´ì—ì„œ ê¸°ì—…ëª…ì„ ì¶”ì¶œ

    COMPANY_ALIASESë¥¼ í™œìš©í•˜ì—¬ í† í”½ì—ì„œ ì–¸ê¸‰ëœ ê¸°ì—…ëª…ì„ ì°¾ì•„
    ì •ê·œí™”ëœ ê¸°ì—…ëª…ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        topic: ë¶„ì„ í† í”½ (ì˜ˆ: "ì‚¼ì„±ì „ì SWOT ë¶„ì„")
        default_company: ê¸°ë³¸ ê¸°ì—…ëª… (í† í”½ì—ì„œ ì°¾ì§€ ëª»í•œ ê²½ìš° ì‚¬ìš©)

    Returns:
        ì •ê·œí™”ëœ ê¸°ì—…ëª… ë˜ëŠ” None

    Example:
        >>> _extract_company_from_topic("ì‚¼ì „ ì¬ë¬´ ë¶„ì„")
        "ì‚¼ì„±ì „ì"
        >>> _extract_company_from_topic("SK Hynix ê°œìš”")
        "SKí•˜ì´ë‹‰ìŠ¤"
    """
    try:
        # ë¡œì»¬ import: ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ í™˜ê²½ì—ì„œë§Œ í•„ìš”í•˜ë©°, ì‹¤íŒ¨í•´ë„ ê¸°ë³¸ê°’ìœ¼ë¡œ í´ë°±í•©ë‹ˆë‹¤.
        from src.common.config import extract_companies_from_query  # type: ignore

        companies = extract_companies_from_query(topic)
        if companies:
            return companies[0]
    except Exception as e:
        # ImportErrorë¿ ì•„ë‹ˆë¼ ì„¤ì •/alias ë¡œë”© ë¬¸ì œ ë“±ë„ ì—¬ê¸°ì„œ ë¡œê¹… í›„ í´ë°±
        logger.warning(f"Could not extract company from topic (fallback to default): {e}")

    return default_company


def create_topic_dir_name(topic: str) -> str:
    """
    í† í”½ëª…ì„ íŒŒì¼ì‹œìŠ¤í…œ í˜¸í™˜ ë””ë ‰í† ë¦¬ëª…ìœ¼ë¡œ ë³€í™˜

    ê·œì¹™:
    1. ê³µë°±ì€ ì–¸ë”ìŠ¤ì½”ì–´(_)ë¡œ ë³€í™˜
    2. ìœˆë„ìš° íŒŒì¼ ì‹œìŠ¤í…œ ê¸ˆì§€ ë¬¸ì(/:*?"<>|)ë§Œ ì œê±°/ë³€í™˜
    3. ê´„í˜¸(), ì‰¼í‘œ, ë“±ì€ ìœ ì§€ (STORMì´ ìœ ì§€í•˜ê¸° ë•Œë¬¸)

    Args:
        topic: ì›ë³¸ í† í”½ëª…

    Returns:
        ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ì—°ê²°ëœ ë””ë ‰í† ë¦¬ëª…
    """
    # 1. ê³µë°±ì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€í™˜
    dir_name = topic.replace(' ', '_')

    # 2. íŒŒì¼ ì‹œìŠ¤í…œ ê¸ˆì§€ ë¬¸ìë§Œ ì œê±° ë˜ëŠ” ë³€í™˜ (/:*?"<>|)
    # STORMì€ ë³´í†µ /ë§Œ _ë¡œ ë°”ê¾¸ê³  ë‚˜ë¨¸ì§€ëŠ” ê·¸ëŒ€ë¡œ ë‘ê±°ë‚˜ ì œê±°í•¨
    dir_name = dir_name.replace('/', '_').replace('\\', '_')
    dir_name = re.sub(r'[:*?"<>|]', '', dir_name)
    return dir_name


def _safe_dir_component(name: str, fallback: str = "unknown") -> str:
    """ë””ë ‰í† ë¦¬ ê²½ë¡œ ì»´í¬ë„ŒíŠ¸ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜í•©ë‹ˆë‹¤ (Windows ê¸ˆì§€ë¬¸ì ì œê±°, ê³µë°±->ì–¸ë”ìŠ¤ì½”ì–´)."""
    if not name:
        return fallback
    safe = name.replace(" ", "_")
    safe = safe.replace("/", "_").replace("\\", "_")
    safe = re.sub(r'[:*?"<>|]', "", safe)
    safe = safe.strip(". ")
    return safe or fallback


def build_run_output_dir(base_output_dir: str, company_name: str, topic: str) -> str:
    """ì‹¤í–‰ë³„ ê²°ê³¼ í´ë”ë¥¼ `base/company/topic/YYYYMMDD_HHMMSS` í˜•íƒœë¡œ ìƒì„±í•©ë‹ˆë‹¤."""
    company_dir = _safe_dir_component(company_name, fallback="unknown_company")
    # topicì€ ì´ë¯¸ íŒŒì¼ì‹œìŠ¤í…œ í˜¸í™˜ ë³€í™˜ ë¡œì§ì´ ìˆìœ¼ë‹ˆ ì¬ì‚¬ìš©
    topic_dir = create_topic_dir_name(topic)
    topic_dir = _safe_dir_component(topic_dir, fallback="unknown_topic")

    # êµ¬ë¶„ ê°€ëŠ¥í•œ íƒ€ì„ìŠ¤íƒ¬í”„ (ì´ˆ ë‹¨ìœ„)
    timestamp_dir = datetime.now().strftime("%Y%m%d_%H%M%S")

    run_dir = os.path.join(base_output_dir, company_dir, topic_dir, timestamp_dir)

    # ê°™ì€ ì´ˆì— ì¬ì‹¤í–‰/ë³‘ë ¬ ì‹¤í–‰ ì‹œ ì¶©ëŒ ë°©ì§€
    suffix = 1
    candidate = run_dir
    while os.path.exists(candidate):
        suffix += 1
        candidate = f"{run_dir}_{suffix}"

    os.makedirs(candidate, exist_ok=True)
    return candidate


def write_run_args_json(run_output_dir: str, *, topic: str, company_filter: str | None, args, model_name: str):
    """ì‹¤í–‰ í´ë”ì— ìŠ¤í¬ë¦½íŠ¸ ë ˆë²¨ ì„¤ì •ì„ JSONìœ¼ë¡œ ê¸°ë¡í•©ë‹ˆë‹¤."""
    payload = {
        "created_at": datetime.now().isoformat(timespec="seconds"),
        "topic": topic,
        "company_filter": company_filter,
        "model_provider": getattr(args, "model_provider", None),
        "model_name": model_name,
        "output_dir": run_output_dir,
        "storm_args": {
            "max_conv_turn": getattr(args, "max_conv_turn", None),
            "max_perspective": getattr(args, "max_perspective", None),
            "search_top_k": getattr(args, "search_top_k", None),
            "min_score": getattr(args, "min_score", None),
            "max_thread_num": getattr(args, "max_thread_num", None),
            "do_research": getattr(args, "do_research", None),
            "do_generate_outline": getattr(args, "do_generate_outline", None),
            "do_generate_article": getattr(args, "do_generate_article", None),
            "do_polish_article": getattr(args, "do_polish_article", None),
        },
        "env": {
            "OPENAI_API_TYPE": os.getenv("OPENAI_API_TYPE"),
            "EMBEDDING_PROVIDER": os.getenv("EMBEDDING_PROVIDER"),
            "PG_HOST": os.getenv("PG_HOST"),
            "PG_PORT": os.getenv("PG_PORT"),
            "PG_DATABASE": os.getenv("PG_DATABASE"),
        },
    }

    path = os.path.join(run_output_dir, "run_args.json")
    with open(path, "w", encoding="utf-8") as f:
        json.dump(payload, f, indent=2, ensure_ascii=False)


def save_report_to_db(topic: str, output_dir: str, secrets_path: str, model_name: str = "gpt-4o") -> bool:
    """
    STORM ì‹¤í–‰ ê²°ê³¼ë¥¼ PostgreSQLì˜ Generated_Reports í…Œì´ë¸”ì— ì ì¬í•©ë‹ˆë‹¤.

    Args:
        topic: ë¶„ì„ ì£¼ì œ
        output_dir: STORM ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        secrets_path: secrets.toml íŒŒì¼ ê²½ë¡œ
        model_name: ì‚¬ìš©ëœ LLM ëª¨ë¸ëª…

    Returns:
        bool: ì„±ê³µ ì—¬ë¶€
    """
    # í† í”½ë³„ ê²°ê³¼ ë””ë ‰í† ë¦¬ ê²½ë¡œ ìƒì„±
    topic_dir_name = create_topic_dir_name(topic)
    topic_output_dir = os.path.join(output_dir, topic_dir_name)

    # ========================================
    # Step 1: í•„ìˆ˜ íŒŒì¼ ì½ê¸°
    # ========================================
    # storm_gen_article_polished.txt (í•„ìˆ˜)
    polished_article_path = os.path.join(topic_output_dir, "storm_gen_article_polished.txt")
    if not os.path.exists(polished_article_path):
        logger.error(f"Required file not found: {polished_article_path}")
        return False

    with open(polished_article_path, "r", encoding="utf-8") as f:
        report_content = f.read()

    # url_to_info.json (í•„ìˆ˜)
    url_to_info_path = os.path.join(topic_output_dir, "url_to_info.json")
    if not os.path.exists(url_to_info_path):
        logger.error(f"Required file not found: {url_to_info_path}")
        return False

    with open(url_to_info_path, "r", encoding="utf-8") as f:
        references_data = json.load(f)

    # ========================================
    # Step 2: ì„ íƒ íŒŒì¼ ì½ê¸°
    # ========================================
    # storm_gen_outline.txt (ì„ íƒ)
    toc_text = None
    outline_path = os.path.join(topic_output_dir, "storm_gen_outline.txt")
    if os.path.exists(outline_path):
        with open(outline_path, "r", encoding="utf-8") as f:
            toc_text = f.read()

    # conversation_log.json (ì„ íƒ)
    conversation_log = None
    conv_log_path = os.path.join(topic_output_dir, "conversation_log.json")
    if os.path.exists(conv_log_path):
        with open(conv_log_path, "r", encoding="utf-8") as f:
            conversation_log = json.load(f)

    # run_config.json (ì„ íƒ)
    run_config_data = None
    config_path = os.path.join(topic_output_dir, "run_config.json")
    if os.path.exists(config_path):
        with open(config_path, "r", encoding="utf-8") as f:
            run_config_data = json.load(f)

    # raw_search_results.json (ì„ íƒ)
    raw_search_results_data = None
    search_results_path = os.path.join(topic_output_dir, "raw_search_results.json")
    if os.path.exists(search_results_path):
        with open(search_results_path, "r", encoding="utf-8") as f:
            raw_search_results_data = json.load(f)

    # ========================================
    # Step 3: meta_info ìƒì„±
    # ========================================
    meta_info = {
        "config": run_config_data,
        "search_results": raw_search_results_data
    }

    # ========================================
    # Step 4: company_name ì¶”ì¶œ ë° company_id ì¡°íšŒ
    # ========================================
    company_name = topic.split()[0] if topic else "Unknown"

    # ========================================
    # Step 5: DB INSERT (with company_id FK)
    # ========================================
    try:
        # DB ì ‘ì† ì •ë³´ ë¡œë“œ
        conn = psycopg2.connect(
            host=os.getenv("PG_HOST"),
            port=os.getenv("PG_PORT", "5432"),
            user=os.getenv("PG_USER"),
            password=os.getenv("PG_PASSWORD"),
            database=os.getenv("PG_DATABASE")
        )

        cursor = conn.cursor()
        
        # ğŸ”§ FIX: company_nameìœ¼ë¡œ company_id ì¡°íšŒ
        cursor.execute("""
            SELECT id FROM "Companies" WHERE company_name = %s
        """, (company_name,))
        result = cursor.fetchone()
        
        if not result:
            logger.warning(f"âš ï¸ Company '{company_name}' not found in Companies table. Inserting without company_id.")
            company_id = None
        else:
            company_id = result[0]
            logger.info(f"âœ“ Found company_id: {company_id} for '{company_name}'")

        insert_query = """
        INSERT INTO "Generated_Reports"
        (company_name, company_id, topic, report_content, toc_text, references_data, conversation_log, meta_info, model_name)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
        """

        cursor.execute(insert_query, (
            company_name,
            company_id,  # ğŸ”§ NEW: FK ì¶”ê°€
            topic,
            report_content,
            toc_text,
            Json(references_data) if references_data else None,
            Json(conversation_log) if conversation_log else None,
            Json(meta_info),
            model_name
        ))

        conn.commit()
        cursor.close()
        conn.close()

        logger.info(f"âœ“ Report saved to DB: {topic} (company_id={company_id})")
        return True

    except Exception as e:
        logger.error(f"âœ— Failed to save report to DB: {e}")
        return False


def setup_lm_configs(provider: str = "openai") -> STORMWikiLMConfigs:
    """
    LLM ì„¤ì •ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

    Args:
        provider: LLM ê³µê¸‰ì ('openai' ë˜ëŠ” 'gemini')

    Returns:
        STORMWikiLMConfigs: ì„¤ì •ëœ LM êµ¬ì„± ê°ì²´
    """
    lm_configs = STORMWikiLMConfigs()

    if provider == "gemini":
        # Google Gemini ëª¨ë¸ ì„¤ì •
        gemini_kwargs = {
            "temperature": 1.0,
            "top_p": 0.9,
        }

        # Gemini ëª¨ë¸ëª… ì„¤ì • (2026ë…„ ìµœì‹  í˜•ì‹: models/ ì ‘ë‘ì‚¬ ì—†ì´ ì‚¬ìš©)
        gemini_flash_model = "gemini-2.0-flash"
        gemini_pro_model = "gemini-2.0-flash"

        # ê° ì»´í¬ë„ŒíŠ¸ë³„ LM ì„¤ì •
        # - conv_simulator_lm, question_asker_lm: ë¹ ë¥¸ ëª¨ë¸ (ëŒ€í™” ì‹œë®¬ë ˆì´ì…˜)
        # - outline_gen_lm, article_gen_lm, article_polish_lm: ê°•ë ¥í•œ ëª¨ë¸ (ì½˜í…ì¸  ìƒì„±)
        conv_simulator_lm = GoogleModel(
            model=gemini_flash_model, max_tokens=2048, **gemini_kwargs  # í† í° ìˆ˜ ì•½ê°„ ìƒí–¥
        )
        question_asker_lm = GoogleModel(
            model=gemini_flash_model, max_tokens=2048, **gemini_kwargs
        )
        outline_gen_lm = GoogleModel(
            model=gemini_pro_model, max_tokens=4096, **gemini_kwargs
        )
        article_gen_lm = GoogleModel(
            model=gemini_pro_model, max_tokens=8192, **gemini_kwargs
        )
        article_polish_lm = GoogleModel(
            model=gemini_pro_model, max_tokens=8192, **gemini_kwargs
        )

        logger.info(f"âœ“ Using Gemini models: {gemini_flash_model} (fast), {gemini_pro_model} (pro)")

    else:
        # OpenAI ëª¨ë¸ ì„¤ì • (ê¸°ë³¸ê°’)
        openai_kwargs = {
            "api_key": os.getenv("OPENAI_API_KEY"),
            "temperature": 1.0,
            "top_p": 0.9,
        }

        # API íƒ€ì…ì— ë”°ë¥¸ ëª¨ë¸ í´ë˜ìŠ¤ ì„ íƒ
        api_type = os.getenv("OPENAI_API_TYPE", "openai")
        ModelClass = OpenAIModel if api_type == "openai" else AzureOpenAIModel

        # ëª¨ë¸ëª… ì„¤ì •
        gpt_35_model_name = "gpt-5-mini-2025-08-07" 
        gpt_4_model_name = "gpt-5.2"

        # Azure ì„¤ì • (í•„ìš”ì‹œ)
        if api_type == "azure":
            openai_kwargs["api_base"] = os.getenv("AZURE_API_BASE")
            openai_kwargs["api_version"] = os.getenv("AZURE_API_VERSION")

        # ê° ì»´í¬ë„ŒíŠ¸ë³„ LM ì„¤ì •
        # - conv_simulator_lm, question_asker_lm: ì €ë ´í•œ ëª¨ë¸ (ëŒ€í™” ì‹œë®¬ë ˆì´ì…˜)
        # - outline_gen_lm, article_gen_lm, article_polish_lm: ê°•ë ¥í•œ ëª¨ë¸ (ì½˜í…ì¸  ìƒì„±)
        conv_simulator_lm = ModelClass(
            model=gpt_35_model_name, max_tokens=500, **openai_kwargs
        )
        question_asker_lm = ModelClass(
            model=gpt_35_model_name, max_tokens=500, **openai_kwargs
        )
        outline_gen_lm = ModelClass(
            model=gpt_4_model_name, max_tokens=400, **openai_kwargs
        )
        article_gen_lm = ModelClass(
            model=gpt_4_model_name, max_tokens=700, **openai_kwargs
        )
        article_polish_lm = ModelClass(
            model=gpt_4_model_name, max_tokens=4000, **openai_kwargs
        )

        logger.info(f"âœ“ Using OpenAI models: {gpt_35_model_name} (fast), {gpt_4_model_name} (pro)")

    lm_configs.set_conv_simulator_lm(conv_simulator_lm)
    lm_configs.set_question_asker_lm(question_asker_lm)
    lm_configs.set_outline_gen_lm(outline_gen_lm)
    lm_configs.set_article_gen_lm(article_gen_lm)
    lm_configs.set_article_polish_lm(article_polish_lm)

    return lm_configs


def fix_topic_json_encoding(topic: str, output_dir: str):
    """
    ë°©ê¸ˆ ìƒì„±ëœ íŠ¹ì • í† í”½ì˜ ê²°ê³¼ í´ë” ë‚´ JSON íŒŒì¼ë“¤ë§Œ ì¸ì½”ë”©ì„ ë³´ì •í•©ë‹ˆë‹¤.
    (ì „ì²´ ë””ë ‰í† ë¦¬ë¥¼ ìŠ¤ìº”í•˜ì§€ ì•Šì•„ íš¨ìœ¨ì ì…ë‹ˆë‹¤.)

    Args:
        topic: ë¶„ì„ ì£¼ì œ (í´ë”ëª… ìƒì„±ìš©)
        output_dir: ì „ì²´ ê²°ê³¼ ì €ì¥ ë£¨íŠ¸ ê²½ë¡œ
    """
    # 1. save_report_to_dbì™€ ë™ì¼í•œ ë¡œì§ìœ¼ë¡œ íƒ€ê²Ÿ í´ë” ê²½ë¡œ ìƒì„±
    topic_dir_name = create_topic_dir_name(topic)
    target_dir = os.path.join(output_dir, topic_dir_name)

    if not os.path.exists(target_dir):
        logger.warning(f"Target directory not found for encoding fix: {target_dir}")
        return

    logger.info(f"Fixing JSON encoding in specific folder: {target_dir}")

    # 2. í•´ë‹¹ í´ë” ë‚´ì˜ íŒŒì¼ë§Œ ìˆœíšŒ
    for file in os.listdir(target_dir):
        if file.endswith(".json"):
            file_path = os.path.join(target_dir, file)
            try:
                # ì½ê¸°
                with open(file_path, "r", encoding="utf-8") as f:
                    data = json.load(f)

                # ë‹¤ì‹œ ì“°ê¸° (ensure_ascii=False)
                with open(file_path, "w", encoding="utf-8") as f:
                    json.dump(data, f, indent=4, ensure_ascii=False)
            except Exception as e:
                logger.warning(f"Failed to fix encoding for {file}: {e}")


def run_batch_analysis(args):
    """
    ë°°ì¹˜ ë¶„ì„ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.

    Args:
        args: ArgumentParserì—ì„œ íŒŒì‹±ëœ ì¸ì
    """
    # secrets.toml ë¡œë“œ
    secrets_path = os.path.join(os.path.dirname(__file__), "..", "secrets.toml")
    if os.path.exists(secrets_path):
        load_api_key(toml_file_path=secrets_path)
        logger.info(f"âœ“ Loaded secrets from: {secrets_path}")
    else:
        # í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œë„ ì°¾ê¸°
        if os.path.exists("secrets.toml"):
            load_api_key(toml_file_path="secrets.toml")
            logger.info("âœ“ Loaded secrets from: secrets.toml")
        else:
            logger.error("âœ— secrets.toml not found!")
            logger.error("  Please create secrets.toml with required API keys and DB credentials.")
            sys.exit(1)

    # LM ì„¤ì • ì´ˆê¸°í™”
    logger.info("Initializing LM configurations...")
    lm_configs = setup_lm_configs(args.model_provider)

    # ëª¨ë¸ëª… ê²°ì • (DB ì €ì¥ìš©)
    if args.model_provider == "gemini":
        current_model_name = "gemini"
    else:
        current_model_name = "gpt-4o"

    # PostgresRM ì´ˆê¸°í™” (ë‚´ë¶€ DB ê²€ìƒ‰)
    # company_filterëŠ” ê° í† í”½ ì²˜ë¦¬ ì‹œ ë™ì ìœ¼ë¡œ ì„¤ì •ë¨
    logger.info("Initializing PostgresRM (Internal DB Search)...")
    rm = PostgresRM(k=args.search_top_k, min_score=args.min_score)
    logger.info(f"âœ“ PostgresRM initialized with k={args.search_top_k}, min_score={args.min_score}")

    # ë¶„ì„ ëŒ€ìƒ ë¦¬ìŠ¤íŠ¸ ê²°ì •
    if args.topics:
        # ì»¤ë§¨ë“œë¼ì¸ì—ì„œ ì§€ì •ëœ í† í”½ ì‚¬ìš©
        analysis_targets = args.topics
    else:
        # ê¸°ë³¸ ë¶„ì„ íƒ€ê²Ÿ ì‚¬ìš©
        analysis_targets = ANALYSIS_TARGETS

    # company_nameì´ ì „ë‹¬ëœ ê²½ìš° (ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œì—ì„œ í˜¸ì¶œ)
    # args.company_nameì´ ìˆìœ¼ë©´ ê·¸ ê°’ì„ ì‚¬ìš©
    default_company_filter = getattr(args, 'company_name', None)

    total_topics = len(analysis_targets)
    successful = 0
    failed = 0

    logger.info("=" * 60)
    logger.info(f"Starting Enterprise STORM Batch Analysis")
    logger.info(f"Model provider: {args.model_provider} ({current_model_name})")
    logger.info(f"Total topics to process: {total_topics}")
    logger.info(f"Output directory: {args.output_dir}")
    if default_company_filter:
        logger.info(f"Default company filter: {default_company_filter}")
    logger.info("=" * 60)

    for idx, topic in enumerate(analysis_targets, 1):
        topic_start_time = datetime.now()
        logger.info("")
        logger.info(f"[{idx}/{total_topics}] Processing: '{topic}'")
        logger.info("-" * 50)

        try:
            # í† í”½ì—ì„œ ê¸°ì—…ëª… ì¶”ì¶œí•˜ì—¬ company_filter ì„¤ì •
            company_filter = _extract_company_from_topic(topic, default_company_filter)
            rm.set_company_filter(company_filter)
            if company_filter:
                logger.info(f"ğŸ“Œ Company filter set to: {company_filter}")

            # ì‹¤í–‰ë³„ë¡œ ë³„ë„ í´ë” êµ¬ì„±: base/company/topic/timestamp
            run_output_dir = build_run_output_dir(args.output_dir, company_filter or default_company_filter, topic)
            logger.info(f"ğŸ“ Run output directory: {run_output_dir}")

            # Engine Arguments ì„¤ì • (output_dirì„ run_output_dirë¡œ ì§€ì •)
            engine_args = STORMWikiRunnerArguments(
                output_dir=run_output_dir,
                max_conv_turn=args.max_conv_turn,
                max_perspective=args.max_perspective,
                search_top_k=args.search_top_k,
                max_thread_num=args.max_thread_num,
            )

            # Runner ìƒì„±
            runner = STORMWikiRunner(engine_args, lm_configs, rm)

            # STORM íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            runner.run(
                topic=topic,
                do_research=args.do_research,
                do_generate_outline=args.do_generate_outline,
                do_generate_article=args.do_generate_article,
                do_polish_article=args.do_polish_article,
            )
            runner.post_run()
            runner.summary()

            # ìŠ¤í¬ë¦½íŠ¸ ë ˆë²¨ ì‹¤í–‰ ì„¤ì • ì €ì¥
            write_run_args_json(
                run_output_dir,
                topic=topic,
                company_filter=company_filter,
                args=args,
                model_name=current_model_name,
            )

            # DB ì €ì¥ ì „ì— 'ë°©ê¸ˆ ë§Œë“  í´ë”'ë§Œ ì¸ì½”ë”© ë³´ì • ìˆ˜í–‰
            fix_topic_json_encoding(topic, run_output_dir)

            # DBì— ê²°ê³¼ ì €ì¥ (run_output_dir ê¸°ì¤€)
            save_report_to_db(topic, run_output_dir, secrets_path, model_name=current_model_name)

            elapsed = datetime.now() - topic_start_time
            logger.info(f"âœ“ Completed '{topic}' in {elapsed.total_seconds():.1f}s")
            successful += 1

        except Exception as e:
            elapsed = datetime.now() - topic_start_time
            logger.error(f"âœ— Failed '{topic}' after {elapsed.total_seconds():.1f}s")
            logger.error(f"  Error: {e}")
            failed += 1

            if args.stop_on_error:
                logger.error("Stopping due to --stop-on-error flag")
                break

    # PostgresRM ì—°ê²° ì¢…ë£Œ
    rm.close()

    # ìµœì¢… ìš”ì•½
    logger.info("")
    logger.info("=" * 60)
    logger.info("Batch Analysis Complete!")
    logger.info(f"  Successful: {successful}/{total_topics}")
    logger.info(f"  Failed: {failed}/{total_topics}")
    logger.info(f"  Output directory: {args.output_dir}")
    logger.info("=" * 60)


def main():
    parser = ArgumentParser(
        description="Enterprise STORM - ê¸°ì—… ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ë„êµ¬"
    )

    # ì‹¤í–‰ ëª¨ë“œ
    parser.add_argument(
        "--batch",
        action="store_true",
        help="ë°°ì¹˜ ëª¨ë“œë¡œ ì‹¤í–‰ (ANALYSIS_TARGETS ë¦¬ìŠ¤íŠ¸ ì¼ê´„ ì²˜ë¦¬). ë¯¸ì§€ì • ì‹œ ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ.",
    )

    # ì¶œë ¥ ì„¤ì •
    parser.add_argument(
        "--output-dir",
        type=str,
        default="./results/enterprise",
        help="ê²°ê³¼ë¬¼ ì €ì¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: ./results/enterprise)",
    )

    # ëª¨ë¸ ê³µê¸‰ì ì„ íƒ
    parser.add_argument(
        "--model-provider",
        type=str,
        choices=["openai", "gemini"],
        default="openai",
        help="ì‚¬ìš©í•  LLM ê³µê¸‰ì ì„ íƒ (openai ë˜ëŠ” gemini, ê¸°ë³¸ê°’: openai)",
    )

    # í† í”½ ì„¤ì • (ì„ íƒì )
    parser.add_argument(
        "--topics",
        type=str,
        nargs="+",
        default=None,
        help="ë¶„ì„í•  í† í”½ ë¦¬ìŠ¤íŠ¸ (ë¯¸ì§€ì •ì‹œ ê¸°ë³¸ ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©)",
    )

    # PostgresRM ì„¤ì •
    parser.add_argument(
        "--search-top-k",
        type=int,
        default=10,
        help="ê²€ìƒ‰ ê²°ê³¼ ìƒìœ„ kê°œ (ê¸°ë³¸ê°’: 10)",
    )
    parser.add_argument(
        "--min-score",
        type=float,
        default=0.5,
        help="ìµœì†Œ ìœ ì‚¬ë„ ì ìˆ˜ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.5)",
    )

    # STORM ì—”ì§„ ì„¤ì •
    parser.add_argument(
        "--max-conv-turn",
        type=int,
        default=3,
        help="ìµœëŒ€ ëŒ€í™” í„´ ìˆ˜ (ê¸°ë³¸ê°’: 3)",
    )
    parser.add_argument(
        "--max-perspective",
        type=int,
        default=3,
        help="ìµœëŒ€ ê´€ì  ìˆ˜ (ê¸°ë³¸ê°’: 3)",
    )
    parser.add_argument(
        "--max-thread-num",
        type=int,
        default=3,
        help="ìµœëŒ€ ìŠ¤ë ˆë“œ ìˆ˜ (ê¸°ë³¸ê°’: 3)",
    )

    # íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ ì„¤ì •
    parser.add_argument(
        "--do-research",
        action="store_true",
        default=True,
        help="ë¦¬ì„œì¹˜ ë‹¨ê³„ ì‹¤í–‰ (ê¸°ë³¸ê°’: True)",
    )
    parser.add_argument(
        "--do-generate-outline",
        action="store_true",
        default=True,
        help="ì•„ì›ƒë¼ì¸ ìƒì„± ë‹¨ê³„ ì‹¤í–‰ (ê¸°ë³¸ê°’: True)",
    )
    parser.add_argument(
        "--do-generate-article",
        action="store_true",
        default=True,
        help="ì•„í‹°í´ ìƒì„± ë‹¨ê³„ ì‹¤í–‰ (ê¸°ë³¸ê°’: True)",
    )
    parser.add_argument(
        "--do-polish-article",
        action="store_true",
        default=True,
        help="ì•„í‹°í´ ë‹¤ë“¬ê¸° ë‹¨ê³„ ì‹¤í–‰ (ê¸°ë³¸ê°’: True)",
    )

    # ì—ëŸ¬ ì²˜ë¦¬
    parser.add_argument(
        "--stop-on-error",
        action="store_true",
        help="ì—ëŸ¬ ë°œìƒ ì‹œ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ë‹¨",
    )

    args = parser.parse_args()

    # action="store_true"ì™€ default=Trueê°€ í•¨ê»˜ ì‚¬ìš©ë˜ë©´ í•­ìƒ Trueê°€ ë˜ë¯€ë¡œ
    # ê¸°ë³¸ê°’ì´ Trueì¸ í”Œë˜ê·¸ë“¤ì€ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •
    if not any([args.do_research, args.do_generate_outline,
                args.do_generate_article, args.do_polish_article]):
        args.do_research = True
        args.do_generate_outline = True
        args.do_generate_article = True
        args.do_polish_article = True

    # ì‹¤í–‰ ëª¨ë“œ ë¶„ê¸°
    if args.batch:
        # ë°°ì¹˜ ëª¨ë“œ: ê¸°ì¡´ ANALYSIS_TARGETS ë¦¬ìŠ¤íŠ¸ ì¼ê´„ ì²˜ë¦¬
        # company_nameì€ í† í”½ì—ì„œ ìë™ ì¶”ì¶œë¨
        args.company_name = None
        run_batch_analysis(args)
    else:
        # ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ: CLIì—ì„œ ê¸°ì—…/ì£¼ì œ ì„ íƒ í›„ ë‹¨ê±´ ì‹¤í–‰
        company_name, topic = select_company_and_topic()
        # ì¿¼ë¦¬ ì¡°í•©: "{ê¸°ì—…ëª…} {ì£¼ì œ}" í˜•ì‹
        final_topic = f"{company_name} {topic}"
        # args.topicsì— ë‹¨ê±´ í• ë‹¹í•˜ì—¬ ê¸°ì¡´ run_batch_analysis ë¡œì§ ì¬ì‚¬ìš©
        args.topics = [final_topic]
        # ì„ íƒëœ ê¸°ì—…ëª…ì„ argsì— ì¶”ê°€ (company_filter ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©)
        args.company_name = company_name
        run_batch_analysis(args)


if __name__ == "__main__":
    main()

