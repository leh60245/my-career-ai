"""
PostgreSQL Vector Search Connector for Enterprise STORM

ì´ ëª¨ë“ˆì€ PostgreSQL DBì˜ Source_Materials í…Œì´ë¸”ì—ì„œ
pgvectorë¥¼ í™œìš©í•œ ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

Database Schema (Source_Materials):
    - id (PK): Integer
    - report_id (FK): Integer (Analysis_Reports ì°¸ì¡°)
    - chunk_type: VARCHAR ('text', 'table', 'noise_merged')
    - section_path: TEXT (ì„¹ì…˜ ê²½ë¡œ)
    - sequence_order: INTEGER (ë¬¸ì„œ ë‚´ ë“±ì¥ ìˆœì„œ)
    - raw_content: TEXT (ë³¸ë¬¸ ë˜ëŠ” Markdown í‘œ)
    - embedding: vector(768) (pgvector)
    - metadata: JSONB (ë©”íƒ€ë°ì´í„°)
        - has_merged_meta: boolean (ë³‘í•©ëœ ë©”íƒ€ ì •ë³´ í¬í•¨ ì—¬ë¶€)
        - is_noise_dropped: boolean (noise_merged íƒ€ì…ì¼ ë•Œë§Œ ì¡´ì¬)
        - has_embedding: boolean
        - context_injected: boolean
        - length: integer

Author: Enterprise STORM Team
Updated: 2026-01-10 - Sliding Window Retrieval & Merged Meta Prompting
"""

import os
import logging
from typing import List, Dict

import numpy as np
import psycopg2
from psycopg2.extras import RealDictCursor
from sentence_transformers import SentenceTransformer

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class PostgresConnector:
    """
    PostgreSQL ë²¡í„° ê²€ìƒ‰ ì»¤ë„¥í„°

    DART ë³´ê³ ì„œ ë°ì´í„°ê°€ ì €ì¥ëœ PostgreSQL DBì—ì„œ
    ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³  STORM í˜¸í™˜ í¬ë§·ìœ¼ë¡œ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    Attributes:
        conn: psycopg2 ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ê°ì²´
        model: SentenceTransformer ì„ë² ë”© ëª¨ë¸

    Example:
        >>> connector = PostgresConnector()
        >>> results = connector.search("ì‚¼ì„±ì „ì ë§¤ì¶œ í˜„í™©", top_k=5)
        >>> for r in results:
        ...     print(r['title'], r['score'])
    """

    def __init__(self):
        """
        PostgresConnector ì´ˆê¸°í™”

        í™˜ê²½ë³€ìˆ˜ì—ì„œ DB ì ‘ì† ì •ë³´ë¥¼ ë¡œë“œí•˜ê³  ì—°ê²°ì„ ì„¤ì •í•©ë‹ˆë‹¤.
        sentence_transformers ëª¨ë¸(paraphrase-multilingual-mpnet-base-v2)ì„ ë¡œë“œí•©ë‹ˆë‹¤.

        Required Environment Variables:
            - PG_HOST: PostgreSQL í˜¸ìŠ¤íŠ¸ ì£¼ì†Œ
            - PG_PORT: PostgreSQL í¬íŠ¸ (ê¸°ë³¸ê°’: 5432)
            - PG_USER: ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš©ìëª…
            - PG_PASSWORD: ë°ì´í„°ë² ì´ìŠ¤ ë¹„ë°€ë²ˆí˜¸
            - PG_DATABASE: ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„

        Raises:
            RuntimeError: DB ì—°ê²° ì •ë³´ê°€ ëˆ„ë½ë˜ì—ˆê±°ë‚˜ ì—°ê²°ì— ì‹¤íŒ¨í•œ ê²½ìš°
        """
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ DB ì ‘ì† ì •ë³´ ë¡œë“œ
        self.host = os.environ.get("PG_HOST")
        self.port = os.environ.get("PG_PORT", "5432")
        self.user = os.environ.get("PG_USER")
        self.password = os.environ.get("PG_PASSWORD")
        self.database = os.environ.get("PG_DATABASE")

        # í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ ê²€ì¦
        missing_vars = []
        if not self.host:
            missing_vars.append("PG_HOST")
        if not self.user:
            missing_vars.append("PG_USER")
        if not self.password:
            missing_vars.append("PG_PASSWORD")
        if not self.database:
            missing_vars.append("PG_DATABASE")

        if missing_vars:
            raise RuntimeError(
                f"Missing required environment variables: {', '.join(missing_vars)}. "
                "Please set them in secrets.toml or as environment variables."
            )

        # DB ì—°ê²° ì„¤ì •
        try:
            self.conn = psycopg2.connect(
                host=self.host,
                port=self.port,
                user=self.user,
                password=self.password,
                database=self.database
            )
            logger.info(f"Successfully connected to PostgreSQL database: {self.database}")
        except psycopg2.Error as e:
            raise RuntimeError(f"Failed to connect to PostgreSQL: {e}")

        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (paraphrase-multilingual-mpnet-base-v2)
        # 768ì°¨ì› ë²¡í„° ìƒì„±, ë‹¤êµ­ì–´ ì§€ì›
        try:
            self.model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')
            logger.info("Successfully loaded SentenceTransformer model")
        except Exception as e:
            raise RuntimeError(f"Failed to load SentenceTransformer model: {e}")

    def _embed_query(self, query: str) -> np.ndarray:
        """
        ì¿¼ë¦¬ ë¬¸ìì—´ì„ ë²¡í„°ë¡œ ë³€í™˜

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬ ë¬¸ìì—´

        Returns:
            768ì°¨ì› numpy ë°°ì—´
        """
        embedding = self.model.encode(query, convert_to_numpy=True)
        return embedding

    def _fetch_window_context(
            self,
            table_rows: List[Dict],
            window_size: int = 1
    ) -> Dict[tuple, Dict[str, str]]:
        """
        í…Œì´ë¸” íƒ€ì… í–‰ë“¤ì— ëŒ€í•´ Sliding Window Contextë¥¼ ì¡°íšŒ

        sequence_order ê¸°ì¤€ìœ¼ë¡œ ì•ë’¤ window_sizeë§Œí¼ì˜ ì¸ì ‘ ì²­í¬ë¥¼ ê°€ì ¸ì™€
        í•˜ë‚˜ì˜ Context Blockìœ¼ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.

        Note:
            DB ì—…ë°ì´íŠ¸ë¡œ ì¸í•´ noise_merged íƒ€ì…ì¸ ì²­í¬ëŠ” ê²€ìƒ‰ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ,
            Sequenceê°€ ë¹„ì–´ìˆì„ ê²½ìš° ìë™ìœ¼ë¡œ ê±´ë„ˆë›°ê²Œ ë©ë‹ˆë‹¤.

        Args:
            table_rows: chunk_type='table'ì¸ ê²€ìƒ‰ ê²°ê³¼ í–‰ë“¤
            window_size: ì•ë’¤ë¡œ ê°€ì ¸ì˜¬ ì²­í¬ ìˆ˜ (ê¸°ë³¸ê°’: 1)

        Returns:
            {(report_id, sequence_order): {'prev': prev_text, 'next': next_text}} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
        """
        if not table_rows:
            return {}

        context_map = {}

        with self.conn.cursor(cursor_factory=RealDictCursor) as cur:
            for row in table_rows:
                report_id = row['report_id']
                current_seq = row['sequence_order']

                context_data = {'prev': None, 'next': None}

                # ì´ì „ ì²­í¬ ì¡°íšŒ (sequence_order - 1 ~ sequence_order - window_size)
                for offset in range(1, window_size + 1):
                    prev_seq = current_seq - offset
                    if prev_seq < 0:
                        continue

                    cur.execute("""
                        SELECT raw_content, section_path, chunk_type
                        FROM "Source_Materials"
                        WHERE report_id = %s AND sequence_order = %s
                    """, (report_id, prev_seq))

                    prev_row = cur.fetchone()
                    if prev_row and prev_row['chunk_type'] != 'noise_merged':
                        if context_data['prev'] is None:
                            context_data['prev'] = prev_row['raw_content']
                        else:
                            # ë” ì•ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì•ì— ë¶™ì„
                            context_data['prev'] = prev_row['raw_content'] + "\n\n" + context_data['prev']

                # ë‹¤ìŒ ì²­í¬ ì¡°íšŒ (sequence_order + 1 ~ sequence_order + window_size)
                for offset in range(1, window_size + 1):
                    next_seq = current_seq + offset

                    cur.execute("""
                        SELECT raw_content, section_path, chunk_type
                        FROM "Source_Materials"
                        WHERE report_id = %s AND sequence_order = %s
                    """, (report_id, next_seq))

                    next_row = cur.fetchone()
                    if next_row and next_row['chunk_type'] != 'noise_merged':
                        if context_data['next'] is None:
                            context_data['next'] = next_row['raw_content']
                        else:
                            # ë” ë’¤ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë’¤ì— ë¶™ì„
                            context_data['next'] = str(context_data['next']) + "\n\n" + next_row['raw_content']

                context_map[(report_id, current_seq)] = context_data

        return context_map

    def search(self, query: str, top_k: int = 5, window_size: int = 1) -> List[Dict]:
        """
        ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰

        ì…ë ¥ëœ ì¿¼ë¦¬ë¥¼ ë²¡í„°í™”í•˜ì—¬ PostgreSQLì˜ Source_Materials í…Œì´ë¸”ì—ì„œ
        ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œë“¤ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.

        chunk_typeì´ 'table'ì¸ ê²½ìš° Sliding Window Contextë¥¼ ì ìš©í•˜ì—¬
        ì•ë’¤ ì¸ì ‘ ì²­í¬ë¥¼ í•¨ê»˜ ê°€ì ¸ì™€ í•˜ë‚˜ì˜ Context Blockìœ¼ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.

        has_merged_metaê°€ trueì¸ ê²½ìš° LLMì—ê²Œ ë³‘í•©ëœ ë©”íƒ€ ì •ë³´(ë‹¨ìœ„, ë²”ë¡€ ë“±)ê°€
        ë¬¸ë‹¨ ëì— í¬í•¨ë˜ì–´ ìˆìŒì„ ì•Œë¦¬ëŠ” ì•ˆë‚´ ë¬¸êµ¬ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬ ë¬¸ìì—´
            top_k: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜ (ê¸°ë³¸ê°’: 5)
            window_size: Table ì²­í¬ì˜ ì•ë’¤ë¡œ ê°€ì ¸ì˜¬ ì¸ì ‘ ì²­í¬ ìˆ˜ (ê¸°ë³¸ê°’: 1)

        Returns:
            STORM í˜¸í™˜ í¬ë§·ì˜ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            [
                {
                    "content": "ê²€ìƒ‰ëœ ë³¸ë¬¸ ë‚´ìš©",
                    "title": "ì„¹ì…˜ ê²½ë¡œ (section_path)",
                    "url": "dart_report_{report_id}",
                    "score": 0.85,  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (1 - distance)
                    "has_merged_meta": true/false  # ë³‘í•©ëœ ë©”íƒ€ ì •ë³´ í¬í•¨ ì—¬ë¶€
                },
                ...
            ]

        Raises:
            psycopg2.Error: ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ì‹¤í–‰ ì‹¤íŒ¨ ì‹œ
        """
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        query_embedding = self._embed_query(query)

        # numpy ë°°ì—´ì„ PostgreSQL vector í˜•ì‹ ë¬¸ìì—´ë¡œ ë³€í™˜
        embedding_str = "[" + ",".join(map(str, query_embedding.tolist())) + "]"

        results = []

        try:
            with self.conn.cursor(cursor_factory=RealDictCursor) as cur:
                # ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ SQL ì‹¤í–‰
                # pgvectorì˜ <=> ì—°ì‚°ì: ì½”ì‚¬ì¸ ê±°ë¦¬ (0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬)
                # chunk_typeì´ 'noise_merged'ì¸ ì²­í¬ëŠ” ê²€ìƒ‰ì—ì„œ ì œì™¸
                cur.execute("""
                    SELECT 
                        id,
                        raw_content, 
                        section_path, 
                        chunk_type, 
                        report_id, 
                        sequence_order,
                        metadata,
                        COALESCE((metadata->>'has_merged_meta')::boolean, false) as has_merged_meta,
                        COALESCE((metadata->>'is_noise_dropped')::boolean, false) as is_noise_dropped,
                        (embedding <=> %s::vector) as distance
                    FROM "Source_Materials"
                    WHERE chunk_type != 'noise_merged'
                    ORDER BY distance ASC
                    LIMIT %s
                """, (embedding_str, top_k))

                rows = cur.fetchall()

                if not rows:
                    logger.warning(f"No results found for query: {query}")
                    return []

                # ğŸš¨ is_noise_dropped í”Œë˜ê·¸ ê²€ì¦ (ì •ìƒì ìœ¼ë¡œ í•„í„°ë§ë˜ì—ˆëŠ”ì§€ í™•ì¸)
                noise_dropped_rows = [row for row in rows if row.get('is_noise_dropped', False)]
                if noise_dropped_rows:
                    logger.error(
                        f"[ALERT] {len(noise_dropped_rows)} rows with is_noise_dropped=true found in search results! "
                        "This should not happen - please check the Vector DB indexing."
                    )

                # Sliding Window Context: table íƒ€ì… í–‰ë“¤ì— ëŒ€í•´ ì•ë’¤ ì²­í¬ ì¡°íšŒ
                table_rows = [row for row in rows if row['chunk_type'] == 'table']
                context_map = self._fetch_window_context(table_rows, window_size=window_size)

                # ê²°ê³¼ ê°€ê³µ ë° STORM í¬ë§· ë³€í™˜
                for row in rows:
                    content = row['raw_content']
                    has_merged = row.get('has_merged_meta', False)

                    # chunk_typeì´ 'table'ì¸ ê²½ìš° Sliding Window Context ì ìš©
                    if row['chunk_type'] == 'table':
                        context_key = (row['report_id'], row['sequence_order'])
                        if context_key in context_map:
                            ctx = context_map[context_key]
                            prev_text = ctx.get('prev')
                            next_text = ctx.get('next')

                            # ì•ë’¤ ë¬¸ë§¥ì„ ì¡°í•©í•˜ì—¬ Context Block êµ¬ì„±
                            if prev_text:
                                content = f"[ì´ì „ ë¬¸ë§¥]\n{prev_text}\n\n[í‘œ ë°ì´í„°]\n{content}"
                            else:
                                content = f"[ì„¹ì…˜: {row['section_path']}]\n\n[í‘œ ë°ì´í„°]\n{content}"

                            if next_text:
                                content = f"{content}\n\n[ì´í›„ ë¬¸ë§¥]\n{next_text}"
                        else:
                            # ë¬¸ë§¥ì´ ì—†ìœ¼ë©´ section_pathë¥¼ ë¬¸ë§¥ìœ¼ë¡œ ì‚¬ìš©
                            content = f"[ì„¹ì…˜: {row['section_path']}]\n\n[í‘œ ë°ì´í„°]\n{content}"

                    # has_merged_metaê°€ trueì¸ ê²½ìš° LLM ì•ˆë‚´ ë¬¸êµ¬ ì¶”ê°€
                    if has_merged:
                        content = (
                            "[ì°¸ê³ : ì´ ë¬¸ë‹¨ ëì— ë³‘í•©ëœ ë©”íƒ€ ì •ë³´(ë‹¨ìœ„, ë²”ë¡€, ê¸°ì¤€ì¼ì ë“±)ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. "
                            "ìˆ˜ì¹˜ í•´ì„ ì‹œ ë°˜ë“œì‹œ í™•ì¸í•˜ì„¸ìš”.]\n\n" + content
                        )

                    # ì½”ì‚¬ì¸ ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ ì ìˆ˜ë¡œ ë³€í™˜ (1 - distance)
                    # distanceê°€ 0ì´ë©´ score=1 (ì™„ì „ ì¼ì¹˜)
                    score = 1 - float(row['distance'])

                    # URLì— ê³ ìœ  IDë¥¼ í¬í•¨í•˜ì—¬ ê° ê²€ìƒ‰ ê²°ê³¼ê°€ ë³„ë„ì˜ ì¶œì²˜ë¡œ ì¸ì‹ë˜ë„ë¡ í•¨
                    # í˜•ì‹: dart_report_{report_id}_chunk_{id}
                    unique_url = f"dart_report_{row['report_id']}_chunk_{row['id']}"

                    results.append({
                        "content": content,
                        "title": row['section_path'],
                        "url": unique_url,
                        "score": score,
                        "has_merged_meta": has_merged
                    })

                logger.info(f"Found {len(results)} results for query: {query}")

                return results

        except psycopg2.Error as e:
            logger.error(f"Database query failed: {e}")
            self.conn.rollback()  # [í•µì‹¬] íŠ¸ëœì­ì…˜ ë³µêµ¬!
            return []  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (í”„ë¡œê·¸ë¨ ì¤‘ë‹¨ ë°©ì§€)

        except Exception as e:
            logger.error(f"Unexpected error in search: {e}")
            return []


    def close(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ"""
        if self.conn:
            self.conn.close()
            logger.info("PostgreSQL connection closed")

    def __enter__(self):
        """Context manager ì§„ì…"""
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager ì¢…ë£Œ ì‹œ ì—°ê²° í•´ì œ"""
        self.close()
        return False


# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    import sys
    import toml

    sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(__file__))))


    def load_api_key(toml_file_path):
        """secrets.tomlì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (í…ŒìŠ¤íŠ¸ìš© ë¡œì»¬ í•¨ìˆ˜)"""
        try:
            with open(toml_file_path, "r") as file:
                data = toml.load(file)
            for key, value in data.items():
                os.environ[key] = str(value)
        except FileNotFoundError:
            print(f"File not found: {toml_file_path}", file=sys.stderr)
        except toml.TomlDecodeError:
            print(f"Error decoding TOML file: {toml_file_path}", file=sys.stderr)


    # secrets.tomlì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
    secrets_path = os.path.join(
        os.path.dirname(os.path.dirname(os.path.dirname(__file__))),
        "secrets.toml"
    )

    if os.path.exists(secrets_path):
        load_api_key(secrets_path)
        print(f"âœ“ Loaded secrets from: {secrets_path}")
    else:
        print(f"âš  secrets.toml not found at: {secrets_path}")
        print("  Please create secrets.toml with PG_HOST, PG_PORT, PG_USER, PG_PASSWORD, PG_DATABASE")
        sys.exit(1)

    try:
        # PostgresConnector ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        print("\n[1] Initializing PostgresConnector...")
        connector = PostgresConnector()
        print("âœ“ PostgresConnector initialized successfully")

        # í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ìˆ˜í–‰
        test_query = "ì‚¼ì„±ì „ì ë§¤ì¶œ í˜„í™©"
        print(f"\n[2] Searching for: '{test_query}'")

        results = connector.search(test_query, top_k=3)

        if results:
            print(f"âœ“ Found {len(results)} results:\n")
            for i, result in enumerate(results, 1):
                print(f"--- Result {i} ---")
                print(f"  Title: {result['title']}")
                print(f"  URL: {result['url']}")
                print(f"  Score: {result['score']:.4f}")
                print(f"  Content (first 200 chars): {result['content'][:200]}...")
                print()
        else:
            print("âš  No results found")

        # ì—°ê²° ì¢…ë£Œ
        connector.close()
        print("[3] PostgresConnector test completed successfully")

    except Exception as e:
        print(f"âœ— Error: {e}")
        sys.exit(1)
