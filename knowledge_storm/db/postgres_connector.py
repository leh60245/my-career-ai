"""
PostgreSQL Vector Search Connector for Enterprise STORM

ì´ ëª¨ë“ˆì€ PostgreSQL DBì˜ Source_Materials í…Œì´ë¸”ì—ì„œ
pgvectorë¥¼ í™œìš©í•œ ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

Database Schema (Source_Materials):
    - id (PK): Integer
    - report_id (FK): Integer (Analysis_Reports ì°¸ì¡°)
    - chunk_type: VARCHAR ('text', 'table', 'noise_merged')
    - section_path: TEXT (ì„¹ì…˜ ê²½ë¡œ)
    - sequence_order: INTEGER (ë¬¸ì„œ ë‚´ ë“±ì¥ ìˆœì„œ)
    - raw_content: TEXT (ë³¸ë¬¸ ë˜ëŠ” Markdown í‘œ)
    - embedding: vector(768) (pgvector)
    - meta_info: JSONB (ë©”íƒ€ë°ì´í„°)
        - has_merged_meta: boolean (ë³‘í•©ëœ ë©”íƒ€ ì •ë³´ í¬í•¨ ì—¬ë¶€)
        - is_noise_dropped: boolean (noise_merged íƒ€ì…ì¼ ë•Œë§Œ ì¡´ì¬)
        - has_embedding: boolean
        - context_injected: boolean
        - length: integer

Author: Enterprise STORM Team
Updated: 2026-01-11 - í†µí•© ì•„í‚¤í…ì²˜ (src.common ëª¨ë“ˆ ì‚¬ìš©)
"""

import os
import logging
from typing import List, Dict

import numpy as np
import psycopg2
from psycopg2.extras import RealDictCursor

# [í†µí•© ì•„í‚¤í…ì²˜] ê³µí†µ ëª¨ë“ˆ ì‚¬ìš©
try:
    from src.common.db_connection import DBConnectionFactory
    from src.common.embedding import EmbeddingService
    from src.common.config import (
        DB_CONFIG,
        COMPANY_ALIASES,
        get_canonical_company_name,
        get_all_aliases,
    )
    _USE_UNIFIED_MODULES = True
except ImportError:
    # í´ë°±: ê¸°ì¡´ ë°©ì‹ (ë…ë¦½ ì‹¤í–‰ ì‹œ)
    from sentence_transformers import SentenceTransformer
    _USE_UNIFIED_MODULES = False
    # í´ë°±ìš© ê¸°ë³¸ COMPANY_ALIASES
    COMPANY_ALIASES = {
        "ì‚¼ì„±ì „ì": ["ì‚¼ì „", "Samsung Electronics", "Samsung", "ì‚¼ì„±ì „ìãˆœ", "SAMSUNG"],
        "SKí•˜ì´ë‹‰ìŠ¤": ["í•˜ì´ë‹‰ìŠ¤", "SK Hynix", "Hynix", "ì—ìŠ¤ì¼€ì´í•˜ì´ë‹‰ìŠ¤", "SKí•˜ì´ë‹‰ìŠ¤ãˆœ"],
    }
    def get_canonical_company_name(name: str) -> str:
        for canonical, aliases in COMPANY_ALIASES.items():
            if name == canonical or name in aliases:
                return canonical
        return name
    def get_all_aliases(company_name: str) -> list:
        canonical = get_canonical_company_name(company_name)
        if canonical in COMPANY_ALIASES:
            return [canonical] + COMPANY_ALIASES[canonical]
        return [company_name]

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class PostgresConnector:
    """
    PostgreSQL ë²¡í„° ê²€ìƒ‰ ì»¤ë„¥í„°

    DART ë³´ê³ ì„œ ë°ì´í„°ê°€ ì €ì¥ëœ PostgreSQL DBì—ì„œ
    ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³  STORM í˜¸í™˜ í¬ë§·ìœ¼ë¡œ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    Attributes:
        conn: psycopg2 ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ê°ì²´
        model: SentenceTransformer ì„ë² ë”© ëª¨ë¸

    Example:
        >>> connector = PostgresConnector()
        >>> results = connector.search("ì‚¼ì„±ì „ì ë§¤ì¶œ í˜„í™©", top_k=5)
        >>> for r in results:
        ...     print(r['title'], r['score'])
    """

    def __init__(self):
        """
        PostgresConnector ì´ˆê¸°í™”

        [í†µí•© ì•„í‚¤í…ì²˜] src.common ëª¨ë“ˆ ì‚¬ìš© ê°€ëŠ¥ ì‹œ í†µí•© DB ì—°ê²° ë° ì„ë² ë”© ì„œë¹„ìŠ¤ ì‚¬ìš©.
        ë…ë¦½ ì‹¤í–‰ ì‹œ ê¸°ì¡´ í™˜ê²½ë³€ìˆ˜ ë°©ì‹ìœ¼ë¡œ í´ë°±.

        Required Environment Variables (í´ë°± ì‹œ):
            - PG_HOST: PostgreSQL í˜¸ìŠ¤íŠ¸ ì£¼ì†Œ
            - PG_PORT: PostgreSQL í¬íŠ¸ (ê¸°ë³¸ê°’: 5432)
            - PG_USER: ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš©ìëª…
            - PG_PASSWORD: ë°ì´í„°ë² ì´ìŠ¤ ë¹„ë°€ë²ˆí˜¸
            - PG_DATABASE: ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„

        Raises:
            RuntimeError: DB ì—°ê²° ì •ë³´ê°€ ëˆ„ë½ë˜ì—ˆê±°ë‚˜ ì—°ê²°ì— ì‹¤íŒ¨í•œ ê²½ìš°
        """
        if _USE_UNIFIED_MODULES:
            # [í†µí•© ì•„í‚¤í…ì²˜] ê³µí†µ ëª¨ë“ˆ ì‚¬ìš©
            try:
                factory = DBConnectionFactory()
                self.conn = factory.create_connection()
                logger.info(f"Successfully connected via unified DBConnectionFactory")

                # [ì•ˆì „ì¥ì¹˜] ì°¨ì› ê²€ì¦ ë¨¼ì € ì‹¤í–‰
                try:
                    from src.common.config import validate_embedding_dimension_compatibility
                    validate_embedding_dimension_compatibility()
                except Exception as e:
                    logger.error(f"âŒ Dimension validation failed: {e}")
                    raise

                # í†µí•© ì„ë² ë”© ì„œë¹„ìŠ¤ ì‚¬ìš©
                self._embedding_service = EmbeddingService(validate_dimension=False)  # ì´ë¯¸ ê²€ì¦í–ˆìœ¼ë¯€ë¡œ skip
                self.model = None  # ë ˆê±°ì‹œ í˜¸í™˜
                logger.info(f"Using unified EmbeddingService (provider: {self._embedding_service.provider})")
            except Exception as e:
                raise RuntimeError(f"Failed to initialize via unified modules: {e}")
        else:
            # [í´ë°±] ê¸°ì¡´ í™˜ê²½ë³€ìˆ˜ ë°©ì‹
            self._embedding_service = None

            # í™˜ê²½ë³€ìˆ˜ì—ì„œ DB ì ‘ì† ì •ë³´ ë¡œë“œ
            self.host = os.environ.get("PG_HOST")
            self.port = os.environ.get("PG_PORT", "5432")
            self.user = os.environ.get("PG_USER")
            self.password = os.environ.get("PG_PASSWORD")
            self.database = os.environ.get("PG_DATABASE")

            # í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ ê²€ì¦
            missing_vars = []
            if not self.host:
                missing_vars.append("PG_HOST")
            if not self.user:
                missing_vars.append("PG_USER")
            if not self.password:
                missing_vars.append("PG_PASSWORD")
            if not self.database:
                missing_vars.append("PG_DATABASE")

            if missing_vars:
                raise RuntimeError(
                    f"Missing required environment variables: {', '.join(missing_vars)}. "
                    "Please set them in secrets.toml or as environment variables."
                )

            # DB ì—°ê²° ì„¤ì •
            try:
                self.conn = psycopg2.connect(
                    host=self.host,
                    port=self.port,
                    user=self.user,
                    password=self.password,
                    database=self.database
                )
                logger.info(f"Successfully connected to PostgreSQL database: {self.database}")
            except psycopg2.Error as e:
                raise RuntimeError(f"Failed to connect to PostgreSQL: {e}")

            # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (í´ë°±)
            try:
                from sentence_transformers import SentenceTransformer
                self.model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')
                logger.info("Successfully loaded SentenceTransformer model (fallback mode)")
            except Exception as e:
                raise RuntimeError(f"Failed to load SentenceTransformer model: {e}")

    def _extract_target_entities(self, query: str) -> List[str]:
        """
        ì¿¼ë¦¬ì—ì„œ íƒ€ê²Ÿ ê¸°ì—…ëª…(Entity) ì¶”ì¶œ

        COMPANY_ALIASESë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¿¼ë¦¬ ë‚´ ê¸°ì—…ëª…ì„ ì‹ë³„í•˜ê³ ,
        í•´ë‹¹ ê¸°ì—…ì˜ ëª¨ë“  ë³„ì¹­ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬ ë¬¸ìì—´

        Returns:
            ì‹ë³„ëœ ê¸°ì—…ì˜ ëª¨ë“  ì•Œë ¤ì§„ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ (ë³„ì¹­ í¬í•¨)
            ì˜ˆ: ["SKí•˜ì´ë‹‰ìŠ¤", "í•˜ì´ë‹‰ìŠ¤", "SK Hynix", ...]

        Example:
            >>> self._extract_target_entities("SKí•˜ì´ë‹‰ìŠ¤ ë§¤ì¶œ í˜„í™©")
            ["SKí•˜ì´ë‹‰ìŠ¤", "í•˜ì´ë‹‰ìŠ¤", "SK Hynix", ...]
        """
        target_keywords = []

        # ëª¨ë“  ê¸°ì—…ì˜ ì •ê·œëª…ê³¼ ë³„ì¹­ì„ ìˆœíšŒí•˜ë©° ì¿¼ë¦¬ì— í¬í•¨ëœ ê¸°ì—… ì°¾ê¸°
        for canonical, aliases in COMPANY_ALIASES.items():
            all_names = [canonical] + aliases

            for name in all_names:
                if name.lower() in query.lower():
                    # ë§¤ì¹­ëœ ê¸°ì—…ì˜ ëª¨ë“  ë³„ì¹­ ë°˜í™˜
                    target_keywords = get_all_aliases(canonical)
                    logger.debug(f"[Entity Extraction] Found entity '{canonical}' in query, aliases: {target_keywords}")
                    return target_keywords

        logger.debug(f"[Entity Extraction] No known company entity found in query: {query}")
        return target_keywords

    def _classify_query_intent(self, query: str) -> str:
        """
        ì§ˆë¬¸ ì˜ë„ ë¶„ë¥˜: Factoid vs Analytical

        Rule-based í‚¤ì›Œë“œ ë§¤ì¹­ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì„ ë¶„ë¥˜í•©ë‹ˆë‹¤.

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬ ë¬¸ìì—´

        Returns:
            "factoid" | "analytical"

        Classification Logic:
        - Factoid: ë‹¨ìˆœ ì‚¬ì‹¤ í™•ì¸ (ì„¤ë¦½ì¼, ì£¼ì†Œ, ëŒ€í‘œ, ì „í™”ë²ˆí˜¸ ë“±)
        - Analytical: ë¹„êµ/ë¶„ì„ ì •ë³´ (ì ìœ ìœ¨, ìˆœìœ„, ì „ë§, SWOT, ê²½ìŸ ë“±)

        Example:
            >>> self._classify_query_intent("SKí•˜ì´ë‹‰ìŠ¤ ì„¤ë¦½ì¼")
            "factoid"
            >>> self._classify_query_intent("ì‚¼ì„±ì „ì ëŒ€ë¹„ ì‹œì¥ ì ìœ ìœ¨")
            "analytical"
        """
        query_lower = query.lower()

        # Analytical Keywords (ìš°ì„  ê²€ì‚¬ - ë” êµ¬ì²´ì )
        analytical_keywords = [
            # ë¹„êµ/ê²½ìŸ
            "ë¹„êµ", "ëŒ€ë¹„", "vs", "ê²½ìŸ", "ê²½ìŸì‚¬",
            # ë¶„ì„
            "ë¶„ì„", "swot", "ì „ë§", "ì¶”ì„¸", "ë™í–¥", "ì „ëµ",
            # ì‹œì¥/ìˆœìœ„
            "ì ìœ ìœ¨", "ìˆœìœ„", "ë­í‚¹", "ìœ„ì¹˜", "ì…ì§€",
            # ì¬ë¬´ ë¶„ì„
            "ì„±ì¥ë¥ ", "ìˆ˜ìµì„±", "ì•ˆì •ì„±", "íš¨ìœ¨ì„±",
            # ê°•ì /ì•½ì 
            "ê°•ì ", "ì•½ì ", "ê¸°íšŒ", "ìœ„í˜‘",
        ]

        for keyword in analytical_keywords:
            if keyword in query_lower:
                logger.debug(f"[Intent] Classified as ANALYTICAL (keyword: '{keyword}')")
                return "analytical"

        # Factoid Keywords
        factoid_keywords = [
            # ê¸°ë³¸ ì •ë³´
            "ì„¤ë¦½", "ì„¤ë¦½ì¼", "ì°½ë¦½", "ì£¼ì†Œ", "ìœ„ì¹˜", "ë³¸ì‚¬",
            # ì¸ë¬¼
            "ëŒ€í‘œ", "ëŒ€í‘œì´ì‚¬", "ceo", "ì„ì›", "ì´ì‚¬",
            # ì—°ë½ì²˜
            "ì „í™”", "ì „í™”ë²ˆí˜¸", "íŒ©ìŠ¤", "ì´ë©”ì¼", "ì—°ë½ì²˜",
            # ì£¼ì£¼/ì§€ë¶„
            "ì£¼ì£¼", "ì§€ë¶„", "ì†Œìœ ", "ìµœëŒ€ì£¼ì£¼",
            # ë‹¨ìˆœ ê°œìš”
            "ê°œìš”", "ì†Œê°œ", "íšŒì‚¬ëª…", "ë²•ì ", "ìƒí˜¸",
        ]

        for keyword in factoid_keywords:
            if keyword in query_lower:
                logger.debug(f"[Intent] Classified as FACTOID (keyword: '{keyword}')")
                return "factoid"

        # ê¸°ë³¸ê°’: Analytical (ë³´ìˆ˜ì  ì ‘ê·¼ - ì •ë³´ ì†ì‹¤ ë°©ì§€)
        logger.debug(f"[Intent] No specific keywords found, defaulting to ANALYTICAL")
        return "analytical"

    def _rerank_by_entity_match(
        self,
        query: str,
        results: List[Dict],
        boost_multiplier: float = 1.3,
        penalty_multiplier: float = 0.5,
        drop_unmatched_tables: bool = True,
        enable_dual_filter: bool = True
    ) -> List[Dict]:
        """
        Entity ë§¤ì¹­ ê¸°ë°˜ ê²°ê³¼ ë¦¬ë­í‚¹ + Dual Filtering

        [FEAT-002 ì¶”ê°€] ì§ˆë¬¸ ì˜ë„(Factoid vs Analytical)ì— ë”°ë¼ í•„í„°ë§ ê°•ë„ ì¡°ì ˆ
        - Factoid: Strict Filter (Entity ë¶ˆì¼ì¹˜ ì‹œ DROP)
        - Analytical: Relaxed Filter (Entity ë¶ˆì¼ì¹˜ ì‹œ Penaltyë§Œ)

        í•µì‹¬ ë¡œì§:
        - ë§¤ì¹­ ì‹œ: ì ìˆ˜ Ã— boost_multiplier (ê°€ì‚°ì )
        - ë¶ˆì¼ì¹˜ + Factoid: DROP (ì˜¤ë‹µ ë°©ì§€)
        - ë¶ˆì¼ì¹˜ + Analytical: ì ìˆ˜ Ã— penalty_multiplier (ì •ë³´ ë³´ì¡´)

        Args:
            query: ì›ë³¸ ê²€ìƒ‰ ì¿¼ë¦¬
            results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (STORM í¬ë§·)
            boost_multiplier: ë§¤ì¹­ ì‹œ ì ìˆ˜ ë°°ìœ¨ (ê¸°ë³¸ê°’: 1.3)
            penalty_multiplier: ë¶ˆì¼ì¹˜ ì‹œ ì ìˆ˜ ë°°ìœ¨ (ê¸°ë³¸ê°’: 0.5)
            drop_unmatched_tables: Table íƒ€ì… ë¶ˆì¼ì¹˜ ì²­í¬ ë“œë¡­ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
            enable_dual_filter: Dual Filtering í™œì„±í™” ì—¬ë¶€ (ê¸°ë³¸ê°’: True)

        Returns:
            ìŠ¤ì½”ì–´ê°€ ì¡°ì •ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (ì •ë ¬ë¨)
        """
        # 1. ì¿¼ë¦¬ì—ì„œ íƒ€ê²Ÿ Entity ì¶”ì¶œ
        target_keywords = self._extract_target_entities(query)

        if not target_keywords:
            logger.info("[Rerank] No target entity found in query - skipping reranking")
            return results

        # 2. ì§ˆë¬¸ ì˜ë„ ë¶„ë¥˜ (Dual Filter)
        query_intent = "analytical"  # ê¸°ë³¸ê°’
        if enable_dual_filter:
            query_intent = self._classify_query_intent(query)
            logger.info(f"[Dual Filter] Query intent: {query_intent.upper()}")

        logger.info(f"[Rerank] Target entities for matching: {target_keywords[:3]}...")

        reranked_results = []
        dropped_count = 0

        for doc in results:
            # 3. ë©”íƒ€ë°ì´í„° ê²°í•© (title + contentì˜ ì¼ë¶€)
            doc_title = doc.get('title', '')
            doc_content = doc.get('content', '')[:500]
            doc_meta = f"{doc_title} {doc_content}".lower()

            # 3.5. [FIX-Search-002] company_name ëˆ„ë½ ì‹œ PASS (Loose Matching)
            # Efficient ëª¨ë“œë¡œ ì ì¬ëœ ë°ì´í„°ì— company_nameì´ ì—†ì„ ìˆ˜ ìˆìŒ
            doc_company_name = doc.get('_company_name', '')
            if not doc_company_name or doc_company_name == 'Unknown Company':
                # ë©”íƒ€ë°ì´í„°ì— company_nameì´ ì—†ìœ¼ë©´ í•„í„°ë§ ìš°íšŒ (ë°ì´í„° ì‚´ë¦¬ê¸°)
                doc['score'] = doc.get('score', 0)  # ì ìˆ˜ ìœ ì§€
                doc['_entity_match'] = None  # ë§¤ì¹­ ì—¬ë¶€ ë¶ˆëª…
                logger.debug(f"[Rerank] PASS (no company_name in meta_info): {doc.get('url', 'unknown')[:40]}...")
                reranked_results.append(doc)
                continue

            # 4. ë§¤ì¹­ ì—¬ë¶€ í™•ì¸ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
            is_matched = any(keyword.lower() in doc_meta for keyword in target_keywords)

            # 5. chunk_type í™•ì¸
            is_table_chunk = "[í‘œ ë°ì´í„°]" in doc.get('content', '')

            # 6. ìŠ¤ì½”ì–´ ì¡°ì • (Dual Filtering ì ìš©)
            original_score = doc.get('score', 0)

            if is_matched:
                # âœ… MATCH: ê°€ì‚°ì 
                doc['score'] = original_score * boost_multiplier
                doc['_entity_match'] = True
                logger.debug(f"[Rerank] MATCH: {doc.get('url', 'unknown')[:40]}... | "
                           f"Score: {original_score:.4f} â†’ {doc['score']:.4f}")
                reranked_results.append(doc)

            else:
                # âŒ NO MATCH: ì˜ë„ì— ë”°ë¼ ì²˜ë¦¬

                # Case 1: Factoid ì§ˆë¬¸ â†’ Strict Filter (DROP)
                if query_intent == "factoid":
                    dropped_count += 1
                    logger.debug(f"[Rerank] DROP (factoid + unmatched): {doc.get('url', 'unknown')[:40]}...")
                    continue

                # Case 2: Analytical ì§ˆë¬¸ â†’ Relaxed Filter
                # Table ì²­í¬ëŠ” ì—¬ì „íˆ ë“œë¡­, TextëŠ” í˜ë„í‹°ë§Œ
                if is_table_chunk and drop_unmatched_tables:
                    dropped_count += 1
                    logger.debug(f"[Rerank] DROP (analytical + unmatched table): {doc.get('url', 'unknown')[:40]}...")
                    continue

                # Text ì²­í¬ëŠ” í˜ë„í‹° ë¶€ì—¬ í›„ ìœ ì§€
                doc['score'] = original_score * penalty_multiplier
                doc['_entity_match'] = False
                logger.debug(f"[Rerank] PENALTY (analytical + unmatched text): {doc.get('url', 'unknown')[:40]}... | "
                           f"Score: {original_score:.4f} â†’ {doc['score']:.4f}")
                reranked_results.append(doc)

        # 7. ì ìˆ˜ìˆœ ì¬ì •ë ¬
        reranked_results.sort(key=lambda x: x.get('score', 0), reverse=True)

        logger.info(f"[Rerank] Completed: {len(reranked_results)} kept, {dropped_count} dropped (intent: {query_intent})")

        return reranked_results

    def _apply_source_tagging(self, results: List[Dict], enable: bool = True) -> List[Dict]:
        """
        Source Tagging: ì²­í¬ contentì— ì¶œì²˜ í—¤ë” ë¬¼ë¦¬ì  ì£¼ì…

        [FEAT-002] LLMì´ ì •ë³´ì˜ ì£¼ì²´ë¥¼ ëª…í™•íˆ êµ¬ë¶„í•  ìˆ˜ ìˆë„ë¡
        ê° ì²­í¬ì˜ ë§¨ ì•ì— [[ì¶œì²˜: íšŒì‚¬ëª…]] íƒœê·¸ë¥¼ ì‚½ì…í•©ë‹ˆë‹¤.

        Args:
            results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            enable: Source Tagging í™œì„±í™” ì—¬ë¶€ (ê¸°ë³¸ê°’: True)

        Returns:
            ì¶œì²˜ í—¤ë”ê°€ ì¶”ê°€ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸

        Example:
            Before: "ë‹¹ì‚¬ëŠ” 1949ë…„ì— ì„¤ë¦½ë˜ì—ˆìŠµë‹ˆë‹¤..."
            After:  "[[ì¶œì²˜: SKí•˜ì´ë‹‰ìŠ¤ ì‚¬ì—…ë³´ê³ ì„œ]]\në‹¹ì‚¬ëŠ” 1949ë…„ì— ì„¤ë¦½ë˜ì—ˆìŠµë‹ˆë‹¤..."
        """
        if not enable:
            return results

        tagged_results = []
        for doc in results:
            # ë©”íƒ€ë°ì´í„°ì—ì„œ ì¶œì²˜ ì •ë³´ ì¶”ì¶œ
            company_name = doc.get('_company_name', 'Unknown Company')
            report_id = doc.get('_report_id', 'N/A')

            # ì¶œì²˜ í—¤ë” ìƒì„±
            source_tag = f"[[ì¶œì²˜: {company_name} ì‚¬ì—…ë³´ê³ ì„œ (Report ID: {report_id})]]"

            # content ë§¨ ì•ì— ì¶œì²˜ í—¤ë” ì£¼ì…
            original_content = doc.get('content', '')
            doc['content'] = f"{source_tag}\n\n{original_content}"

            # ë‚´ë¶€ ë©”íƒ€ë°ì´í„°ëŠ” ì œê±° (LLMì—ê²Œ ì „ë‹¬ ë¶ˆí•„ìš”)
            doc.pop('_company_name', None)
            doc.pop('_report_id', None)

            tagged_results.append(doc)

            logger.debug(f"[Source Tag] Applied to {doc.get('url', 'unknown')[:40]}... | Company: {company_name}")

        logger.info(f"[Source Tag] Applied source tags to {len(tagged_results)} chunks")
        return tagged_results

    def _embed_query(self, query: str) -> np.ndarray:
        """
        ì¿¼ë¦¬ ë¬¸ìì—´ì„ ë²¡í„°ë¡œ ë³€í™˜

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬ ë¬¸ìì—´

        Returns:
            ì„ë² ë”© ë²¡í„° (numpy ë°°ì—´)
        """
        if self._embedding_service is not None:
            # [í†µí•© ì•„í‚¤í…ì²˜] ê³µí†µ ì„ë² ë”© ì„œë¹„ìŠ¤ ì‚¬ìš©
            embedding = self._embedding_service.embed_text(query)
            return np.array(embedding)
        else:
            # [í´ë°±] SentenceTransformer ì§ì ‘ ì‚¬ìš©
            embedding = self.model.encode(query, convert_to_numpy=True)
            return embedding

    def _fetch_window_context(
            self,
            table_rows: List[Dict],
            window_size: int = 1
    ) -> Dict[tuple, Dict[str, str]]:
        """
        í…Œì´ë¸” íƒ€ì… í–‰ë“¤ì— ëŒ€í•´ Sliding Window Contextë¥¼ ì¡°íšŒ

        sequence_order ê¸°ì¤€ìœ¼ë¡œ ì•ë’¤ window_sizeë§Œí¼ì˜ ì¸ì ‘ ì²­í¬ë¥¼ ê°€ì ¸ì™€
        í•˜ë‚˜ì˜ Context Blockìœ¼ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.

        Note:
            DB ì—…ë°ì´íŠ¸ë¡œ ì¸í•´ noise_merged íƒ€ì…ì¸ ì²­í¬ëŠ” ê²€ìƒ‰ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ,
            Sequenceê°€ ë¹„ì–´ìˆì„ ê²½ìš° ìë™ìœ¼ë¡œ ê±´ë„ˆë›°ê²Œ ë©ë‹ˆë‹¤.

        Args:
            table_rows: chunk_type='table'ì¸ ê²€ìƒ‰ ê²°ê³¼ í–‰ë“¤
            window_size: ì•ë’¤ë¡œ ê°€ì ¸ì˜¬ ì²­í¬ ìˆ˜ (ê¸°ë³¸ê°’: 1)

        Returns:
            {(report_id, sequence_order): {'prev': prev_text, 'next': next_text}} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
        """
        if not table_rows:
            return {}

        context_map = {}

        with self.conn.cursor(cursor_factory=RealDictCursor) as cur:
            for row in table_rows:
                report_id = row['report_id']
                current_seq = row['sequence_order']

                context_data = {'prev': None, 'next': None}

                # ì´ì „ ì²­í¬ ì¡°íšŒ (sequence_order - 1 ~ sequence_order - window_size)
                for offset in range(1, window_size + 1):
                    prev_seq = current_seq - offset
                    if prev_seq < 0:
                        continue

                    cur.execute("""
                        SELECT raw_content, section_path, chunk_type
                        FROM "Source_Materials"
                        WHERE report_id = %s AND sequence_order = %s
                    """, (report_id, prev_seq))

                    prev_row = cur.fetchone()
                    if prev_row and prev_row['chunk_type'] != 'noise_merged':
                        if context_data['prev'] is None:
                            context_data['prev'] = prev_row['raw_content']
                        else:
                            # ë” ì•ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì•ì— ë¶™ì„
                            context_data['prev'] = prev_row['raw_content'] + "\n\n" + context_data['prev']

                # ë‹¤ìŒ ì²­í¬ ì¡°íšŒ (sequence_order + 1 ~ sequence_order + window_size)
                for offset in range(1, window_size + 1):
                    next_seq = current_seq + offset

                    cur.execute("""
                        SELECT raw_content, section_path, chunk_type
                        FROM "Source_Materials"
                        WHERE report_id = %s AND sequence_order = %s
                    """, (report_id, next_seq))

                    next_row = cur.fetchone()
                    if next_row and next_row['chunk_type'] != 'noise_merged':
                        if context_data['next'] is None:
                            context_data['next'] = next_row['raw_content']
                        else:
                            # ë” ë’¤ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë’¤ì— ë¶™ì„
                            context_data['next'] = str(context_data['next']) + "\n\n" + next_row['raw_content']

                context_map[(report_id, current_seq)] = context_data

        return context_map

    def search(
        self,
        query: str,
        top_k: int = 5,
        window_size: int = 1,
        company_filter: str = None,
        company_filter_list: List[str] = None
    ) -> List[Dict]:
        """
        ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰ (ê¸°ì—…ëª… í•„í„°ë§ ì§€ì›)

        ì…ë ¥ëœ ì¿¼ë¦¬ë¥¼ ë²¡í„°í™”í•˜ì—¬ PostgreSQLì˜ Source_Materials í…Œì´ë¸”ì—ì„œ
        ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œë“¤ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.

        ê¸°ì—…ëª… í•„í„°ë§:
        - company_filter: ë‹¨ì¼ ê¸°ì—…ëª… í•„í„° (ê¸°ë³¸ ëª¨ë“œ)
        - company_filter_list: ë³µìˆ˜ ê¸°ì—…ëª… í•„í„° (ë¹„êµ ë¶„ì„ ëª¨ë“œ)
        - ë‘˜ ë‹¤ Noneì´ë©´ ì „ì²´ ê²€ìƒ‰ (í•„í„° ì—†ìŒ)

        chunk_typeì´ 'table'ì¸ ê²½ìš° Sliding Window Contextë¥¼ ì ìš©í•˜ì—¬
        ì•ë’¤ ì¸ì ‘ ì²­í¬ë¥¼ í•¨ê»˜ ê°€ì ¸ì™€ í•˜ë‚˜ì˜ Context Blockìœ¼ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.

        has_merged_metaê°€ trueì¸ ê²½ìš° LLMì—ê²Œ ë³‘í•©ëœ ë©”íƒ€ ì •ë³´(ë‹¨ìœ„, ë²”ë¡€ ë“±)ê°€
        ë¬¸ë‹¨ ëì— í¬í•¨ë˜ì–´ ìˆìŒì„ ì•Œë¦¬ëŠ” ì•ˆë‚´ ë¬¸êµ¬ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬ ë¬¸ìì—´
            top_k: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜ (ê¸°ë³¸ê°’: 5)
            window_size: Table ì²­í¬ì˜ ì•ë’¤ë¡œ ê°€ì ¸ì˜¬ ì¸ì ‘ ì²­í¬ ìˆ˜ (ê¸°ë³¸ê°’: 1)
            company_filter: ë‹¨ì¼ ê¸°ì—…ëª… í•„í„° (meta_info->>'company_name' = ?)
            company_filter_list: ë³µìˆ˜ ê¸°ì—…ëª… í•„í„° (meta_info->>'company_name' IN (?))

        Returns:
            STORM í˜¸í™˜ í¬ë§·ì˜ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            [
                {
                    "content": "ê²€ìƒ‰ëœ ë³¸ë¬¸ ë‚´ìš©",
                    "title": "ì„¹ì…˜ ê²½ë¡œ (section_path)",
                    "url": "dart_report_{report_id}",
                    "score": 0.85,  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (1 - distance)
                    "has_merged_meta": true/false  # ë³‘í•©ëœ ë©”íƒ€ ì •ë³´ í¬í•¨ ì—¬ë¶€
                },
                ...
            ]

        Raises:
            psycopg2.Error: ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ì‹¤í–‰ ì‹¤íŒ¨ ì‹œ
        """
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        query_embedding = self._embed_query(query)

        # numpy ë°°ì—´ì„ PostgreSQL vector í˜•ì‹ ë¬¸ìì—´ë¡œ ë³€í™˜
        embedding_str = "[" + ",".join(map(str, query_embedding.tolist())) + "]"

        results = []

        # [FIX-Search-002] ê¸°ì—…ëª… í•„í„° ì¡°ê±´ ìƒì„±
        # ë©”íƒ€ë°ì´í„°ê°€ ì•„ë‹Œ Companies í…Œì´ë¸” JOINìœ¼ë¡œ ê¸°ì—…ëª… ì¡°íšŒ (efficient ëª¨ë“œ í˜¸í™˜)
        company_condition = ""
        query_params = [embedding_str]

        if company_filter_list and len(company_filter_list) > 0:
            # ë³µìˆ˜ ê¸°ì—… í•„í„° (ë¹„êµ ë¶„ì„ ëª¨ë“œ)
            placeholders = ", ".join(["%s"] * len(company_filter_list))
            company_condition = f"AND c.company_name IN ({placeholders})"
            query_params.extend(company_filter_list)
            logger.info(f"[Filter] Searching with company_filter_list: {company_filter_list}")
        elif company_filter:
            # ë‹¨ì¼ ê¸°ì—… í•„í„° (ê¸°ë³¸ ëª¨ë“œ)
            company_condition = "AND c.company_name = %s"
            query_params.append(company_filter)
            logger.info(f"[Filter] Searching with company_filter: {company_filter}")
        else:
            logger.info("[Filter] No company filter applied - searching all documents")

        query_params.append(top_k)

        try:
            with self.conn.cursor(cursor_factory=RealDictCursor) as cur:
                # [FIX-Search-002] JOIN ê¸°ë°˜ ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ SQL
                # - Source_Materials â†’ Analysis_Reports â†’ Companies JOIN
                # - ë©”íƒ€ë°ì´í„°ì— company_nameì´ ì—†ì–´ë„ Companies í…Œì´ë¸”ì—ì„œ ì¡°íšŒ
                # - pgvectorì˜ <=> ì—°ì‚°ì: ì½”ì‚¬ì¸ ê±°ë¦¬ (0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬)
                sql = f"""
                    SELECT 
                        sm.id,
                        sm.raw_content, 
                        sm.section_path, 
                        sm.chunk_type, 
                        sm.report_id, 
                        sm.sequence_order,
                        sm.meta_info,
                        c.company_name as resolved_company_name,
                        COALESCE((sm.meta_info->>'has_merged_meta')::boolean, false) as has_merged_meta,
                        COALESCE((sm.meta_info->>'is_noise_dropped')::boolean, false) as is_noise_dropped,
                        (sm.embedding <=> %s::vector) as distance
                    FROM "Source_Materials" sm
                    JOIN "Analysis_Reports" ar ON sm.report_id = ar.id
                    JOIN "Companies" c ON ar.company_id = c.id
                    WHERE sm.chunk_type != 'noise_merged'
                    {company_condition}
                    ORDER BY distance ASC
                    LIMIT %s
                """

                cur.execute(sql, query_params)

                rows = cur.fetchall()

                if not rows:
                    logger.warning(f"No results found for query: {query}")
                    return []

                # ğŸš¨ is_noise_dropped í”Œë˜ê·¸ ê²€ì¦ (ì •ìƒì ìœ¼ë¡œ í•„í„°ë§ë˜ì—ˆëŠ”ì§€ í™•ì¸)
                noise_dropped_rows = [row for row in rows if row.get('is_noise_dropped', False)]
                if noise_dropped_rows:
                    logger.error(
                        f"[ALERT] {len(noise_dropped_rows)} rows with is_noise_dropped=true found in search results! "
                        "This should not happen - please check the Vector DB indexing."
                    )

                # Sliding Window Context: table íƒ€ì… í–‰ë“¤ì— ëŒ€í•´ ì•ë’¤ ì²­í¬ ì¡°íšŒ
                table_rows = [row for row in rows if row['chunk_type'] == 'table']
                context_map = self._fetch_window_context(table_rows, window_size=window_size)

                # ê²°ê³¼ ê°€ê³µ ë° STORM í¬ë§· ë³€í™˜
                for row in rows:
                    content = row['raw_content']
                    has_merged = row.get('has_merged_meta', False)

                    # chunk_typeì´ 'table'ì¸ ê²½ìš° Sliding Window Context ì ìš©
                    if row['chunk_type'] == 'table':
                        context_key = (row['report_id'], row['sequence_order'])
                        if context_key in context_map:
                            ctx = context_map[context_key]
                            prev_text = ctx.get('prev')
                            next_text = ctx.get('next')

                            # ì•ë’¤ ë¬¸ë§¥ì„ ì¡°í•©í•˜ì—¬ Context Block êµ¬ì„±
                            if prev_text:
                                content = f"[ì´ì „ ë¬¸ë§¥]\n{prev_text}\n\n[í‘œ ë°ì´í„°]\n{content}"
                            else:
                                content = f"[ì„¹ì…˜: {row['section_path']}]\n\n[í‘œ ë°ì´í„°]\n{content}"

                            if next_text:
                                content = f"{content}\n\n[ì´í›„ ë¬¸ë§¥]\n{next_text}"
                        else:
                            # ë¬¸ë§¥ì´ ì—†ìœ¼ë©´ section_pathë¥¼ ë¬¸ë§¥ìœ¼ë¡œ ì‚¬ìš©
                            content = f"[ì„¹ì…˜: {row['section_path']}]\n\n[í‘œ ë°ì´í„°]\n{content}"

                    # has_merged_metaê°€ trueì¸ ê²½ìš° LLM ì•ˆë‚´ ë¬¸êµ¬ ì¶”ê°€
                    if has_merged:
                        content = (
                            "[ì°¸ê³ : ì´ ë¬¸ë‹¨ ëì— ë³‘í•©ëœ ë©”íƒ€ ì •ë³´(ë‹¨ìœ„, ë²”ë¡€, ê¸°ì¤€ì¼ì ë“±)ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. "
                            "ìˆ˜ì¹˜ í•´ì„ ì‹œ ë°˜ë“œì‹œ í™•ì¸í•˜ì„¸ìš”.]\n\n" + content
                        )

                    # ì½”ì‚¬ì¸ ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ ì ìˆ˜ë¡œ ë³€í™˜ (1 - distance)
                    # distanceê°€ 0ì´ë©´ score=1 (ì™„ì „ ì¼ì¹˜)
                    score = 1 - float(row['distance'])

                    # URLì— ê³ ìœ  IDë¥¼ í¬í•¨í•˜ì—¬ ê° ê²€ìƒ‰ ê²°ê³¼ê°€ ë³„ë„ì˜ ì¶œì²˜ë¡œ ì¸ì‹ë˜ë„ë¡ í•¨
                    # í˜•ì‹: dart_report_{report_id}_chunk_{id}
                    unique_url = f"dart_report_{row['report_id']}_chunk_{row['id']}"

                    # [FIX-Search-002] Source Taggingì„ ìœ„í•œ ë©”íƒ€ë°ì´í„° ì¶”ê°€
                    # JOINì—ì„œ ê°€ì ¸ì˜¨ resolved_company_name ìš°ì„  ì‚¬ìš© (efficient ëª¨ë“œ í˜¸í™˜)
                    chunk_meta_info = row.get('meta_info', {}) or {}
                    company_name = row.get('resolved_company_name') or chunk_meta_info.get('company_name', 'Unknown Company')
                    report_id = row['report_id']

                    results.append({
                        "content": content,
                        "title": row['section_path'],
                        "url": unique_url,
                        "score": score,
                        "has_merged_meta": has_merged,
                        # Source Taggingìš© ë©”íƒ€ë°ì´í„°
                        "_company_name": company_name,
                        "_report_id": report_id,
                    })

                logger.info(f"Found {len(results)} results for query: {query}")

                # [FIX-Search-002] ë¹ˆ ê²°ê³¼ í¬ë˜ì‹œ ë°©ì–´
                # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ Reranker í˜¸ì¶œí•˜ì§€ ì•Šê³  ì¦‰ì‹œ ë°˜í™˜
                if not results:
                    logger.warning(f"PostgresRM: Found 0 results for query '{query}'. Skipping rerank.")
                    return []

                # [FEAT-001] Entity Bias ë°©ì§€: Entity ë§¤ì¹­ ê¸°ë°˜ ë¦¬ë­í‚¹ + Dual Filtering
                # - Factoid ì§ˆë¬¸: Entity ë¶ˆì¼ì¹˜ ì‹œ DROP (Strict Filter)
                # - Analytical ì§ˆë¬¸: Entity ë¶ˆì¼ì¹˜ ì‹œ Penalty (Relaxed Filter)
                results = self._rerank_by_entity_match(
                    query=query,
                    results=results,
                    boost_multiplier=1.3,
                    penalty_multiplier=0.5,
                    drop_unmatched_tables=True,
                    enable_dual_filter=True  # [FEAT-002] Dual Filtering í™œì„±í™”
                )

                # [FEAT-002] Source Tagging: ì²­í¬ì— ì¶œì²˜ í—¤ë” ë¬¼ë¦¬ì  ì£¼ì…
                # LLMì´ ì •ë³´ì˜ ì¶œì²˜ë¥¼ ëª…í™•íˆ ì¸ì‹í•˜ë„ë¡ [[ì¶œì²˜: íšŒì‚¬ëª…]] íƒœê·¸ ì¶”ê°€
                results = self._apply_source_tagging(
                    results=results,
                    enable=True  # Source Tagging í™œì„±í™”
                )

                return results

        except psycopg2.Error as e:
            logger.error(f"Database query failed: {e}")
            self.conn.rollback()  # [í•µì‹¬] íŠ¸ëœì­ì…˜ ë³µêµ¬!
            return []  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (í”„ë¡œê·¸ë¨ ì¤‘ë‹¨ ë°©ì§€)

        except Exception as e:
            logger.error(f"Unexpected error in search: {e}")
            return []


    def close(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ"""
        if self.conn:
            self.conn.close()
            logger.info("PostgreSQL connection closed")

    def __enter__(self):
        """Context manager ì§„ì…"""
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager ì¢…ë£Œ ì‹œ ì—°ê²° í•´ì œ"""
        self.close()
        return False


# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    import sys
    import toml

    sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(__file__))))


    def load_api_key(toml_file_path):
        """secrets.tomlì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (í…ŒìŠ¤íŠ¸ìš© ë¡œì»¬ í•¨ìˆ˜)"""
        try:
            with open(toml_file_path, "r") as file:
                data = toml.load(file)
            for key, value in data.items():
                os.environ[key] = str(value)
        except FileNotFoundError:
            print(f"File not found: {toml_file_path}", file=sys.stderr)
        except toml.TomlDecodeError:
            print(f"Error decoding TOML file: {toml_file_path}", file=sys.stderr)


    # secrets.tomlì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
    secrets_path = os.path.join(
        os.path.dirname(os.path.dirname(os.path.dirname(__file__))),
        "secrets.toml"
    )

    if os.path.exists(secrets_path):
        load_api_key(secrets_path)
        print(f"âœ“ Loaded secrets from: {secrets_path}")
    else:
        print(f"âš  secrets.toml not found at: {secrets_path}")
        print("  Please create secrets.toml with PG_HOST, PG_PORT, PG_USER, PG_PASSWORD, PG_DATABASE")
        sys.exit(1)

    try:
        # PostgresConnector ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        print("\n[1] Initializing PostgresConnector...")
        connector = PostgresConnector()
        print("âœ“ PostgresConnector initialized successfully")

        # í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ìˆ˜í–‰
        test_query = "ì‚¼ì„±ì „ì ë§¤ì¶œ í˜„í™©"
        print(f"\n[2] Searching for: '{test_query}'")

        results = connector.search(test_query, top_k=3)

        if results:
            print(f"âœ“ Found {len(results)} results:\n")
            for i, result in enumerate(results, 1):
                print(f"--- Result {i} ---")
                print(f"  Title: {result['title']}")
                print(f"  URL: {result['url']}")
                print(f"  Score: {result['score']:.4f}")
                print(f"  Content (first 200 chars): {result['content'][:200]}...")
                print()
        else:
            print("âš  No results found")

        # ì—°ê²° ì¢…ë£Œ
        connector.close()
        print("[3] PostgresConnector test completed successfully")

    except Exception as e:
        print(f"âœ— Error: {e}")
        sys.exit(1)
