import asyncio
import logging
from collections.abc import Sequence

import torch
from sentence_transformers import CrossEncoder

from src.common import AI_CONFIG
from src.schemas import SearchResult

logger = logging.getLogger(__name__)


class RerankerService:
    def __init__(self, model_name: str | None = None) -> None:
        self.model_name = model_name or AI_CONFIG.get("reranker_model", "BAAI/bge-reranker-v2-m3")
        self.max_length = int(AI_CONFIG.get("reranker_max_length", 1024))
        self.batch_size = int(AI_CONFIG.get("reranker_batch_size", 8))
        self.device = self._get_optimal_device()
        logger.info(f"ğŸ”„ Loading Reranker model: {self.model_name} on {self.device}")

        # [ì„¤ì •] max_length ëª…ì‹œ (BGE-M3ëŠ” ë³´í†µ 8192ê¹Œì§€ ê°€ëŠ¥í•˜ì§€ë§Œ, ë©”ëª¨ë¦¬/ì†ë„ë¥¼ ìœ„í•´ 512~1024 ê¶Œì¥)
        self.model = CrossEncoder(
            model_name_or_path=self.model_name,
            device=self.device,
            max_length=self.max_length,
        )
        logger.info("âœ… Reranker model loaded.")

    def _get_optimal_device(self) -> str:
        forced_device = AI_CONFIG.get("reranker_device")
        if isinstance(forced_device, str) and forced_device:
            return forced_device
        if torch.cuda.is_available():
            return "cuda"
        if torch.backends.mps.is_available():
            return "mps"
        return "cpu"

    async def rerank(self, query: str, docs: Sequence[SearchResult], top_k: int = 10) -> Sequence[SearchResult]:
        """
        ë¹„ë™ê¸° Reranking ë©”ì„œë“œ.
        Heavy Computationì„ ThreadPoolì—ì„œ ì‹¤í–‰í•˜ì—¬ Event Loop Blockingì„ ë°©ì§€í•¨.
        """
        if not docs:
            return []

        # 1. ì…ë ¥ ìŒ ìƒì„±
        pairs = [(query, doc.get("content", "")) for doc in docs]

        # 2. [í•µì‹¬] Blocking ë°©ì§€ë¥¼ ìœ„í•´ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
        loop = asyncio.get_running_loop()

        try:
            # run_in_executorì˜ ì²« ì¸ìê°€ Noneì´ë©´ ê¸°ë³¸ ThreadPoolExecutor ì‚¬ìš©
            scores = await loop.run_in_executor(
                None,
                lambda: self.model.predict(
                    pairs,
                    batch_size=self.batch_size,  # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ ê´€ë¦¬
                    show_progress_bar=False,
                    activation_fn=torch.nn.Sigmoid(),  # [ì¤‘ìš”] Logits -> 0~1 í™•ë¥ ê°’ ë³€í™˜
                ),
            )
        except Exception as e:
            logger.error(f"Reranking failed: {e}. Returning original order.")
            # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ê·¸ëŒ€ë¡œ ë°˜í™˜ (ì„œë¹„ìŠ¤ ì¤‘ë‹¨ ë°©ì§€)
            return list(docs)[:top_k]

        # 3. ì ìˆ˜ ë§¤í•‘ ë° ì •ë ¬ (ì–•ì€ ë³µì‚¬ë³¸ ìƒì„±í•˜ì—¬ ì›ë³¸ ë³´ì¡´)
        reranked_docs = []
        for i, doc in enumerate(docs):
            new_doc = doc.copy()
            new_doc["score"] = float(scores[i])
            reranked_docs.append(new_doc)

        # ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        reranked_docs.sort(key=lambda x: x["score"], reverse=True)

        return reranked_docs[:top_k]
