"""
Context Look-back ì„ë² ë”© ì›Œì»¤

í‘œ(Table) ë°ì´í„°ì˜ ì„ë² ë”© í’ˆì§ˆì„ ë†’ì´ê¸° ìœ„í•´ ì§ì „ í…ìŠ¤íŠ¸ ë¬¸ë§¥ì„ í¬í•¨í•˜ì—¬
ì„ë² ë”©ì„ ìƒì„±í•˜ê³  DBë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.

í•µì‹¬ ë¡œì§:
- í‘œ(table) ë°ì´í„°ëŠ” ê·¸ ìì²´ë§Œìœ¼ë¡œëŠ” ë‹¨ìœ„(Unit)ë‚˜ ê¸°ì¤€ ë‚ ì§œ ì •ë³´ê°€ ë¶€ì¡±í•¨
- ë³´í†µ í‘œ ë°”ë¡œ ìœ„ì— ì„¤ëª… í…ìŠ¤íŠ¸ê°€ ì¡´ì¬í•˜ë¯€ë¡œ, ì´ë¥¼ í•©ì³ì„œ ë²¡í„°í™”
- 'previous_row'ë¥¼ ìºì‹±í•˜ë©° ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬

ì‚¬ìš©ë²•:
    python -m scripts.run_ingestion --embed --batch-size 32
    python -m scripts.run_ingestion --embed --limit 100  # í…ŒìŠ¤íŠ¸ìš©
    python -m scripts.run_ingestion --embed --force      # ê¸°ì¡´ ì„ë² ë”© ì¬ìƒì„±


ë³€ê²½ ì´ë ¥:
    PHASE 3.5: Legacy Code Migration
    - Removed DBManager dependency (Raw SQL removed)
    - Uses VectorSearchService & SourceMaterialRepository via ORM
    - Fully Async implementation
    - [Fixed] Ensures metadata merging (parity with SQL jsonb_set)
"""

import asyncio
import logging
import re
from datetime import datetime
from typing import Dict, List, Optional

from tqdm.asyncio import tqdm

from src.common.embedding import EmbeddingService

# [í†µí•© ì•„í‚¤í…ì²˜]
from src.database import AsyncDatabaseEngine
from src.database.models.source_material import SourceMaterial
from src.database.repositories import SourceMaterialRepository

logger = logging.getLogger(__name__)


class ContextLookbackEmbeddingWorker:
    """
    Context Look-back ë°©ì‹ìœ¼ë¡œ ì„ë² ë”©ì„ ìƒì„±í•˜ëŠ” ì›Œì»¤ í´ë˜ìŠ¤.
    í‘œ(Table) ë°ì´í„°ì˜ ë¬¸ë§¥ì„ ë³´ê°•í•˜ê³  ë…¸ì´ì¦ˆë¥¼ ì œê±°í•©ë‹ˆë‹¤.
    """

    NOISE_KEYWORDS = [
        "ë‹¨ìœ„",
        "Unit",
        "ë²”ë¡€",
        "ì°¸ì¡°",
        "â€»",
        "ì£¼)",
        "(ì£¼)",
        "ì›",
        "ì²œì›",
        "ë°±ë§Œì›",
        "ì–µì›",
        "ì£¼1)",
        "ì£¼2)",
        "(ë‹¨ìœ„",
    ]
    NOISE_TABLE_MAX_ROWS = 2

    def __init__(self, batch_size: int = 32):
        self.batch_size = batch_size
        self._embedding_service: Optional[EmbeddingService] = None
        self.stats = {
            "total": 0,
            "processed": 0,
            "failed": 0,
            "text_count": 0,
            "table_count": 0,
            "table_with_context": 0,
            "noise_tables_merged": 0,
            "start_time": None,
            "end_time": None,
        }

    def _init_generator(self):
        if self._embedding_service is None:
            self._embedding_service = EmbeddingService()
            logger.info(f"   Provider: {self._embedding_service.provider}")

    # ==================== ë…¸ì´ì¦ˆ ê°ì§€ ë¡œì§ ====================
    def _is_noise_table(self, table_content: str) -> bool:
        if not table_content:
            return False
        lines = table_content.strip().split("\n")
        table_rows = [line for line in lines if line.strip().startswith("|")]
        data_rows = [
            row for row in table_rows if not re.match(r"^\|[\s\-:]+\|$", row.strip())
        ]

        if len(data_rows) <= self.NOISE_TABLE_MAX_ROWS:
            for keyword in self.NOISE_KEYWORDS:
                if keyword in table_content:
                    return True

        content_text = re.sub(r"[|\-:]+", " ", table_content)
        words = [w.strip() for w in content_text.split() if len(w.strip()) > 0]
        if not words:
            return False

        keyword_count = sum(
            1 for w in words if any(k in w for k in self.NOISE_KEYWORDS)
        )
        return (keyword_count / len(words)) >= 0.5

    # ==================== DB ì¡°íšŒ (ORM) ====================

    async def fetch_pending_materials(
        self, repo: SourceMaterialRepository, limit: Optional[int], force: bool
    ) -> List[SourceMaterial]:
        """ì„ë² ë”© ëŒ€ìƒ ì¡°íšŒ"""
        from sqlalchemy import select

        stmt = select(SourceMaterial).where(SourceMaterial.chunk_type != "noise_merged")
        if not force:
            stmt = stmt.where(SourceMaterial.embedding == None)

        stmt = stmt.order_by(
            SourceMaterial.report_id.asc(),
            SourceMaterial.sequence_order.asc(),
            SourceMaterial.id.asc(),
        )
        if limit:
            stmt = stmt.limit(limit)

        result = await repo.session.execute(stmt)
        return result.scalars().all()

    async def fetch_previous_row(
        self, repo: SourceMaterialRepository, current: SourceMaterial
    ) -> Optional[SourceMaterial]:
        """ì§ì „ í–‰ ì¡°íšŒ"""
        from sqlalchemy import select, desc

        stmt = (
            select(SourceMaterial)
            .where(
                SourceMaterial.report_id == current.report_id,
                SourceMaterial.sequence_order < current.sequence_order,
            )
            .order_by(desc(SourceMaterial.sequence_order))
            .limit(1)
        )
        result = await repo.session.execute(stmt)
        return result.scalar_one_or_none()

    # ==================== ë°°ì¹˜ ì²˜ë¦¬ (í•µì‹¬ ë¡œì§) ====================

    async def process_batch(
        self,
        repo: SourceMaterialRepository,
        batch: List[SourceMaterial],
        prev_cache: Dict[int, SourceMaterial],
    ):
        texts_to_embed = []
        # (material_id, has_context, existing_meta)
        embed_targets = []

        for current in batch:
            prev = prev_cache.get(current.report_id)
            if not prev or prev.sequence_order != current.sequence_order - 1:
                prev = await self.fetch_previous_row(repo, current)

            # [CASE 1] ë…¸ì´ì¦ˆ í…Œì´ë¸” ì²˜ë¦¬
            if (
                current.chunk_type == "table"
                and self._is_noise_table(current.raw_content)
                and prev
            ):
                # 1. Previousì— ë‚´ìš© ë³‘í•© (Python ë¬¸ìì—´ ì—°ì‚°)
                merged_content = (
                    (prev.raw_content or "")
                    + "\n\n[ì°¸ì¡° ì •ë³´]\n"
                    + (current.raw_content or "")
                )

                # ë©”íƒ€ë°ì´í„° ë³‘í•© (ê¸°ì¡´ ìœ ì§€ + í”Œë˜ê·¸ ì¶”ê°€)
                prev_meta = prev.meta_info.copy() if prev.meta_info else {}
                prev_meta["has_merged_meta"] = True

                await repo.update(
                    prev.id,
                    {
                        "raw_content": merged_content,
                        "embedding": None,  # ì¬ì„ë² ë”© ìœ ë„
                        "meta_info": prev_meta,
                    },
                )

                # 2. CurrentëŠ” Drop ì²˜ë¦¬
                curr_meta = current.meta_info.copy() if current.meta_info else {}
                curr_meta["is_noise_dropped"] = True

                await repo.update(
                    current.id,
                    {
                        "chunk_type": "noise_merged",
                        "embedding": None,
                        "meta_info": curr_meta,
                    },
                )

                # 3. Previous ì¬ì„ë² ë”© ì˜ˆì•½
                path = prev.section_path or "ì•Œ ìˆ˜ ì—†ìŒ"
                texts_to_embed.append(f"ë¬¸ì„œ ê²½ë¡œ: {path}\n{merged_content}")
                embed_targets.append((prev.id, False, prev_meta))

                # ìºì‹œ ê°±ì‹  (ë©”ëª¨ë¦¬ ìƒ ê°ì²´ë„ ì—…ë°ì´íŠ¸)
                prev.raw_content = merged_content
                prev_cache[current.report_id] = prev
                self.stats["noise_tables_merged"] += 1
                continue

            # [CASE 2] ì¼ë°˜ ì²˜ë¦¬
            section = current.section_path or "ì•Œ ìˆ˜ ì—†ìŒ"
            raw = current.raw_content or ""
            has_ctx = False

            # ë¬¸ë§¥ ì£¼ì… ì¡°ê±´
            if (
                current.chunk_type == "table"
                and prev
                and prev.chunk_type == "text"
                and prev.section_path == current.section_path
            ):
                ctx = (
                    prev.raw_content[:500] + "..."
                    if len(prev.raw_content or "") > 500
                    else prev.raw_content
                )
                text = f"ë¬¸ì„œ ê²½ë¡œ: {section}\n[ë¬¸ë§¥ ì„¤ëª…: {ctx}]\n[í‘œ ë°ì´í„°]\n{raw}"
                has_ctx = True
            else:
                text = f"ë¬¸ì„œ ê²½ë¡œ: {section}\n{raw}"

            texts_to_embed.append(text)

            # ë©”íƒ€ë°ì´í„° ì¤€ë¹„ (ê¸°ì¡´ ë°ì´í„° ë¡œë“œ)
            current_meta = current.meta_info.copy() if current.meta_info else {}
            embed_targets.append((current.id, has_ctx, current_meta))

            prev_cache[current.report_id] = current

            if current.chunk_type == "text":
                self.stats["text_count"] += 1
            else:
                self.stats["table_count"] += 1
                if has_ctx:
                    self.stats["table_with_context"] += 1

        # [ì„ë² ë”© ì¼ê´„ ìƒì„±]
        if texts_to_embed:
            embeddings = self._embedding_service.embed_texts(texts_to_embed)

            for (mid, has_ctx, meta), vec in zip(embed_targets, embeddings):
                # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ (ë©”ëª¨ë¦¬ì—ì„œ ë³‘í•©ëœ dict ì‚¬ìš©)
                meta["has_embedding"] = True
                meta["context_injected"] = has_ctx

                # DB ì—…ë°ì´íŠ¸
                await repo.update(mid, {"embedding": vec, "meta_info": meta})

        return prev_cache

    # ==================== ì‹¤í–‰ (Async) ====================

    async def run_async(self, limit: Optional[int], force: bool):
        self.stats["start_time"] = datetime.now()
        logger.info("ğŸš€ Embedding Worker Started (Async/ORM)")

        self._init_generator()
        db_engine = AsyncDatabaseEngine()

        async with db_engine.get_session() as session:
            repo = SourceMaterialRepository(session)

            pending = await self.fetch_pending_materials(repo, limit, force)
            self.stats["total"] = len(pending)
            logger.info(f"ğŸ“‹ Targets: {len(pending)}")

            if not pending:
                return self.stats

            batches = [
                pending[i : i + self.batch_size]
                for i in range(0, len(pending), self.batch_size)
            ]
            prev_cache = {}

            for batch in tqdm(batches, desc="Embedding..."):
                try:
                    prev_cache = await self.process_batch(repo, batch, prev_cache)
                    await session.commit()
                    self.stats["processed"] += len(batch)
                except Exception as e:
                    await session.rollback()
                    logger.error(f"Batch failed: {e}")
                    self.stats["failed"] += len(batch)

        await db_engine.dispose()
        self.stats["end_time"] = datetime.now()
        self._print_summary()
        return self.stats

    def _print_summary(self):
        duration = self.stats["end_time"] - self.stats["start_time"]
        logger.info(f"ğŸ Finished in {duration}")
        logger.info(f"   Success: {self.stats['processed']}/{self.stats['total']}")
        logger.info(f"   Merged Noise Tables: {self.stats['noise_tables_merged']}")

    def run(self, limit=None, force=False):
        asyncio.run(self.run_async(limit, force))
