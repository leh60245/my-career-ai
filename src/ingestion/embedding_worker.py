"""
Context Look-back ì„ë² ë”© ì›Œì»¤

í‘œ(Table) ë°ì´í„°ì˜ ì„ë² ë”© í’ˆì§ˆì„ ë†’ì´ê¸° ìœ„í•´ ì§ì „ í…ìŠ¤íŠ¸ ë¬¸ë§¥ì„ í¬í•¨í•˜ì—¬
ì„ë² ë”©ì„ ìƒì„±í•˜ê³  DBë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.

í•µì‹¬ ë¡œì§:
- í‘œ(table) ë°ì´í„°ëŠ” ê·¸ ìì²´ë§Œìœ¼ë¡œëŠ” ë‹¨ìœ„(Unit)ë‚˜ ê¸°ì¤€ ë‚ ì§œ ì •ë³´ê°€ ë¶€ì¡±í•¨
- ë³´í†µ í‘œ ë°”ë¡œ ìœ„ì— ì„¤ëª… í…ìŠ¤íŠ¸ê°€ ì¡´ì¬í•˜ë¯€ë¡œ, ì´ë¥¼ í•©ì³ì„œ ë²¡í„°í™”
- 'previous_row'ë¥¼ ìºì‹±í•˜ë©° ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬

ì‚¬ìš©ë²•:
    python -m scripts.run_ingestion --embed --batch-size 32
    python -m scripts.run_ingestion --embed --limit 100  # í…ŒìŠ¤íŠ¸ìš©
    python -m scripts.run_ingestion --embed --force      # ê¸°ì¡´ ì„ë² ë”© ì¬ìƒì„±
"""
import sys
import os
import re
import time
import argparse
from datetime import datetime
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass

from tqdm import tqdm

# [í†µí•© ì•„í‚¤í…ì²˜] ê³µí†µ ëª¨ë“ˆ ë° ê°™ì€ íŒ¨í‚¤ì§€ ëª¨ë“ˆ import
from src.common.embedding import EmbeddingService
from src.common.config import CHUNK_CONFIG
from .db_manager import DBManager


@dataclass
class MaterialRow:
    """Source_Materials í…Œì´ë¸”ì˜ í–‰ì„ ë‚˜íƒ€ë‚´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    id: int
    report_id: int
    chunk_type: str
    section_path: Optional[str]
    sequence_order: int
    raw_content: str


class ContextLookbackEmbeddingWorker:
    """
    Context Look-back ë°©ì‹ìœ¼ë¡œ ì„ë² ë”©ì„ ìƒì„±í•˜ëŠ” ì›Œì»¤ í´ë˜ìŠ¤

    í‘œ(table) ë°ì´í„°ì˜ ê²½ìš°, ê°™ì€ ì„¹ì…˜ ë‚´ ì§ì „ í…ìŠ¤íŠ¸ ë¸”ë¡ì˜ ë‚´ìš©ì„
    ë¬¸ë§¥ìœ¼ë¡œ í¬í•¨í•˜ì—¬ ì„ë² ë”© í’ˆì§ˆì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

    ë…¸ì´ì¦ˆ í…Œì´ë¸”(ë‹¨ìœ„, ë²”ë¡€ ë“±)ì€ ì§ì „ ì²­í¬ì— ë³‘í•©í•˜ì—¬ ê²€ìƒ‰ ì •í™•ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.
    """

    # ë…¸ì´ì¦ˆ í…Œì´ë¸” ê°ì§€ë¥¼ ìœ„í•œ í‚¤ì›Œë“œ
    NOISE_KEYWORDS = ['ë‹¨ìœ„', 'Unit', 'ë²”ë¡€', 'ì°¸ì¡°', 'â€»', 'ì£¼)', '(ì£¼)',
                      'ì›', 'ì²œì›', 'ë°±ë§Œì›', 'ì–µì›', 'ì£¼1)', 'ì£¼2)', '(ë‹¨ìœ„']

    # ë…¸ì´ì¦ˆ í…Œì´ë¸” ìµœëŒ€ í–‰ ìˆ˜ (Markdown í…Œì´ë¸” ê¸°ì¤€)
    NOISE_TABLE_MAX_ROWS = 2

    # ë…¸ì´ì¦ˆ í…Œì´ë¸” ìµœëŒ€ í…ìŠ¤íŠ¸ ê¸¸ì´ (ë¬¸ì ìˆ˜)
    NOISE_TABLE_MAX_LENGTH = 150

    def __init__(self, batch_size: int = 32):
        self.batch_size = batch_size
        self._embedding_service: Optional[EmbeddingService] = None
        self.stats = {
            "total": 0,
            "processed": 0,
            "text_count": 0,
            "table_count": 0,
            "table_with_context": 0,  # ë¬¸ë§¥ì´ ì£¼ì…ëœ í…Œì´ë¸” ìˆ˜
            "noise_tables_merged": 0,  # ë…¸ì´ì¦ˆ í…Œì´ë¸”ë¡œ ë³‘í•©ëœ ìˆ˜
            "noise_tables_skipped": 0,  # ë³‘í•© ëŒ€ìƒ ì—†ì–´ ìŠ¤í‚µëœ ë…¸ì´ì¦ˆ í…Œì´ë¸” ìˆ˜
            "failed": 0,
            "start_time": None,
            "end_time": None
        }

    def _init_generator(self):
        """ì„ë² ë”© ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (lazy loading)"""
        if self._embedding_service is None:
            self._embedding_service = EmbeddingService()
            print(f"   ì„ë² ë”© í”„ë¡œë°”ì´ë”: {self._embedding_service.provider}")
            print(f"   ì„ë² ë”© ì°¨ì›: {self._embedding_service.dimension}")

    # ==================== ë…¸ì´ì¦ˆ í…Œì´ë¸” ê°ì§€ ====================

    def _is_noise_table(self, table_content: str) -> bool:
        """
        í…Œì´ë¸”ì´ ë…¸ì´ì¦ˆ ë°ì´í„°(ë‹¨ìœ„, ë²”ë¡€ ë“±)ì¸ì§€ íŒë‹¨

        Heuristic íŒë‹¨ ë¡œì§:
        - ì¡°ê±´ A: í…Œì´ë¸”ì˜ í–‰(Row) ìˆ˜ê°€ 2ì¤„ ì´í•˜ì´ê³  í‚¤ì›Œë“œ í¬í•¨
        - ì¡°ê±´ B: í‚¤ì›Œë“œ í¬í•¨ ë¹„ìœ¨ì´ 50% ì´ìƒ (ë‹¨ì–´ ê¸°ì¤€)

        Args:
            table_content: Markdown í˜•ì‹ì˜ í…Œì´ë¸” ì½˜í…ì¸ 

        Returns:
            bool: ë…¸ì´ì¦ˆ í…Œì´ë¸”ì´ë©´ True, ì•„ë‹ˆë©´ False
        """
        if not table_content:
            return False

        # Markdown í…Œì´ë¸” í–‰ íŒŒì‹± (| ë¡œ ì‹œì‘í•˜ëŠ” ì¤„)
        lines = table_content.strip().split('\n')
        table_rows = [line for line in lines if line.strip().startswith('|')]

        # í—¤ë” êµ¬ë¶„ì„  ì œê±° (|---|---| í˜•íƒœ)
        data_rows = [row for row in table_rows if not re.match(r'^\|[\s\-:]+\|$', row.strip())]

        # ì¡°ê±´ A: í–‰ ìˆ˜ê°€ 2ì¤„ ì´í•˜ì´ê³  í‚¤ì›Œë“œ í¬í•¨
        row_count = len(data_rows)
        if row_count <= self.NOISE_TABLE_MAX_ROWS:
            for keyword in self.NOISE_KEYWORDS:
                if keyword in table_content:
                    return True

        # ì¡°ê±´ B: í‚¤ì›Œë“œ í¬í•¨ ë¹„ìœ¨ 50% ì´ìƒ (ë‹¨ì–´ ê¸°ì¤€)
        content_text = re.sub(r'[|\-:]+', ' ', table_content)
        words = [w.strip() for w in content_text.split() if len(w.strip()) > 0]

        if len(words) == 0:
            return False

        # í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ” ë‹¨ì–´ ìˆ˜ ê³„ì‚°
        keyword_word_count = 0
        for word in words:
            for keyword in self.NOISE_KEYWORDS:
                if keyword in word:
                    keyword_word_count += 1
                    break

        keyword_ratio = keyword_word_count / len(words)
        if keyword_ratio >= 0.5:
            return True

        return False

    # ==================== ë°ì´í„° ì¡°íšŒ ====================

    def fetch_pending_materials(
            self,
            db: DBManager,
            limit: Optional[int] = None,
            force: bool = False
    ) -> List[MaterialRow]:
        """
        ì„ë² ë”©ì´ ì—†ëŠ”(ë˜ëŠ” force=Trueë©´ ì „ì²´) Source_Materials ì¡°íšŒ

        Args:
            db: DBManager ì¸ìŠ¤í„´ìŠ¤
            limit: ìµœëŒ€ ì¡°íšŒ ê°œìˆ˜ (í…ŒìŠ¤íŠ¸ìš©)
            force: Trueë©´ ê¸°ì¡´ ì„ë² ë”©ì´ ìˆì–´ë„ ì¬ì²˜ë¦¬

        Returns:
            List[MaterialRow]: id ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ì •ë ¬ëœ ë°ì´í„° ë¦¬ìŠ¤íŠ¸

        Note:
            - ë°˜ë“œì‹œ id ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ì •ë ¬í•´ì•¼ ë¬¸ë§¥ íŒŒì•…ì´ ê°€ëŠ¥
            - report_id, sequence_order ê¸°ì¤€ìœ¼ë¡œë„ ì •ë ¬í•˜ì—¬ ë¬¸ì„œ ë‚´ ìˆœì„œ ìœ ì§€
        """
        if force:
            # ì „ì²´ ë°ì´í„° ì¡°íšŒ (ì¬ì²˜ë¦¬) - ë‹¨, noise_mergedëŠ” ì œì™¸
            sql = """
                    SELECT id, report_id, chunk_type, section_path, 
                           sequence_order, raw_content
                    FROM "Source_Materials"
                    WHERE chunk_type != 'noise_merged'
                    ORDER BY report_id, sequence_order, id
                    """
        else:
            # ì„ë² ë”©ì´ ì—†ëŠ” ë°ì´í„°ë§Œ ì¡°íšŒ - ë‹¨, noise_mergedëŠ” ì œì™¸
            sql = """
                        SELECT id, report_id, chunk_type, section_path, 
                               sequence_order, raw_content
                        FROM "Source_Materials"
                        WHERE embedding IS NULL
                          AND chunk_type != 'noise_merged'
                        ORDER BY report_id, sequence_order, id
                    """

        if limit is not None:
            sql = sql.rstrip() + f" LIMIT {limit}"

        db.cursor.execute(sql)
        rows = db.cursor.fetchall()

        return [
            MaterialRow(
                id=row[0],
                report_id=row[1],
                chunk_type=row[2],
                section_path=row[3],
                sequence_order=row[4],
                raw_content=row[5]
            )
            for row in rows
        ]

    def fetch_previous_row(self, db: DBManager, current: MaterialRow) -> Optional[MaterialRow]:
        """
        í˜„ì¬ í–‰ì˜ ì§ì „ í–‰ì„ ì¡°íšŒ (ê°™ì€ report_id ë‚´ì—ì„œ)

        ë°°ì¹˜ ì²˜ë¦¬ ì‹œ ì§ì „ í–‰ì´ ë°°ì¹˜ì— í¬í•¨ë˜ì§€ ì•Šì€ ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬
        DBì—ì„œ ì§ì ‘ ì¡°íšŒí•©ë‹ˆë‹¤.

        Args:
            db: DBManager ì¸ìŠ¤í„´ìŠ¤
            current: í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ í–‰

        Returns:
            ì§ì „ í–‰ (ì—†ìœ¼ë©´ None)
        """
        sql = """
            SELECT id, report_id, chunk_type, section_path, 
                   sequence_order, raw_content
            FROM "Source_Materials"
            WHERE report_id = %s 
              AND sequence_order < %s
            ORDER BY sequence_order DESC
            LIMIT 1
        """
        db.cursor.execute(sql, (current.report_id, current.sequence_order))
        row = db.cursor.fetchone()

        if row:
            return MaterialRow(
                id=row[0],
                report_id=row[1],
                chunk_type=row[2],
                section_path=row[3],
                sequence_order=row[4],
                raw_content=row[5]
            )
        return None

    # ==================== ë¬¸ë§¥ ì£¼ì… ì „ì²˜ë¦¬ ====================

    # Note: build_embedding_textëŠ” ë ˆê±°ì‹œ ë©”ì„œë“œì…ë‹ˆë‹¤.
    # ì‹¤ì œ ì²˜ë¦¬ëŠ” process_batchì—ì„œ _build_normal_embedding_textë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

    def _build_normal_embedding_text(
            self,
            current: MaterialRow,
            previous: Optional[MaterialRow]
    ) -> Tuple[str, bool]:
        """
        ì¼ë°˜ ë¸”ë¡(ë…¸ì´ì¦ˆê°€ ì•„ë‹Œ)ì˜ ì„ë² ë”© í…ìŠ¤íŠ¸ë¥¼ êµ¬ì„±

        Args:
            current: í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ í–‰
            previous: ì§ì „ í–‰ (ì—†ì„ ìˆ˜ ìˆìŒ)

        Returns:
            Tuple[str, bool]: (ì„ë² ë”©ìš© í…ìŠ¤íŠ¸, ë¬¸ë§¥ ì£¼ì… ì—¬ë¶€)
        """
        section_path = current.section_path or "ì•Œ ìˆ˜ ì—†ìŒ"
        raw_content = current.raw_content or ""

        # Case A: í‘œ(table)ì— ì§ì „ í…ìŠ¤íŠ¸ ë¬¸ë§¥ ì£¼ì…
        if (
                current.chunk_type == 'table'
                and previous is not None
                and previous.chunk_type == 'text'
                and previous.section_path == current.section_path
        ):
            context_text = previous.raw_content or ""
            max_context_len = 500
            if len(context_text) > max_context_len:
                context_text = context_text[:max_context_len] + "..."

            embedding_text = (
                f"ë¬¸ì„œ ê²½ë¡œ: {section_path}\n"
                f"[ë¬¸ë§¥ ì„¤ëª…: {context_text}]\n"
                f"[í‘œ ë°ì´í„°]\n"
                f"{raw_content}"
            )
            return embedding_text, True

        # Case B: ì¼ë°˜ í…ìŠ¤íŠ¸ ë˜ëŠ” ë¬¸ë§¥ ì—†ëŠ” í‘œ
        embedding_text = f"ë¬¸ì„œ ê²½ë¡œ: {section_path}\n{raw_content}"
        return embedding_text, False

    # ==================== ì„ë² ë”© ìƒì„± ë° DB ì—…ë°ì´íŠ¸ ====================

    def update_embedding(
            self,
            db: DBManager,
            material_id: int,
            embedding: List[float],
            has_context: bool = False,
            has_merged_meta: bool = False
    ):
        """
        Source_Materials í…Œì´ë¸”ì— ì„ë² ë”© ì—…ë°ì´íŠ¸

        Args:
            db: DBManager ì¸ìŠ¤í„´ìŠ¤
            material_id: Source_Materials.id
            embedding: ì„ë² ë”© ë²¡í„°
            has_context: ë¬¸ë§¥ì´ ì£¼ì…ë˜ì—ˆëŠ”ì§€ ì—¬ë¶€ (ë©”íƒ€ë°ì´í„°ì— ê¸°ë¡)
            has_merged_meta: ë…¸ì´ì¦ˆ í…Œì´ë¸”ì´ ë³‘í•©ë˜ì—ˆëŠ”ì§€ ì—¬ë¶€ (ë©”íƒ€ë°ì´í„°ì— ê¸°ë¡)
        """
        sql = """
            UPDATE "Source_Materials"
            SET embedding = %s,
                metadata = jsonb_set(
                    jsonb_set(
                        jsonb_set(
                            COALESCE(metadata, '{}'), 
                            '{has_embedding}', 
                            'true'
                        ),
                        '{context_injected}',
                        %s
                    ),
                    '{has_merged_meta}',
                    %s
                )
            WHERE id = %s
        """
        db.cursor.execute(sql, (
            embedding,
            'true' if has_context else 'false',
            'true' if has_merged_meta else 'false',
            material_id
        ))

    def _merge_noise_to_previous(self, db: DBManager, prev_id: int, noise_table_content: str):
        """
        ì§ì „ ì²­í¬(Previous)ì— ë…¸ì´ì¦ˆ í…Œì´ë¸” ë‚´ìš©ì„ ì˜êµ¬ì ìœ¼ë¡œ ë³‘í•©

        Args:
            db: DBManager ì¸ìŠ¤í„´ìŠ¤
            prev_id: ì§ì „ ì²­í¬ì˜ ID
            noise_table_content: ë…¸ì´ì¦ˆ í…Œì´ë¸” ë‚´ìš© (Markdown)
        """
        sql = """
            UPDATE "Source_Materials"
            SET raw_content = raw_content || E'\n\n[ì°¸ì¡° ì •ë³´]\n' || %s,
                metadata = jsonb_set(COALESCE(metadata, '{}'), '{has_merged_meta}', 'true'),
                embedding = NULL
            WHERE id = %s
        """
        db.cursor.execute(sql, (noise_table_content, prev_id))

    def _mark_as_noise_merged(self, db: DBManager, current_id: int):
        """
        í˜„ì¬ ë…¸ì´ì¦ˆ í…Œì´ë¸”ì„ Drop ì²˜ë¦¬ (chunk_type ë³€ê²½, ì„ë² ë”© ì œê±°)

        Args:
            db: DBManager ì¸ìŠ¤í„´ìŠ¤
            current_id: ë…¸ì´ì¦ˆ í…Œì´ë¸”ì˜ ID
        """
        sql = """
            UPDATE "Source_Materials"
            SET chunk_type = 'noise_merged',
                embedding = NULL,
                metadata = jsonb_set(COALESCE(metadata, '{}'), '{is_noise_dropped}', 'true')
            WHERE id = %s
        """
        db.cursor.execute(sql, (current_id,))

    def process_batch(
            self,
            db: DBManager,
            batch: List[MaterialRow],
            previous_cache: Dict[int, MaterialRow]
    ) -> Dict[int, MaterialRow]:
        """
        ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì„ë² ë”© ìƒì„± ë° ì—…ë°ì´íŠ¸

        ë…¸ì´ì¦ˆ í…Œì´ë¸” ë³‘í•© ë¡œì§:
        - ë…¸ì´ì¦ˆ í…Œì´ë¸” ê°ì§€ ì‹œ â†’ Previousì— ë‚´ìš© Append, CurrentëŠ” Drop
        - Previousì˜ ì„ë² ë”©ì„ ì¬ìƒì„±í•´ì•¼ í•˜ë¯€ë¡œ, í•´ë‹¹ Previousë¥¼ ì„ë² ë”© ëŒ€ìƒì— ì¶”ê°€

        Args:
            db: DBManager ì¸ìŠ¤í„´ìŠ¤
            batch: ì²˜ë¦¬í•  MaterialRow ë¦¬ìŠ¤íŠ¸
            previous_cache: report_idë³„ ë§ˆì§€ë§‰ ì²˜ë¦¬ í–‰ ìºì‹œ

        Returns:
            ì—…ë°ì´íŠ¸ëœ previous_cache
        """
        embedding_inputs = []  # (material_id, embedding_text, has_context, has_merged_meta)
        ids_to_skip = set()  # Drop ì²˜ë¦¬í•  ë…¸ì´ì¦ˆ í…Œì´ë¸” ID
        prev_ids_to_reembed = {}  # ì¬ì„ë² ë”©ì´ í•„ìš”í•œ Previous: {prev_id: merged_content}

        for current in batch:
            # ì§ì „ í–‰ ì¡°íšŒ: ìºì‹œì—ì„œ ë¨¼ì € í™•ì¸, ì—†ìœ¼ë©´ DB ì¡°íšŒ
            previous = previous_cache.get(current.report_id)

            # ìºì‹œëœ previousê°€ í˜„ì¬ í–‰ì˜ ì§ì „ì´ ì•„ë‹ ìˆ˜ ìˆìŒ (sequence_order ì²´í¬)
            if previous is not None:
                if previous.sequence_order != current.sequence_order - 1:
                    previous = self.fetch_previous_row(db, current)
            else:
                previous = self.fetch_previous_row(db, current)

            # --- ë…¸ì´ì¦ˆ í…Œì´ë¸” ê°ì§€ ë° ë³‘í•© ì²˜ë¦¬ ---
            if (
                    current.chunk_type == 'table'
                    and self._is_noise_table(current.raw_content)
                    and previous is not None
            ):
                # 1. Previousì— Current ë‚´ìš© Append (DB ì—…ë°ì´íŠ¸)
                self._merge_noise_to_previous(db, previous.id, current.raw_content)

                # 2. Currentë¥¼ Drop ì²˜ë¦¬ (DB ì—…ë°ì´íŠ¸)
                self._mark_as_noise_merged(db, current.id)
                ids_to_skip.add(current.id)

                # 3. Previousì˜ ë³‘í•©ëœ ë‚´ìš©ì„ ê¸°ë¡ (ë‚˜ì¤‘ì— ì„ë² ë”© ì¬ìƒì„±)
                merged_content = (previous.raw_content or "") + "\n\n[ì°¸ì¡° ì •ë³´]\n" + (current.raw_content or "")
                prev_ids_to_reembed[previous.id] = {
                    "content": merged_content,
                    "section_path": previous.section_path
                }

                # 4. ìºì‹œ ì—…ë°ì´íŠ¸: Previousì˜ ë‚´ìš©ì´ ë³€ê²½ë˜ì—ˆìœ¼ë¯€ë¡œ ê°±ì‹ 
                previous.raw_content = merged_content
                previous_cache[current.report_id] = previous

                # 5. í†µê³„ ì—…ë°ì´íŠ¸
                self.stats["table_count"] += 1
                self.stats["noise_tables_merged"] += 1
                continue  # CurrentëŠ” ì„ë² ë”© ëŒ€ìƒì—ì„œ ì œì™¸

            # --- ì¼ë°˜ ì²˜ë¦¬ (ë…¸ì´ì¦ˆê°€ ì•„ë‹Œ ê²½ìš°) ---
            embedding_text, has_context = self._build_normal_embedding_text(current, previous)
            embedding_inputs.append((current.id, embedding_text, has_context, False))

            # í†µê³„ ì—…ë°ì´íŠ¸
            if current.chunk_type == 'text':
                self.stats["text_count"] += 1
            else:
                self.stats["table_count"] += 1
                if has_context:
                    self.stats["table_with_context"] += 1

            # ìºì‹œ ì—…ë°ì´íŠ¸
            previous_cache[current.report_id] = current

        # Previous ì¬ì„ë² ë”© ëŒ€ìƒ ì¶”ê°€
        for prev_id, data in prev_ids_to_reembed.items():
            section_path = data["section_path"] or "ì•Œ ìˆ˜ ì—†ìŒ"
            embedding_text = f"ë¬¸ì„œ ê²½ë¡œ: {section_path}\n{data['content']}"
            # has_merged_meta=Trueë¡œ í‘œì‹œ
            embedding_inputs.append((prev_id, embedding_text, False, True))

        # ë°°ì¹˜ ì„ë² ë”© ìƒì„±
        if not embedding_inputs:
            db.conn.commit()
            return previous_cache

        texts = [item[1] for item in embedding_inputs]
        try:
            embeddings = self._embedding_service.embed_texts(texts)

            # DB ì—…ë°ì´íŠ¸
            for (material_id, _, has_context, has_merged_meta), embedding in zip(embedding_inputs, embeddings):
                self.update_embedding(db, material_id, embedding, has_context, has_merged_meta)

            db.conn.commit()
            self.stats["processed"] += len(batch) - len(ids_to_skip)

        except Exception as e:
            db.conn.rollback()
            print(f"\nâš ï¸ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            self.stats["failed"] += len(batch)

        return previous_cache

    # ==================== ë©”ì¸ ì‹¤í–‰ ====================

    def run(
            self,
            limit: Optional[int] = None,
            force: bool = False
    ):
        """
        Context Look-back ì„ë² ë”© íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

        Args:
            limit: ìµœëŒ€ ì²˜ë¦¬ ê°œìˆ˜ (í…ŒìŠ¤íŠ¸ìš©)
            force: Trueë©´ ê¸°ì¡´ ì„ë² ë”©ì´ ìˆì–´ë„ ì¬ì²˜ë¦¬
        """
        self.stats["start_time"] = datetime.now()

        print("\n" + "=" * 70)
        print("ğŸ§  Context Look-back ì„ë² ë”© ì›Œì»¤ ì‹œì‘")
        print("=" * 70)
        print(f"   ì‹œì‘ ì‹œê°„: {self.stats['start_time'].strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"   ë°°ì¹˜ í¬ê¸°: {self.batch_size}")
        print(f"   ê°•ì œ ì¬ìƒì„±: {'ì˜ˆ' if force else 'ì•„ë‹ˆì˜¤'}")

        # 1. ì„ë² ë”© ìƒì„±ê¸° ì´ˆê¸°í™”
        self._init_generator()

        # 2. ì²˜ë¦¬ ëŒ€ìƒ ë°ì´í„° ì¡°íšŒ
        with DBManager() as db:
            pending_materials = self.fetch_pending_materials(db, limit, force)

        self.stats["total"] = len(pending_materials)
        print(f"\nğŸ“‹ ì²˜ë¦¬ ëŒ€ìƒ: {self.stats['total']}ê°œ ì²­í¬")

        if self.stats["total"] == 0:
            print("âœ… ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return self.stats

        # 3. ë°°ì¹˜ ë¶„í• 
        batches = [
            pending_materials[i:i + self.batch_size]
            for i in range(0, len(pending_materials), self.batch_size)
        ]
        print(f"ğŸ“¦ ë°°ì¹˜ ìˆ˜: {len(batches)}")

        # 4. ë°°ì¹˜ ì²˜ë¦¬
        # previous_cache: report_id â†’ ë§ˆì§€ë§‰ ì²˜ë¦¬ëœ MaterialRow
        # ì´ë¥¼ í†µí•´ ë°°ì¹˜ ê°„ì—ë„ ì§ì „ í–‰ ì •ë³´ë¥¼ ìœ ì§€
        previous_cache: Dict[int, MaterialRow] = {}

        with DBManager() as db:
            for batch in tqdm(batches, desc="ì„ë² ë”© ìƒì„±"):
                previous_cache = self.process_batch(db, batch, previous_cache)

                # ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ ìœ„í•œ ì§§ì€ ë”œë ˆì´
                time.sleep(0.05)

        # 5. ê²°ê³¼ ìš”ì•½
        self.stats["end_time"] = datetime.now()
        self._print_summary()

        return self.stats

    def _print_summary(self):
        """ì‹¤í–‰ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        duration = self.stats["end_time"] - self.stats["start_time"]

        print("\n" + "=" * 70)
        print("ğŸ“Š Context Look-back ì„ë² ë”© ê²°ê³¼")
        print("=" * 70)
        print(f"   ì‹œì‘ ì‹œê°„: {self.stats['start_time'].strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"   ì¢…ë£Œ ì‹œê°„: {self.stats['end_time'].strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"   ì†Œìš” ì‹œê°„: {duration}")

        print(f"\n   ğŸ“ˆ ì²˜ë¦¬ í†µê³„:")
        print(f"      - ì „ì²´ ëŒ€ìƒ: {self.stats['total']}")
        print(f"      - ì„±ê³µ: {self.stats['processed']}")
        print(f"      - ì‹¤íŒ¨: {self.stats['failed']}")

        print(f"\n   ğŸ“ íƒ€ì…ë³„ í†µê³„:")
        print(f"      - í…ìŠ¤íŠ¸ ë¸”ë¡: {self.stats['text_count']}")
        print(f"      - í…Œì´ë¸” ë¸”ë¡: {self.stats['table_count']}")
        print(f"      - ë¬¸ë§¥ ì£¼ì…ëœ í…Œì´ë¸”: {self.stats['table_with_context']}")
        print(f"      - ë…¸ì´ì¦ˆ í…Œì´ë¸” ë³‘í•©: {self.stats['noise_tables_merged']}")

        if self.stats['table_count'] > 0:
            context_rate = (self.stats['table_with_context'] / self.stats['table_count']) * 100
            noise_rate = (self.stats['noise_tables_merged'] / self.stats['table_count']) * 100
            print(f"      - í…Œì´ë¸” ë¬¸ë§¥ ì£¼ì…ë¥ : {context_rate:.1f}%")
            print(f"      - ë…¸ì´ì¦ˆ í…Œì´ë¸” ë³‘í•©ë¥ : {noise_rate:.1f}%")

        if self.stats['total'] > 0:
            success_rate = (self.stats['processed'] / self.stats['total']) * 100
            print(f"\n      - ì „ì²´ ì„±ê³µë¥ : {success_rate:.1f}%")

            # ì²˜ë¦¬ ì†ë„
            seconds = duration.total_seconds()
            if seconds > 0:
                rate = self.stats['processed'] / seconds
                print(f"      - ì²˜ë¦¬ ì†ë„: {rate:.1f} ì²­í¬/ì´ˆ")

        # DB í˜„í™©
        with DBManager() as db:
            stats = db.get_stats()
            print(f"\n   ğŸ“¦ DB í˜„í™©:")
            print(f"      - ì „ì²´ ì›ì²œ ë°ì´í„°: {stats['materials']}")
            print(f"      - ì„ë² ë”© ì™„ë£Œ: {stats['embedded_materials']}")

            if stats['materials'] > 0:
                embed_rate = (stats['embedded_materials'] / stats['materials']) * 100
                print(f"      - ì„ë² ë”© ë¹„ìœ¨: {embed_rate:.1f}%")

        print("=" * 70)


def main():
    """CLI ì—”íŠ¸ë¦¬í¬ì¸íŠ¸"""
    parser = argparse.ArgumentParser(
        description="Context Look-back ì„ë² ë”© ì›Œì»¤ - í‘œ ë°ì´í„°ì— ì§ì „ í…ìŠ¤íŠ¸ ë¬¸ë§¥ì„ ì£¼ì…í•˜ì—¬ ì„ë² ë”© ìƒì„±"
    )
    parser.add_argument(
        '--batch-size',
        type=int,
        default=32,
        help='í•œ ë²ˆì— ì²˜ë¦¬í•  ì²­í¬ ìˆ˜ (ê¸°ë³¸: 32)'
    )
    parser.add_argument(
        '--limit',
        type=int,
        help='ìµœëŒ€ ì²˜ë¦¬ ê°œìˆ˜ (í…ŒìŠ¤íŠ¸ìš©)'
    )
    parser.add_argument(
        '--force',
        action='store_true',
        help='ê¸°ì¡´ ì„ë² ë”©ì´ ìˆì–´ë„ ì¬ìƒì„±'
    )

    args = parser.parse_args()

    worker = ContextLookbackEmbeddingWorker(batch_size=args.batch_size)
    worker.run(limit=args.limit, force=args.force)


if __name__ == "__main__":
    main()
