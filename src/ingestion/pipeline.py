"""
íŒŒì´í”„ë¼ì¸ ëª¨ë“ˆ

PHASE 3.5: Legacy Code Migration
- Removed DBManager dependency (Replaced with Service Layer)
- Unified duplicated logic for 'efficient' and 'standard' modes
- Fully Async implementation with retry logic
- Retains DART orchestration responsibilities
"""

import asyncio
import logging
from datetime import datetime
from typing import Any, Dict, List, Optional

# [í†µí•© ì•„í‚¤í…ì²˜]
from src.common.config import BATCH_CONFIG
from src.database import AsyncDatabaseEngine
from src.database.repositories import (
    AnalysisReportRepository,
    CompanyRepository,
    SourceMaterialRepository,
)
from src.services import AnalysisService, CompanyService, VectorSearchService

from .dart_agent import DartReportAgent

logger = logging.getLogger(__name__)


class DataPipeline:
    """
    DART ì‚¬ì—…ë³´ê³ ì„œ ìˆ˜ì§‘ ë° ì ì¬ íŒŒì´í”„ë¼ì¸ (Async Orchestrator).
    """

    def __init__(self):
        self.agent = DartReportAgent()
        self.stats = {
            "total": 0,
            "success": 0,
            "skipped": 0,
            "failed": 0,
            "start_time": None,
            "end_time": None,
        }
        self.failed_corps = []  # List of {'corp_name': ..., 'corp_code': ...}

    # ==================== Async Service Integration ====================

    async def _save_to_db(self, session, corp, report, sections: List[Dict]) -> bool:
        """
        Service Layerë¥¼ í†µí•´ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.
        """
        # 1. Services ì´ˆê¸°í™” (Dependency Injection)
        comp_service = CompanyService(CompanyRepository(session))
        # AnalysisService needs both repos to verify company existence
        anal_service = AnalysisService(
            AnalysisReportRepository(session), CompanyRepository(session)
        )
        vec_service = VectorSearchService(SourceMaterialRepository(session))

        # 2. ê¸°ì—… ë“±ë¡ (Idempotent)
        try:
            # onboard_companyëŠ” ì¤‘ë³µ ì‹œ DuplicateEntity ë°œìƒ ê°€ëŠ¥ -> getìœ¼ë¡œ ì²˜ë¦¬
            # í•˜ì§€ë§Œ Service ë¡œì§ì— ë”°ë¼ onboardê°€ ì¡´ì¬ ì²´í¬ë¥¼ í•  ìˆ˜ë„ ìˆìŒ.
            # ì—¬ê¸°ì„œëŠ” ì•ˆì „í•˜ê²Œ ì¡´ì¬ í™•ì¸ í›„ ë“±ë¡ ì‹œë„ íŒ¨í„´ ì‚¬ìš©
            company = await comp_service.get_company(
                0
            )  # ID 0ì€ ì—†ì„í…Œë‹ˆ ì—ëŸ¬ ë°©ì§€ìš© ë”ë¯¸ í˜¸ì¶œ í˜¹ì€ ë¡œì§ ìˆ˜ì •
            # Serviceì˜ onboard_companyê°€ ì¤‘ë³µì²´í¬ë¥¼ í•˜ë¯€ë¡œ try-except ì‚¬ìš©
            company = await comp_service.onboard_company(
                company_name=corp.corp_name,
                corp_code=corp.corp_code,
                stock_code=corp.stock_code,
            )
        except Exception:
            # ì´ë¯¸ ì¡´ì¬í•˜ë©´ ì¡°íšŒ
            repo = CompanyRepository(session)
            company = await repo.get_by_name(corp.corp_name)

        if not company:
            logger.error(f"Failed to resolve company: {corp.corp_name}")
            return False

        # 3. ë¦¬í¬íŠ¸ ë©”íƒ€ë°ì´í„° ì €ì¥
        report_info = self.agent.get_report_info(report)
        try:
            # return_existing=Trueë¡œ ì„¤ì •í•˜ì—¬ ì¤‘ë³µ ì‹œ ê¸°ì¡´ ê°ì²´ ë°˜í™˜
            analysis_report = await anal_service.save_report_metadata(
                company_id=company.id, data=report_info, return_existing=True
            )
        except Exception as e:
            logger.error(f"Failed to save report metadata: {e}")
            return False

        # 4. ì²­í¬(Source Materials) ì €ì¥
        total_blocks = 0
        common_meta = {
            "source": "dart",
            "company_name": corp.corp_name,
            "rcept_no": report_info.get("rcept_no"),
        }

        for section in sections:
            blocks = section.get("blocks", [])
            # ë©”íƒ€ë°ì´í„° ë³´ê°•
            for b in blocks:
                if "meta_info" not in b:
                    b["meta_info"] = {}
                b["meta_info"].update(common_meta)

            # ë²¡í„° ì„œë¹„ìŠ¤ë¡œ ì €ì¥ (ì„ë² ë”©ì€ ë‚˜ì¤‘ì— Workerê°€ ì²˜ë¦¬í•˜ê±°ë‚˜, ì—¬ê¸°ì„œ ì²˜ë¦¬ ê°€ëŠ¥)
            # ì—¬ê¸°ì„œëŠ” Raw Data ì €ì¥ì´ ì£¼ ëª©ì ì´ë¯€ë¡œ ì„ë² ë”©ì€ NULLì¼ ìˆ˜ ìˆìŒ
            saved = await vec_service.save_chunks(analysis_report.id, blocks)
            total_blocks += len(saved)

        logger.info(f"   ğŸ“¥ Saved {total_blocks} blocks for {corp.corp_name}")
        return True

    # ==================== Core Processing Logic ====================

    async def _process_corp_async(self, session, corp) -> Optional[bool]:
        """ë‹¨ì¼ ê¸°ì—… ì²˜ë¦¬ ë¡œì§ (DART Fetch -> DB Save)"""

        # 1. ë³´ê³ ì„œ ì¡°íšŒ (Sync - Network)
        # Note: DART AgentëŠ” requestsë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ Blocking Callì„.
        # ëŒ€ëŸ‰ ì²˜ë¦¬ ì‹œ run_in_executor ê³ ë ¤ ê°€ëŠ¥í•˜ë‚˜, Rate Limit ë•Œë¬¸ì— ìˆœì°¨ ì‹¤í–‰ì´ ìœ ë¦¬í•  ìˆ˜ ìˆìŒ.  # noqa: E501
        report = self.agent.get_annual_report(corp.corp_code)

        if not report:
            logger.warning(f"   âš ï¸ No annual report found for {corp.corp_name}")
            return None

        logger.info(f"   ğŸ“„ Report Found: {report.report_nm}")

        # 2. ë°ì´í„° ì¶”ì¶œ (Sync - CPU)
        sections = self.agent.extract_target_sections_sequential(report)
        if not sections:
            logger.warning(f"   âš ï¸ No valid sections extracted for {corp.corp_name}")
            return None

        # 3. DB ì €ì¥ (Async)
        return await self._save_to_db(session, corp, report, sections)

    async def run_pipeline_async(self, targets: List[Any], reset_db: bool = False):
        """
        ë¹„ë™ê¸° íŒŒì´í”„ë¼ì¸ ë©”ì¸ ë£¨í”„
        """
        self.stats["start_time"] = datetime.now()
        self.stats["total"] = len(targets)
        logger.info(f"ğŸš€ Pipeline Started. Targets: {len(targets)}")

        db_engine = AsyncDatabaseEngine()

        # Note: reset_db ê¸°ëŠ¥ì€ íŒŒê´´ì ì´ë¯€ë¡œ Service Layerë¡œ ì´ê´€í•˜ì§€ ì•Šê³ 
        # í•„ìš”í•˜ë‹¤ë©´ ë³„ë„ ìŠ¤í¬ë¦½íŠ¸ë‚˜ Admin ë„êµ¬ë¡œ ë¶„ë¦¬í•˜ëŠ” ê²ƒì´ ì•ˆì „í•©ë‹ˆë‹¤.
        if reset_db:
            logger.warning(
                "âš ï¸ 'reset_db' flag is ignored in Service Layer mode for safety."
            )

        # Batching
        batch_size = BATCH_CONFIG.get("batch_size", 10)
        batches = [
            targets[i : i + batch_size] for i in range(0, len(targets), batch_size)
        ]

        async with db_engine.get_session() as session:
            for b_idx, batch in enumerate(batches):
                logger.info(f"\nğŸ“¦ Processing Batch {b_idx + 1}/{len(batches)}")

                for corp in batch:
                    try:
                        logger.info(
                            f"â–¶ï¸ Processing: {corp.corp_name} ({corp.stock_code})"
                        )

                        success = await self._process_corp_async(session, corp)

                        # ê¸°ì—… ë‹¨ìœ„ ì»¤ë°‹ (ì˜¤ë¥˜ ê²©ë¦¬)
                        await session.commit()

                        if success:
                            self.stats["success"] += 1
                        elif success is None:
                            self.stats["skipped"] += 1
                        else:
                            self.stats["failed"] += 1
                            self.failed_corps.append(
                                {
                                    "corp_name": corp.corp_name,
                                    "corp_code": corp.corp_code,
                                    "stock_code": corp.stock_code,
                                }
                            )

                    except Exception as e:
                        await session.rollback()
                        logger.error(f"âŒ Failed to process {corp.corp_name}: {e}")
                        self.stats["failed"] += 1
                        self.failed_corps.append(
                            {
                                "corp_name": corp.corp_name,
                                "corp_code": getattr(corp, "corp_code", "unknown"),
                                "stock_code": getattr(corp, "stock_code", "unknown"),
                            }
                        )

                    # Rate Limiting
                    await asyncio.sleep(BATCH_CONFIG.get("request_delay_sec", 1))

                # Batch Delay
                if b_idx < len(batches) - 1:
                    await asyncio.sleep(BATCH_CONFIG.get("batch_delay_sec", 2))

        await db_engine.dispose()
        self.stats["end_time"] = datetime.now()
        self._print_summary()
        return self.stats

    async def retry_failed_async(self):
        """ì‹¤íŒ¨í•œ ê¸°ì—… ì¬ì‹œë„ ë¡œì§"""
        if not self.failed_corps:
            logger.info("âœ… No failed corporations to retry.")
            return

        logger.info(f"\nğŸ”„ Retrying {len(self.failed_corps)} failed corporations...")

        # Corp ê°ì²´ ì¬ìƒì„±
        retry_targets = []
        for fc in self.failed_corps:
            # agent.get_corp_by_stock_code ë“±ì„ ì‚¬ìš©í•´ ê°ì²´ ë³µì›
            corp = self.agent.get_corp_by_stock_code(fc["stock_code"])
            if corp:
                retry_targets.append(corp)

        # ìƒíƒœ ì´ˆê¸°í™” í›„ ì¬ì‹¤í–‰
        self.failed_corps = []
        await self.run_pipeline_async(retry_targets, reset_db=False)

    # ==================== Public Interfaces (Sync Wrappers) ====================

    def run(
        self,
        stock_codes: Optional[List[str]] = None,
        limit: Optional[int] = None,
        reset_db: bool = False,
    ):
        """ê¸°ë³¸ ì‹¤í–‰ ëª¨ë“œ"""
        if stock_codes:
            targets = []
            for code in stock_codes:
                corp = self.agent.get_corp_by_stock_code(code)
                if corp:
                    targets.append(corp)
        else:
            targets = self.agent.get_listed_corps()

        if limit:
            targets = targets[:limit]

        asyncio.run(self.run_pipeline_async(targets, reset_db))

    def run_efficient(
        self,
        bgn_de: str,
        end_de: str,
        reset_db: bool = False,
        limit: Optional[int] = None,
    ):
        """
        íš¨ìœ¨ ëª¨ë“œ: ì‚¬ì—…ë³´ê³ ì„œê°€ ìˆëŠ” ê¸°ì—…ë§Œ ì„ ë³„í•˜ì—¬ ì‹¤í–‰
        """
        logger.info("ğŸ” Searching for companies with reports (Efficient Mode)...")

        # 1. ë³´ê³ ì„œê°€ ìˆëŠ” ê¸°ì—… ëª©ë¡ ì¡°íšŒ (Sync Agent ì‚¬ìš©)
        # corps_with_reportsëŠ” (Corpê°ì²´, ReportInfo) íŠœí”Œ ë¦¬ìŠ¤íŠ¸ì„
        corps_with_reports = self.agent.get_corps_with_reports(bgn_de, end_de)

        if limit:
            corps_with_reports = corps_with_reports[:limit]

        # 2. Corp ê°ì²´ë§Œ ì¶”ì¶œ
        targets = [item[0] for item in corps_with_reports]

        logger.info(f"ğŸ“‹ Found {len(targets)} active targets.")

        # 3. íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        asyncio.run(self.run_pipeline_async(targets, reset_db))

    def retry_failed(self):
        """ì¬ì‹œë„ ë˜í¼"""
        asyncio.run(self.retry_failed_async())

    def _print_summary(self):
        duration = self.stats["end_time"] - self.stats["start_time"]
        print("\n" + "=" * 60)
        print("ğŸ“Š Pipeline Execution Summary")
        print(f"   Duration: {duration}")
        print(f"   Total: {self.stats['total']}")
        print(f"   Success: {self.stats['success']}")
        print(f"   Skipped: {self.stats['skipped']}")
        print(f"   Failed: {self.stats['failed']}")

        if self.failed_corps:
            print("\n   âš ï¸ Failed List:")
            for fc in self.failed_corps[:5]:
                print(f"      - {fc['corp_name']}")
            if len(self.failed_corps) > 5:
                print(f"      ... and {len(self.failed_corps) - 5} more")
        print("=" * 60)
        logger.info("ğŸš€ Pipeline Completed.")
