"""
DART Î≥¥Í≥†ÏÑú ÏóêÏù¥Ï†ÑÌä∏ Î™®Îìà (Refactored)

PHASE 3.5: Legacy Code Migration
- Removed legacy parsing logic (Advanced/Page-Sync versions)
- Standardized on 'Sequential Block Processing' used by Pipeline v3
- Cleaned up dependencies and type hints

DART APIÎ•º ÌÜµÌï¥ ÏÇ¨ÏóÖÎ≥¥Í≥†ÏÑúÎ•º ÏàòÏßëÌïòÍ≥†, RAGÏóê ÏµúÏ†ÅÌôîÎêú ÌòïÌÉúÎ°ú ÌååÏã±Ìï©ÎãàÎã§.
"""

import logging
import re
import time
from datetime import datetime, timedelta
from io import StringIO
from typing import Any

import dart_fss as dart
import pandas as pd
from bs4 import BeautifulSoup, Tag
from bs4.element import NavigableString

# [ÌÜµÌï© ÏïÑÌÇ§ÌÖçÏ≤ò] ÏÑ§Ï†ï Î°úÎìú
from src.common.config import CHUNK_CONFIG, DART_CONFIG, TARGET_SECTIONS

logger = logging.getLogger(__name__)

# Î†àÍ±∞Ïãú Ìò∏Ìôò Î≥ÄÏàò
DART_API_KEY = DART_CONFIG.get("api_key")
REPORT_SEARCH_CONFIG = {
    "bgn_de": DART_CONFIG.get(
        "search_start_date", (datetime.now() - timedelta(days=90)).strftime("%Y%m%d")
    ),  # noqa: E501
    "pblntf_detail_ty": DART_CONFIG.get("report_type_code", "a001"),
    "page_count": DART_CONFIG.get("page_count", 100),
    "page_delay_sec": DART_CONFIG.get("page_delay_sec", 0.5),
    "max_search_days": DART_CONFIG.get("max_search_days", 90),
}

BEGIN_DATE_LIMIT = REPORT_SEARCH_CONFIG["bgn_de"]
END_DATE_LIMIT = datetime.now().strftime("%Y%m%d")


class DartReportAgent:
    """
    DART API ÏóêÏù¥Ï†ÑÌä∏.
    ÏÇ¨ÏóÖÎ≥¥Í≥†ÏÑú Í≤ÄÏÉâ Î∞è HTML ÌååÏã±(ÏàúÏ∞®Ï†Å Î∏îÎ°ù Ï∂îÏ∂ú)ÏùÑ Îã¥ÎãπÌï©ÎãàÎã§.
    """

    def __init__(self):
        """ÏóêÏù¥Ï†ÑÌä∏ Ï¥àÍ∏∞Ìôî Î∞è DART API ÏÑ§Ï†ï"""
        if not DART_API_KEY:
            logger.warning("‚ö†Ô∏è DART_API_KEY is missing in configuration.")
        else:
            dart.set_api_key(api_key=DART_API_KEY)

        self._corp_list = None

    @property
    def corp_list(self):
        """Í∏∞ÏóÖ Î¶¨Ïä§Ìä∏ (Lazy Loading)"""
        if self._corp_list is None:
            logger.info("üîÑ Loading DART Corp List...")
            try:
                self._corp_list = dart.get_corp_list()
                logger.info(f"‚úÖ Loaded {len(self._corp_list)} corporations.")
            except Exception as e:
                logger.error(f"Failed to load corp list: {e}")
                self._corp_list = []
        return self._corp_list

    # ==================== Í∏∞ÏóÖ Î∞è Î≥¥Í≥†ÏÑú Í≤ÄÏÉâ ====================

    def get_corp_by_stock_code(self, stock_code: str):
        """Ï¢ÖÎ™©ÏΩîÎìúÎ°ú Í∏∞ÏóÖ Ï†ïÎ≥¥ Ï°∞Ìöå"""
        for corp in self.corp_list:
            if corp.stock_code == stock_code:
                return corp
        return None

    def get_listed_corps(self) -> list:
        """ÏÉÅÏû• Í∏∞ÏóÖ ÌïÑÌÑ∞ÎßÅ"""
        return [c for c in self.corp_list if c.stock_code]

    def get_annual_report(
        self, corp_code: str, bgn_de: str = BEGIN_DATE_LIMIT
    ) -> Any | None:
        """ÏÇ¨ÏóÖÎ≥¥Í≥†ÏÑú Í≤ÄÏÉâ (ÏµúÏã† 1Í±¥)"""
        bgn_de = bgn_de or REPORT_SEARCH_CONFIG["bgn_de"]
        try:
            search_results = dart.search(
                corp_code=corp_code,
                bgn_de=bgn_de,
                pblntf_detail_ty=REPORT_SEARCH_CONFIG["pblntf_detail_ty"],
            )
            return search_results[0] if search_results else None
        except Exception as e:
            logger.error(f"Failed to search report for {corp_code}: {e}")
            return None

    def search_all_reports(
        self,
        bgn_de: str = BEGIN_DATE_LIMIT,
        end_de: str = END_DATE_LIMIT,
        corp_code: str = None,
    ) -> list[Any]:
        """
        Í∏∞Í∞Ñ ÎÇ¥ Î™®Îì† ÏÇ¨ÏóÖÎ≥¥Í≥†ÏÑúÎ•º ÏùºÍ¥Ñ Í≤ÄÏÉâ (Efficient ModeÏö©)
        """
        if end_de is None:
            end_de = datetime.now().strftime("%Y%m%d")
        if bgn_de is None:
            bgn_de = REPORT_SEARCH_CONFIG["bgn_de"]

        # Í∏∞Í∞Ñ Ï†úÌïú Î°úÏßÅ
        if corp_code is None:
            max_days = REPORT_SEARCH_CONFIG.get("max_search_days", 90)
            bgn_date = datetime.strptime(bgn_de, "%Y%m%d")
            end_date = datetime.strptime(end_de, "%Y%m%d")
            if (end_date - bgn_date).days > max_days:
                bgn_date = end_date - timedelta(days=max_days)
                bgn_de = bgn_date.strftime("%Y%m%d")
                logger.warning(
                    f"‚ö†Ô∏è Search period limited to {max_days} days: {bgn_de} ~ {end_de}"
                )

        all_reports = []
        page_no = 1
        page_count = REPORT_SEARCH_CONFIG.get("page_count", 100)

        logger.info(f"üìã Searching reports: {bgn_de} ~ {end_de}")

        while True:
            try:
                search_kwargs = {
                    "bgn_de": bgn_de,
                    "end_de": end_de,
                    "pblntf_detail_ty": REPORT_SEARCH_CONFIG["pblntf_detail_ty"],
                    "page_count": page_count,
                    "page_no": page_no,
                }
                if corp_code:
                    search_kwargs["corp_code"] = corp_code

                res = dart.filings.search(**search_kwargs)
                report_list = getattr(res, "report_list", []) or []

                if not report_list:
                    break

                all_reports.extend(report_list)
                total_page = getattr(res, "total_page", 1) or 1

                if page_no >= total_page:
                    break
                page_no += 1
                time.sleep(REPORT_SEARCH_CONFIG.get("page_delay_sec", 0.5))

            except Exception as e:
                logger.error(f"Search failed at page {page_no}: {e}")
                break

        return all_reports

    def get_corps_with_reports(
        self, bgn_de: str = BEGIN_DATE_LIMIT, end_de: str = END_DATE_LIMIT
    ) -> list[tuple]:
        """
        Î≥¥Í≥†ÏÑúÍ∞Ä Ï°¥Ïû¨ÌïòÎäî Í∏∞ÏóÖ Î™©Î°ù Î∞òÌôò (Ï§ëÎ≥µ Ï†úÍ±∞)
        Returns: [(CorpObject, ReportObject), ...]
        """
        all_reports = self.search_all_reports(bgn_de, end_de)
        if not all_reports:
            return []

        # Í∏∞ÏóÖÎ≥Ñ ÏµúÏã† Î≥¥Í≥†ÏÑú ÏÑ†Î≥Ñ
        corp_latest = {}
        for r in all_reports:
            code = getattr(r, "corp_code", None)
            dt = getattr(r, "rcept_dt", "")
            if code not in corp_latest or dt > getattr(
                corp_latest[code], "rcept_dt", ""
            ):
                corp_latest[code] = r

        results = []
        for r in corp_latest.values():
            corp = self.get_corp_by_corp_code(getattr(r, "corp_code", None))
            if corp:
                results.append((corp, r))

        return results

    def get_report_info(self, report) -> dict:
        """DB Ï†ÄÏû•ÏùÑ ÏúÑÌïú Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú"""
        return {
            "title": report.report_nm,
            "rcept_no": report.rcept_no,
            "rcept_dt": report.rcept_dt,
            "corp_code": report.corp_code,
            "corp_name": report.corp_name,
            "report_type": "annual",
        }

    # ==================== ÌïµÏã¨ ÌååÏã± Î°úÏßÅ (Sequential Block) ====================

    def extract_target_sections_sequential(self, report) -> list[dict]:
        """
        [Main Parsing Method]
        ÌïµÏã¨ ÏÑπÏÖòÎì§ÏùÑ ÏàúÏ∞®Ï†Å Î∏îÎ°ù(Sequential Block) Î∞©ÏãùÏúºÎ°ú Ï∂îÏ∂úÌï©ÎãàÎã§.
        Î¨∏ÏÑúÏùò ÌùêÎ¶Ñ(Header -> Text -> Table)ÏùÑ Ïú†ÏßÄÌï©ÎãàÎã§.
        """
        extracted = []
        global_sequence = 0

        for section_name in TARGET_SECTIONS:
            section_data = self._extract_section_sequential(report, section_name)
            if section_data:
                # ÏãúÌÄÄÏä§ Î≤àÌò∏ Ïû¨Ï°∞Ï†ï (ÏÑπÏÖò Í∞Ñ Ïó∞ÏÜçÏÑ± Î≥¥Ïû•)
                blocks = section_data.get("blocks", [])
                for block in blocks:
                    block["sequence_order"] = global_sequence
                    global_sequence += 1

                extracted.append(section_data)
                logger.info(f"   ‚úÖ Extracted '{section_name}': {len(blocks)} blocks")
            else:
                logger.debug(f"   ‚ö†Ô∏è Section '{section_name}' not found")

        return extracted

    def _extract_section_sequential(self, report, section_keyword: str) -> dict | None:
        try:
            result = report.find_all(includes=section_keyword)
            pages = result.get("pages", [])
            if not pages:
                return None

            all_blocks = []

            # ÌéòÏù¥ÏßÄÎ≥Ñ ÏàúÌöå
            for page in pages:
                soup = BeautifulSoup(page.html, "html.parser")
                page_title = getattr(page, "title", section_keyword)

                # Ïû¨Í∑ÄÏ†Å ÌååÏã±
                blocks = self._parse_sequential_blocks(
                    soup.body if soup.body else soup,
                    current_path=page_title,
                    start_sequence=0,  # ÏûÑÏãú ÏãúÌÄÄÏä§ (ÎÇòÏ§ëÏóê Ïû¨Ï°∞Ï†ïÎê®)
                )
                all_blocks.extend(blocks)

            return {
                "chapter": section_keyword,
                "blocks": all_blocks,
                "page_count": len(pages),
            }
        except Exception as e:
            logger.error(f"Parsing failed for '{section_keyword}': {e}")
            return None

    def _parse_sequential_blocks(
        self, container, current_path: str, start_sequence: int
    ) -> list[dict]:
        """HTML ÏöîÏÜåÎ•º ÏàúÌöåÌïòÎ©∞ ÌÖçÏä§Ìä∏/ÌÖåÏù¥Î∏î Î∏îÎ°ù ÏÉùÏÑ±"""
        blocks = []
        sequence = start_sequence
        text_buffer = []
        header_tags = ["h1", "h2", "h3", "h4", "h5", "h6"]

        def flush_text_buffer():
            nonlocal sequence
            if not text_buffer:
                return

            combined_text = self._clean_text("\n".join(text_buffer))
            if len(combined_text) >= CHUNK_CONFIG["min_chunk_size"]:
                # Ï≤≠ÌÅ¨ Î∂ÑÌï†
                chunks = self.chunk_text(combined_text)
                for chunk in chunks:
                    blocks.append(
                        {
                            "chunk_type": "text",
                            "section_path": current_path,
                            "content": chunk,
                            "sequence_order": sequence,
                            "table_metadata": None,
                            "raw_content": chunk,  # for compatibility
                        }
                    )
                    sequence += 1
            text_buffer.clear()

        def process_element(element):
            nonlocal current_path, sequence

            if isinstance(element, NavigableString):
                text = str(element).strip()
                if text:
                    text_buffer.append(text)
                return

            if not isinstance(element, Tag):
                return
            tag = element.name

            # 1. Header: Flush & Update Path
            if tag in header_tags:
                flush_text_buffer()
                header_text = element.get_text(strip=True)
                if header_text:
                    current_path = self._update_path(current_path, header_text, tag)
                return

            # 2. Table: Flush & Parse
            if tag == "table":
                flush_text_buffer()
                md_table, meta = self.convert_table_to_markdown(element)
                if md_table:
                    blocks.append(
                        {
                            "chunk_type": "table",
                            "section_path": current_path,
                            "content": md_table,
                            "sequence_order": sequence,
                            "table_metadata": meta,
                            "raw_content": md_table,
                        }
                    )
                    sequence += 1
                return

            # 3. Block Elements: Append text
            if tag in ["p", "li", "span", "td", "th", "div"]:
                # Note: div/section are containers but sometimes contain direct text
                pass

            # 4. Recursion
            if tag in [
                "div",
                "section",
                "article",
                "body",
                "tr",
                "tbody",
                "thead",
                "p",
                "li",
            ]:
                for child in element.children:
                    process_element(child)
                return

            # Fallback
            text = element.get_text(strip=True)
            if text:
                text_buffer.append(text)

        for child in container.children:
            process_element(child)

        flush_text_buffer()
        return blocks

    # ==================== Helpers ====================

    def _update_path(self, current: str, header: str, tag: str) -> str:
        """Ìó§Îçî Î†àÎ≤®Ïóê Îî∞Îùº ÏÑπÏÖò Í≤ΩÎ°ú ÏóÖÎç∞Ïù¥Ìä∏"""
        level = int(tag[1])
        parts = current.split(" > ") if current else []
        if level <= len(parts):
            parts = parts[: level - 1]
        parts.append(header)
        return " > ".join(parts)

    def _clean_text(self, text: str) -> str:
        text = re.sub(r"\n{3,}", "\n\n", text)
        text = re.sub(r" {2,}", " ", text)
        return text.replace("\xa0", " ").replace("\r", "").strip()

    def chunk_text(self, text: str) -> list[str]:
        """ÌÖçÏä§Ìä∏ Ï≤≠ÌÇπ"""
        chunk_size = CHUNK_CONFIG["max_chunk_size"]
        overlap = CHUNK_CONFIG["overlap"]
        min_size = CHUNK_CONFIG["min_chunk_size"]

        if len(text) <= chunk_size:
            return [text] if len(text) >= min_size else []

        chunks = []
        start = 0
        while start < len(text):
            end = start + chunk_size
            # Î¨∏Ïû• Í≤ΩÍ≥Ñ Î≥¥Ï†ï
            if end < len(text):
                for sep in ["\n\n", "\n", ". "]:
                    last_sep = text[start:end].rfind(sep)
                    if last_sep > chunk_size // 2:
                        end = start + last_sep + len(sep)
                        break

            chunk = text[start:end].strip()
            if len(chunk) >= min_size:
                chunks.append(chunk)
            elif chunks:  # ÎÑàÎ¨¥ ÏûëÏúºÎ©¥ Ïù¥Ï†Ñ Ï≤≠ÌÅ¨Ïóê Î≥ëÌï©
                chunks[-1] += " " + chunk

            start = end - overlap
        return chunks

    def convert_table_to_markdown(self, table_element) -> tuple[str, dict]:
        """HTML ÌÖåÏù¥Î∏î -> Markdown Î≥ÄÌôò"""
        try:
            dfs = pd.read_html(StringIO(str(table_element)), flavor="bs4")
            if not dfs:
                return "", {}
            df = dfs[0].fillna("")

            # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞
            meta = {
                "rows": len(df),
                "cols": len(df.columns),
                "columns": [str(c) for c in df.columns],
            }
            caption = table_element.find("caption")
            if caption:
                meta["title"] = caption.get_text(strip=True)

            return df.to_markdown(index=False), meta
        except Exception as e:
            text = table_element.get_text(separator=" ", strip=True)
            return f"[Ìëú Îç∞Ïù¥ÌÑ∞]\n{text}", {"error": str(e)}

    # Legacy Compatibility Methods (Optional)
    def get_corp_by_corp_code(self, corp_code: str):
        for c in self.corp_list:
            if c.corp_code == corp_code:
                return c
        return None
