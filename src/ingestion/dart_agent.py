"""
DART ë³´ê³ ì„œ ì—ì´ì „íŠ¸ ëª¨ë“ˆ - DART APIë¥¼ í†µí•œ ì‚¬ì—…ë³´ê³ ì„œ ìˆ˜ì§‘ ë° íŒŒì‹±
ìˆœì°¨ì  ë¸”ë¡ ì²˜ë¦¬(Sequential Block Processing) ì§€ì›
"""
import dart_fss as dart
from bs4 import BeautifulSoup, NavigableString, Tag
import re
import json
import time
from datetime import datetime, timedelta
import pandas as pd
from io import StringIO
from typing import Optional, List, Dict, Tuple

# [í†µí•© ì•„í‚¤í…ì²˜] ê³µí†µ ëª¨ë“ˆì—ì„œ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
from src.common.config import DART_CONFIG, CHUNK_CONFIG, TARGET_SECTIONS

# ë ˆê±°ì‹œ í˜¸í™˜ ë³€ìˆ˜
DART_API_KEY = DART_CONFIG.get("api_key")
REPORT_SEARCH_CONFIG = {
    "bgn_de": DART_CONFIG.get("search_start_date", "20240101"),
    "pblntf_detail_ty": DART_CONFIG.get("report_type_code", "a001"),
    "page_count": DART_CONFIG.get("page_count", 100),
    "page_delay_sec": DART_CONFIG.get("page_delay_sec", 0.5),
    "max_search_days": DART_CONFIG.get("max_search_days", 90),
}


class DartReportAgent:
    """
    DART APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ì—…ë³´ê³ ì„œë¥¼ ìˆ˜ì§‘í•˜ê³  íŒŒì‹±í•˜ëŠ” ì—ì´ì „íŠ¸
    """

    def __init__(self):
        """ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ë° DART API ì„¤ì •"""
        dart.set_api_key(api_key=DART_API_KEY)
        print("ğŸ”„ ê¸°ì—… ë¦¬ìŠ¤íŠ¸ ë¡œë”© ì¤‘...")
        self._corp_list = None

    @property
    def corp_list(self):
        """ê¸°ì—… ë¦¬ìŠ¤íŠ¸ (lazy loading)"""
        if self._corp_list is None:
            self._corp_list = dart.get_corp_list()
            print(f"âœ… ê¸°ì—… ë¦¬ìŠ¤íŠ¸ ë¡œë“œ ì™„ë£Œ: {len(self._corp_list)}ê°œ ê¸°ì—…")
        return self._corp_list

    # ==================== ê¸°ì—… ì¡°íšŒ ====================

    def get_corp_by_stock_code(self, stock_code: str):
        """ì¢…ëª©ì½”ë“œë¡œ ê¸°ì—… ì •ë³´ ì¡°íšŒ"""
        for corp in self.corp_list:
            if corp.stock_code == stock_code:
                return corp
        return None

    def get_corp_by_corp_code(self, corp_code: str):
        """ë²•ì¸ì½”ë“œë¡œ ê¸°ì—… ì •ë³´ ì¡°íšŒ"""
        for corp in self.corp_list:
            if corp.corp_code == corp_code:
                return corp
        return None

    def get_listed_corps(self) -> List:
        """ìƒì¥ ê¸°ì—…ë§Œ í•„í„°ë§ (ì‚¬ì—…ë³´ê³ ì„œ ì¡´ì¬ ê°€ëŠ¥ì„± ë†’ìŒ)"""
        return [c for c in self.corp_list if c.stock_code]

    def search_all_reports(
        self,
        bgn_de: str = None,
        end_de: str = None,
        corp_code: str = None
    ) -> List[Dict]:
        """
        ê¸°ê°„ ë‚´ ëª¨ë“  ì‚¬ì—…ë³´ê³ ì„œë¥¼ ì¼ê´„ ê²€ìƒ‰ (íš¨ìœ¨ì ì¸ ë°©ì‹)

        corp_codeê°€ ì—†ìœ¼ë©´ ê²€ìƒ‰ ê¸°ê°„ì€ ìµœëŒ€ 3ê°œì›”(90ì¼)ë¡œ ì œí•œë©ë‹ˆë‹¤.

        Args:
            bgn_de: ê²€ìƒ‰ ì‹œì‘ì¼ (YYYYMMDD), ê¸°ë³¸ê°’ì€ configì—ì„œ ê°€ì ¸ì˜´
            end_de: ê²€ìƒ‰ ì¢…ë£Œì¼ (YYYYMMDD), ê¸°ë³¸ê°’ì€ ì˜¤ëŠ˜
            corp_code: íŠ¹ì • ê¸°ì—…ë§Œ ê²€ìƒ‰í•  ê²½ìš° ë²•ì¸ì½”ë“œ ì§€ì •

        Returns:
            List[Dict]: ë³´ê³ ì„œ ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
                - corp_code, corp_name, stock_code, rcept_no, rcept_dt, report_nm ë“±
        """
        # ê¸°ë³¸ê°’ ì„¤ì •
        if end_de is None:
            end_de = datetime.now().strftime("%Y%m%d")
        if bgn_de is None:
            bgn_de = REPORT_SEARCH_CONFIG['bgn_de']

        # corp_codeê°€ ì—†ìœ¼ë©´ ê²€ìƒ‰ ê¸°ê°„ì„ ìµœëŒ€ 90ì¼(3ê°œì›”)ë¡œ ì œí•œ
        if corp_code is None:
            max_days = REPORT_SEARCH_CONFIG.get('max_search_days', 90)
            bgn_date = datetime.strptime(bgn_de, "%Y%m%d")
            end_date = datetime.strptime(end_de, "%Y%m%d")

            if (end_date - bgn_date).days > max_days:
                bgn_date = end_date - timedelta(days=max_days)
                bgn_de = bgn_date.strftime("%Y%m%d")
                print(f"âš ï¸ corp_code ë¯¸ì§€ì •: ê²€ìƒ‰ ê¸°ê°„ì„ ìµœëŒ€ {max_days}ì¼ë¡œ ì œí•œ ({bgn_de} ~ {end_de})")

        all_reports = []
        page_no = 1
        page_count = REPORT_SEARCH_CONFIG.get('page_count', 100)
        page_delay = REPORT_SEARCH_CONFIG.get('page_delay_sec', 0.5)

        print(f"ğŸ“‹ ì‚¬ì—…ë³´ê³ ì„œ ê²€ìƒ‰ ì‹œì‘: {bgn_de} ~ {end_de}")
        if corp_code:
            print(f"   ëŒ€ìƒ ê¸°ì—…: {corp_code}")

        while True:
            try:
                # dart.filings.search ì‚¬ìš© (ê¸°ê°„ ë‚´ ëª¨ë“  ì‚¬ì—…ë³´ê³ ì„œ ê²€ìƒ‰)
                search_kwargs = {
                    'bgn_de': bgn_de,
                    'end_de': end_de,
                    'pblntf_detail_ty': REPORT_SEARCH_CONFIG['pblntf_detail_ty'],
                    'page_count': page_count,
                    'page_no': page_no
                }

                # corp_codeê°€ ìˆìœ¼ë©´ íŠ¹ì • ê¸°ì—…ë§Œ ê²€ìƒ‰
                if corp_code:
                    search_kwargs['corp_code'] = corp_code

                search_result = dart.filings.search(**search_kwargs)

                # ê²°ê³¼ ì¶”ì¶œ (SearchResults ê°ì²´ëŠ” ì†ì„±ìœ¼ë¡œ ì ‘ê·¼)
                report_list = getattr(search_result, 'report_list', []) or []
                if not report_list:
                    break

                all_reports.extend(report_list)

                total_page = getattr(search_result, 'total_page', 1) or 1
                total_count = getattr(search_result, 'total_count', 0) or 0

                print(f"   ğŸ“„ Page {page_no}/{total_page}: {len(report_list)}ê±´ (ëˆ„ì  {len(all_reports)}/{total_count})")

                # ë§ˆì§€ë§‰ í˜ì´ì§€ë©´ ì¢…ë£Œ
                if page_no >= total_page:
                    break

                page_no += 1
                time.sleep(page_delay)  # Rate Limiting

            except Exception as e:
                print(f"âš ï¸ ë³´ê³ ì„œ ê²€ìƒ‰ ì˜¤ë¥˜ (page={page_no}): {e}")
                break

        print(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: ì´ {len(all_reports)}ê±´ì˜ ì‚¬ì—…ë³´ê³ ì„œ")
        return all_reports

    def get_corps_with_reports(
        self,
        bgn_de: str = None,
        end_de: str = None,
        deduplicate: bool = True
    ) -> List[Tuple]:
        """
        ì‚¬ì—…ë³´ê³ ì„œê°€ ìˆëŠ” ê¸°ì—… ëª©ë¡ ë°˜í™˜ (íš¨ìœ¨ì ì¸ ì¼ê´„ ê²€ìƒ‰ ë°©ì‹)

        ê¸°ì¡´ ë°©ì‹: ì „ì²´ ìƒì¥ì‚¬ ìˆœíšŒí•˜ë©° ê°œë³„ API í˜¸ì¶œ (ë¹„íš¨ìœ¨)
        ìƒˆë¡œìš´ ë°©ì‹: dart.filings.searchë¡œ ê¸°ê°„ ë‚´ ì‚¬ì—…ë³´ê³ ì„œ ì¼ê´„ ê²€ìƒ‰ (íš¨ìœ¨)

        Args:
            bgn_de: ê²€ìƒ‰ ì‹œì‘ì¼ (YYYYMMDD)
            end_de: ê²€ìƒ‰ ì¢…ë£Œì¼ (YYYYMMDD)
            deduplicate: Trueë©´ ê¸°ì—…ë‹¹ ìµœì‹  ë³´ê³ ì„œ 1ê±´ë§Œ ë°˜í™˜ (ê¸°ë³¸ê°’)

        Returns:
            List[Tuple]: (corp ê°ì²´, report ë”•ì…”ë„ˆë¦¬) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        # ì¼ê´„ ê²€ìƒ‰ìœ¼ë¡œ ì‚¬ì—…ë³´ê³ ì„œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        all_reports = self.search_all_reports(bgn_de=bgn_de, end_de=end_de)

        if not all_reports:
            return []

        # ê¸°ì—…ë³„ ìµœì‹  ë³´ê³ ì„œë§Œ ë‚¨ê¸°ê¸° (ì¤‘ë³µ ì œê±°)
        if deduplicate:
            corp_latest = {}
            for report in all_reports:
                # Report ê°ì²´ëŠ” ì†ì„±ìœ¼ë¡œ ì ‘ê·¼ (ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹˜)
                corp_code = getattr(report, 'corp_code', None)
                rcept_dt = getattr(report, 'rcept_dt', '')

                if corp_code not in corp_latest:
                    corp_latest[corp_code] = report
                else:
                    # ë” ìµœì‹  ë³´ê³ ì„œë¡œ êµì²´
                    if rcept_dt > getattr(corp_latest[corp_code], 'rcept_dt', ''):
                        corp_latest[corp_code] = report

            reports_to_process = list(corp_latest.values())
            print(f"ğŸ“Œ ì¤‘ë³µ ì œê±° í›„: {len(reports_to_process)}ê°œ ê¸°ì—…")
        else:
            reports_to_process = all_reports

        # (corp ê°ì²´, report ê°ì²´) íŠœí”Œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        corps_with_reports = []
        for report in reports_to_process:
            corp_code = getattr(report, 'corp_code', None)
            corp = self.get_corp_by_corp_code(corp_code)

            if corp:
                corps_with_reports.append((corp, report))
            else:
                # corp_listì— ì—†ëŠ” ê²½ìš° (ë¹„ìƒì¥ì‚¬ ë“±)
                corp_name = getattr(report, 'corp_name', 'Unknown')
                print(f"   âš ï¸ ê¸°ì—… ì •ë³´ ì—†ìŒ: {corp_name} ({corp_code})")

        return corps_with_reports

    # ==================== ë³´ê³ ì„œ ê²€ìƒ‰ ====================

    def get_annual_report(self, corp_code: str, bgn_de: str = None):
        """
        ì‚¬ì—…ë³´ê³ ì„œ ê²€ìƒ‰ (ê°€ì¥ ìµœê·¼ 1ê±´)

        Args:
            corp_code: ë²•ì¸ì½”ë“œ
            bgn_de: ê²€ìƒ‰ ì‹œì‘ì¼ (YYYYMMDD)

        Returns:
            Report ê°ì²´ ë˜ëŠ” None
        """
        bgn_de = bgn_de or REPORT_SEARCH_CONFIG['bgn_de']

        try:
            search_results = dart.search(
                corp_code=corp_code,
                bgn_de=bgn_de,
                pblntf_detail_ty=REPORT_SEARCH_CONFIG['pblntf_detail_ty']
            )
            return search_results[0] if search_results else None
        except Exception as e:
            print(f"âš ï¸ ë³´ê³ ì„œ ê²€ìƒ‰ ì˜¤ë¥˜ (corp_code={corp_code}): {e}")
            return None

    def get_report_info(self, report) -> Dict:
        """ë³´ê³ ì„œ ë©”íƒ€ì •ë³´ ì¶”ì¶œ"""
        return {
            "title": report.report_nm,
            "rcept_no": report.rcept_no,
            "rcept_dt": report.rcept_dt,
            "corp_code": report.corp_code,
            "corp_name": report.corp_name,
            "report_type": "annual"
        }

    # ==================== ì„¹ì…˜ ì¶”ì¶œ ====================

    def get_all_sections(self, report) -> List[Dict]:
        """
        ë³´ê³ ì„œì˜ ëª¨ë“  ì„¹ì…˜ ëª©ë¡ ì¡°íšŒ

        Returns:
            List[Dict]: ì„¹ì…˜ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        try:
            all_pages = report.find_all()
            pages = all_pages.get('pages', [])

            sections = []
            for i, page in enumerate(pages):
                sections.append({
                    "index": i,
                    "title": getattr(page, 'title', f'Page_{i}'),
                    "type": type(page).__name__
                })
            return sections
        except Exception as e:
            print(f"âš ï¸ ì„¹ì…˜ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    def extract_section(self, report, section_keyword: str) -> Optional[Dict]:
        """
        íŠ¹ì • í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ” ì„¹ì…˜ ì¶”ì¶œ

        Args:
            report: DART ë³´ê³ ì„œ ê°ì²´
            section_keyword: ì„¹ì…˜ ê²€ìƒ‰ í‚¤ì›Œë“œ

        Returns:
            Dict: {"section_name": str, "text": str, "tables": list, "page_count": int}
        """
        try:
            result = report.find_all(includes=section_keyword)
            pages = result.get('pages', [])

            if not pages:
                return None

            # ëª¨ë“  í˜ì´ì§€ í…ìŠ¤íŠ¸ ë³‘í•©
            full_text = ""
            tables = []

            for page in pages:
                soup = BeautifulSoup(page.html, 'html.parser')

                # í…Œì´ë¸” ì¶”ì¶œ
                for table in soup.find_all('table'):
                    table_data = self._parse_table(table)
                    if table_data:
                        tables.append(table_data)

                # í…ìŠ¤íŠ¸ ì¶”ì¶œ (í…Œì´ë¸” í¬í•¨)
                text = soup.get_text(separator='\n').strip()
                text = self._clean_text(text)
                full_text += text + "\n\n"

            return {
                "section_name": section_keyword,
                "text": full_text.strip(),
                "tables": tables,
                "page_count": len(pages)
            }

        except Exception as e:
            print(f"âš ï¸ ì„¹ì…˜ ì¶”ì¶œ ì‹¤íŒ¨ ({section_keyword}): {e}")
            return None

    def extract_target_sections(self, report) -> List[Dict]:
        """
        í•µì‹¬ ì„¹ì…˜ë“¤ ì¶”ì¶œ (config.pyì˜ TARGET_SECTIONS ê¸°ì¤€)

        Returns:
            List[Dict]: ì¶”ì¶œëœ ì„¹ì…˜ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        extracted = []

        for section_name in TARGET_SECTIONS:
            section_data = self.extract_section(report, section_name)
            if section_data:
                extracted.append(section_data)
                print(f"   âœ… '{section_name}' ì¶”ì¶œ ì™„ë£Œ ({section_data['page_count']}í˜ì´ì§€)")
            else:
                print(f"   âš ï¸ '{section_name}' ì„¹ì…˜ ì—†ìŒ")

        return extracted

    # ==================== ê³ ê¸‰ ì¶”ì¶œ (í…Œì´ë¸”/í…ìŠ¤íŠ¸ ë¶„ë¦¬) ====================

    def extract_page_data_with_tables(self, page) -> Dict:
        """
        DART í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ì™€ í…Œì´ë¸”ì„ ë¶„ë¦¬ ì¶”ì¶œ

        í…Œì´ë¸”ì€ tables_jsonì— ì €ì¥í•˜ê³ , content_textì—ëŠ” í…Œì´ë¸”ì´ ì œì™¸ëœ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ì €ì¥

        Args:
            page: DART í˜ì´ì§€ ê°ì²´

        Returns:
            Dict: {"title": str, "content_text": str, "tables": list}
        """
        html = page.html
        soup = BeautifulSoup(html, 'html.parser')

        # 1. ë¨¼ì € í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ (JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ)
        tables_json = []
        try:
            html_io = StringIO(str(soup))
            dfs = pd.read_html(html_io, flavor='bs4')

            for idx, df in enumerate(dfs):
                # NaN -> ë¹ˆ ë¬¸ìì—´
                df_clean = df.where(pd.notnull(df), "")
                # DataFrame -> JSON -> Dict ë³€í™˜ (Numpy íƒ€ì… í•´ê²°)
                json_str = df_clean.to_json(orient='records', force_ascii=False)
                table_data = json.loads(json_str)

                tables_json.append({
                    "table_index": idx,
                    "data": table_data
                })
        except ValueError:
            pass  # í…Œì´ë¸” ì—†ìŒ
        except Exception as e:
            print(f"   âš ï¸ í…Œì´ë¸” íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")

        # 2. HTMLì—ì„œ í…Œì´ë¸” ìš”ì†Œ ì œê±° í›„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        # ë³µì‚¬ë³¸ì—ì„œ ì‘ì—…
        soup_for_text = BeautifulSoup(html, 'html.parser')

        # ëª¨ë“  <table> íƒœê·¸ì™€ ê·¸ ë‚´ìš©ì„ ì œê±°
        for table in soup_for_text.find_all('table'):
            # í…Œì´ë¸” ìœ„ì¹˜ì— ë§ˆì»¤ ì¶”ê°€
            table.replace_with('[TABLE]')

        # í…Œì´ë¸” ì œê±° í›„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        content_text = soup_for_text.get_text(separator='\n').strip()
        content_text = self._clean_text(content_text)

        # [TABLE] ë§ˆì»¤ ì •ë¦¬ (ì—°ì†ëœ ë§ˆì»¤ ì œê±° ë° ì•ˆë‚´ë¬¸ ë³€í™˜)
        content_text = re.sub(r'\[TABLE\]\s*(\[TABLE\]\s*)+', '[TABLE]\n', content_text)
        content_text = re.sub(r'\[TABLE\]\s*', '\n[í…Œì´ë¸” ì°¸ì¡°]\n', content_text)

        return {
            "title": getattr(page, 'title', 'Unknown'),
            "content_text": content_text,
            "tables": tables_json
        }

    def parse_hierarchical_content(self, page_title: str, text: str) -> List[Dict]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ê³„ì¸µì ìœ¼ë¡œ íŒŒì‹± (chapter/section/sub_section)

        Args:
            page_title: ìƒìœ„ ì±•í„°ëª… (ì˜ˆ: "II. ì‚¬ì—…ì˜ ë‚´ìš©")
            text: íŒŒì‹±í•  í…ìŠ¤íŠ¸

        Returns:
            List[Dict]: íŒŒì‹±ëœ ì„¹ì…˜ ë¦¬ìŠ¤íŠ¸
        """
        # ì¤‘ë‹¨ì› íŒ¨í„´ (1. ì‚¬ì—…ì˜ ê°œìš”)
        main_pattern = re.compile(r'\n(\d+\.\s+[^\n]+)')
        # ì†Œë‹¨ì› íŒ¨í„´ (ê°€. ì—…ê³„ì˜ í˜„í™©)
        sub_pattern = re.compile(r'\n([ê°€-í•˜]\.\s+[^\n]+)')

        main_matches = list(main_pattern.finditer(text))
        parsed_data = []

        # ì¤‘ë‹¨ì›ì´ ì—†ëŠ” ê²½ìš° ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ì„¹ì…˜ìœ¼ë¡œ ì²˜ë¦¬
        if not main_matches:
            parsed_data.append({
                'chapter': page_title,
                'section': page_title,
                'sub_section': None,
                'content': text.strip()
            })
            return parsed_data

        for i in range(len(main_matches)):
            m_start = main_matches[i].start()
            m_title = main_matches[i].group(1).strip()

            if i < len(main_matches) - 1:
                m_end = main_matches[i + 1].start()
            else:
                m_end = len(text)

            m_content = text[m_start + len(m_title) + 1: m_end]

            # ì†Œë‹¨ì›ìœ¼ë¡œ ì¬ë¶„í• 
            sub_matches = list(sub_pattern.finditer(m_content))

            if not sub_matches:
                # ì†Œë‹¨ì›ì´ ì—†ìœ¼ë©´ ì¤‘ë‹¨ì› ì „ì²´ë¥¼ ì €ì¥
                parsed_data.append({
                    'chapter': page_title,
                    'section': m_title,
                    'sub_section': None,
                    'content': m_content.strip()
                })
            else:
                for j in range(len(sub_matches)):
                    s_start = sub_matches[j].start()
                    s_title = sub_matches[j].group(1).strip()

                    if j < len(sub_matches) - 1:
                        s_end = sub_matches[j + 1].start()
                    else:
                        s_end = len(m_content)

                    s_content = m_content[s_start + len(s_title) + 1: s_end]

                    parsed_data.append({
                        'chapter': page_title,
                        'section': m_title,
                        'sub_section': s_title,
                        'content': s_content.strip()
                    })

        return parsed_data

    def extract_section_advanced(self, report, section_keyword: str) -> Optional[Dict]:
        """
        ê³ ê¸‰ ì„¹ì…˜ ì¶”ì¶œ - í˜ì´ì§€ë³„ë¡œ í…ìŠ¤íŠ¸ì™€ í…Œì´ë¸” ë™ê¸°í™”

        ê° í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ì™€ í…Œì´ë¸”ì„ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ë™ê¸°í™” ë³´ì¥

        Args:
            report: DART ë³´ê³ ì„œ ê°ì²´
            section_keyword: ì„¹ì…˜ ê²€ìƒ‰ í‚¤ì›Œë“œ

        Returns:
            Dict: {"chapter": str, "pages_data": list, "page_count": int}
        """
        try:
            result = report.find_all(includes=section_keyword)
            pages = result.get('pages', [])

            if not pages:
                return None

            # í˜ì´ì§€ë³„ë¡œ ì²˜ë¦¬í•˜ì—¬ í…ìŠ¤íŠ¸-í…Œì´ë¸” ë™ê¸°í™”
            pages_data = []

            for page in pages:
                page_data = self.extract_page_data_with_tables(page)
                page_title = page_data['title']  # ì‹¤ì œ í˜ì´ì§€ ì œëª© ì‚¬ìš©
                content_text = page_data['content_text']
                tables = page_data['tables']

                # ì´ í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ë¥¼ ê³„ì¸µì ìœ¼ë¡œ íŒŒì‹±
                parsed_sections = self.parse_hierarchical_content(page_title, content_text)

                # [í…Œì´ë¸” ì°¸ì¡°] ë§ˆì»¤ê°€ ìˆëŠ” ì„¹ì…˜ì— í•´ë‹¹ í…Œì´ë¸” ì—°ê²°
                table_idx = 0
                for parsed in parsed_sections:
                    content = parsed.get('content', '')
                    marker_count = content.count('[í…Œì´ë¸” ì°¸ì¡°]')

                    if marker_count > 0 and table_idx < len(tables):
                        # ë§ˆì»¤ ê°œìˆ˜ë§Œí¼ í…Œì´ë¸” í• ë‹¹
                        parsed['tables'] = tables[table_idx:table_idx + marker_count]
                        table_idx += marker_count
                    else:
                        parsed['tables'] = []

                pages_data.append({
                    'page_title': page_title,
                    'sections': parsed_sections,
                    'tables_in_page': len(tables)
                })

            return {
                "chapter": section_keyword,
                "pages_data": pages_data,
                "page_count": len(pages)
            }

        except Exception as e:
            print(f"âš ï¸ ê³ ê¸‰ ì„¹ì…˜ ì¶”ì¶œ ì‹¤íŒ¨ ({section_keyword}): {e}")
            import traceback
            traceback.print_exc()
            return None

    def extract_target_sections_advanced(self, report) -> List[Dict]:
        """
        í•µì‹¬ ì„¹ì…˜ë“¤ ê³ ê¸‰ ì¶”ì¶œ (í…Œì´ë¸”/í…ìŠ¤íŠ¸ ë¶„ë¦¬, í˜ì´ì§€ë³„ ë™ê¸°í™”)

        Returns:
            List[Dict]: ì¶”ì¶œëœ ì„¹ì…˜ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        extracted = []

        for section_name in TARGET_SECTIONS:
            section_data = self.extract_section_advanced(report, section_name)
            if section_data:
                extracted.append(section_data)
                page_count = section_data['page_count']
                total_sections = sum(len(p['sections']) for p in section_data['pages_data'])
                total_tables = sum(p['tables_in_page'] for p in section_data['pages_data'])
                print(f"   âœ… '{section_name}' ì¶”ì¶œ ì™„ë£Œ "
                      f"({page_count}í˜ì´ì§€, {total_sections}ê°œ ì„¹ì…˜, {total_tables}ê°œ í…Œì´ë¸”)")
            else:
                print(f"   âš ï¸ '{section_name}' ì„¹ì…˜ ì—†ìŒ")

        return extracted

    # ==================== ì²­í‚¹ ====================

    def chunk_text(
        self,
        text: str,
        chunk_size: int = None,
        overlap: int = None
    ) -> List[str]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 

        Args:
            text: ë¶„í• í•  í…ìŠ¤íŠ¸
            chunk_size: ì²­í¬ ìµœëŒ€ í¬ê¸°
            overlap: ì²­í¬ ê°„ ì˜¤ë²„ë©

        Returns:
            List[str]: ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        chunk_size = chunk_size or CHUNK_CONFIG['max_chunk_size']
        overlap = overlap or CHUNK_CONFIG['overlap']
        min_size = CHUNK_CONFIG['min_chunk_size']

        if len(text) <= chunk_size:
            return [text] if len(text) >= min_size else []

        chunks = []
        start = 0

        while start < len(text):
            end = start + chunk_size

            # ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸° ì‹œë„
            if end < len(text):
                # ë§ˆì¹¨í‘œ, ì¤„ë°”ê¿ˆ ë“±ì—ì„œ ìë¥´ê¸°
                for sep in ['\n\n', '\n', '. ', 'ë‹¤. ', 'ìš”. ']:
                    last_sep = text[start:end].rfind(sep)
                    if last_sep > chunk_size // 2:  # ìµœì†Œ ì ˆë°˜ ì´ìƒì¼ ë•Œë§Œ
                        end = start + last_sep + len(sep)
                        break

            chunk = text[start:end].strip()

            if len(chunk) >= min_size:
                chunks.append(chunk)
            elif chunks:
                # ë„ˆë¬´ ì‘ìœ¼ë©´ ì´ì „ ì²­í¬ì— ë³‘í•©
                chunks[-1] += " " + chunk

            start = end - overlap

            # ë¬´í•œ ë£¨í”„ ë°©ì§€
            if start >= len(text) - min_size:
                break

        return chunks

    def chunk_section(self, section_data: Dict) -> List[Dict]:
        """
        ì„¹ì…˜ ë°ì´í„°ë¥¼ ì²­í¬ë¡œ ë¶„í• 

        Args:
            section_data: extract_section()ì˜ ë°˜í™˜ê°’

        Returns:
            List[Dict]: ì²­í¬ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        text = section_data.get('text', '')
        section_name = section_data.get('section_name', 'Unknown')
        tables = section_data.get('tables', [])

        chunks = self.chunk_text(text)

        result = []
        for idx, chunk in enumerate(chunks):
            result.append({
                "section_name": section_name,
                "chunk_index": idx,
                "content": chunk,
                "metadata": {
                    "total_chunks": len(chunks),
                    "has_tables": len(tables) > 0,
                    "tables": tables if idx == 0 else []  # ì²« ì²­í¬ì—ë§Œ í…Œì´ë¸” í¬í•¨
                }
            })

        return result

    def chunk_section_advanced(self, section_data: Dict) -> List[Dict]:
        """
        ê³ ê¸‰ ì„¹ì…˜ ì²­í‚¹ - í˜ì´ì§€ë³„ ë°ì´í„°ì—ì„œ ì²­í¬ ìƒì„±

        ê° í˜ì´ì§€ì˜ ì„¹ì…˜ê³¼ í…Œì´ë¸”ì´ ì´ë¯¸ ë™ê¸°í™”ë˜ì–´ ìˆìŒ

        Args:
            section_data: extract_section_advanced()ì˜ ë°˜í™˜ê°’

        Returns:
            List[Dict]: ì²­í¬ ì •ë³´ ë¦¬ìŠ¤íŠ¸ (chapter, section_name, sub_section, content, tables í¬í•¨)
        """
        search_keyword = section_data.get('chapter', 'Unknown')
        pages_data = section_data.get('pages_data', [])

        result = []
        global_chunk_idx = 0

        for page_info in pages_data:
            page_title = page_info.get('page_title', search_keyword)
            sections = page_info.get('sections', [])

            for parsed in sections:
                # í˜ì´ì§€ ì œëª©ì„ chapterë¡œ ì‚¬ìš© (ì‹¤ì œ DART í˜ì´ì§€ ì œëª©)
                chapter = parsed.get('chapter', page_title)
                section_name = parsed.get('section', page_title)
                sub_section = parsed.get('sub_section')
                content = parsed.get('content', '')
                tables = parsed.get('tables', [])  # ì´ë¯¸ ë™ê¸°í™”ëœ í…Œì´ë¸”

                if not content or len(content.strip()) < CHUNK_CONFIG['min_chunk_size']:
                    continue

                # ì½˜í…ì¸ ê°€ ì²­í¬ í¬ê¸°ë³´ë‹¤ í¬ë©´ ë¶„í• 
                chunks = self.chunk_text(content)

                for idx, chunk in enumerate(chunks):
                    # ì²« ë²ˆì§¸ ì²­í¬ì—ë§Œ í•´ë‹¹ ì„¹ì…˜ì˜ í…Œì´ë¸” ì—°ê²°
                    chunk_tables = tables if idx == 0 and tables else None

                    result.append({
                        "chapter": chapter,
                        "section_name": section_name,
                        "sub_section": sub_section,
                        "chunk_index": global_chunk_idx,
                        "content": chunk,
                        "tables": chunk_tables,
                        "metadata": {
                            "local_chunk_index": idx,
                            "total_local_chunks": len(chunks),
                            "has_tables": chunk_tables is not None and len(chunk_tables) > 0,
                            "table_count": len(chunk_tables) if chunk_tables else 0,
                            "page_title": page_title
                        }
                    })
                    global_chunk_idx += 1

        return result

    # ==================== ìœ í‹¸ë¦¬í‹° ====================

    def _clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ì œ"""
        # ì—°ì† ê³µë°±/ì¤„ë°”ê¿ˆ ì •ë¦¬
        text = re.sub(r'\n{3,}', '\n\n', text)
        text = re.sub(r' {2,}', ' ', text)
        text = re.sub(r'\t+', ' ', text)

        # ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°
        text = text.replace('\xa0', ' ')
        text = text.replace('\r', '')

        return text.strip()

    def _parse_table(self, table_element) -> Optional[List[Dict]]:
        """HTML í…Œì´ë¸”ì„ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ íŒŒì‹±"""
        try:
            rows = table_element.find_all('tr')
            if not rows:
                return None

            # í—¤ë” ì¶”ì¶œ
            header_row = rows[0]
            headers = []
            for th in header_row.find_all(['th', 'td']):
                headers.append(th.get_text(strip=True))

            if not headers:
                return None

            # ë°ì´í„° í–‰ ì¶”ì¶œ
            data = []
            for row in rows[1:]:
                cells = row.find_all(['td', 'th'])
                if len(cells) == len(headers):
                    row_data = {}
                    for i, cell in enumerate(cells):
                        row_data[headers[i]] = cell.get_text(strip=True)
                    data.append(row_data)

            return data if data else None

        except Exception:
            return None

    # ==================== ìˆœì°¨ì  ë¸”ë¡ ì²˜ë¦¬ (Sequential Block Processing) ====================

    def convert_table_to_markdown(self, table_element) -> Tuple[str, Dict]:
        """
        HTML í…Œì´ë¸”ì„ Markdown í˜•ì‹ìœ¼ë¡œ ë³€í™˜

        Args:
            table_element: BeautifulSoup table ìš”ì†Œ

        Returns:
            Tuple[str, Dict]: (Markdown í…Œì´ë¸” ë¬¸ìì—´, ë©”íƒ€ë°ì´í„°)
        """
        try:
            # pandasë¡œ í…Œì´ë¸” íŒŒì‹±
            html_str = str(table_element)
            dfs = pd.read_html(StringIO(html_str), flavor='bs4')

            if not dfs:
                return "", {}

            df = dfs[0]

            # NaN ì²˜ë¦¬
            df = df.fillna('')

            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            metadata = {
                "rows": len(df),
                "cols": len(df.columns),
                "columns": [str(col) for col in df.columns.tolist()]
            }

            # í…Œì´ë¸” ì œëª© ì¶”ì¶œ ì‹œë„ (caption ë˜ëŠ” ì²« ë²ˆì§¸ í–‰)
            caption = table_element.find('caption')
            if caption:
                metadata["title"] = caption.get_text(strip=True)

            # Markdown í…Œì´ë¸” ìƒì„±
            markdown_lines = []

            # í—¤ë” í–‰ - ì…€ ë‚´ìš© ì •ë¦¬
            headers = []
            for col in df.columns:
                header_text = str(col).replace("|", "ï½œ").replace("\n", " ").strip()
                headers.append(header_text)

            markdown_lines.append("| " + " | ".join(headers) + " |")
            markdown_lines.append("|" + "|".join(["---"] * len(headers)) + "|")

            # ë°ì´í„° í–‰
            for _, row in df.iterrows():
                cells = []
                for val in row:
                    cell_text = str(val).replace("|", "ï½œ").replace("\n", " ").strip()
                    cells.append(cell_text)
                markdown_lines.append("| " + " | ".join(cells) + " |")

            return "\n".join(markdown_lines), metadata

        except Exception as e:
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ë¡œ ì¶”ì¶œ
            text = table_element.get_text(separator=' ', strip=True)
            return f"[í‘œ ë°ì´í„°]\n{text}", {"error": str(e)}

    def extract_section_sequential(self, report, section_keyword: str) -> Optional[Dict]:
        """
        ìˆœì°¨ì  ë¸”ë¡ ì²˜ë¦¬ ë°©ì‹ìœ¼ë¡œ ì„¹ì…˜ ì¶”ì¶œ

        HTMLì„ ìœ„ì—ì„œ ì•„ë˜ë¡œ ì½ìœ¼ë©° ë§Œë‚˜ëŠ” ìˆœì„œëŒ€ë¡œ ë¸”ë¡ ìˆ˜ì§‘

        Args:
            report: DART ë³´ê³ ì„œ ê°ì²´
            section_keyword: ì„¹ì…˜ ê²€ìƒ‰ í‚¤ì›Œë“œ

        Returns:
            Dict: {"chapter": str, "blocks": list, "page_count": int}
        """
        try:
            result = report.find_all(includes=section_keyword)
            pages = result.get('pages', [])

            if not pages:
                return None

            all_blocks = []
            global_sequence = 0

            for page in pages:
                soup = BeautifulSoup(page.html, 'html.parser')
                page_title = getattr(page, 'title', section_keyword)

                # í˜„ì¬ ì„¹ì…˜ ê²½ë¡œ ì´ˆê¸°í™”
                current_path = page_title

                # í˜ì´ì§€ì˜ ë¸”ë¡ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬
                blocks, global_sequence = self._parse_sequential_blocks(
                    soup.body if soup.body else soup,
                    current_path,
                    global_sequence
                )
                all_blocks.extend(blocks)

            return {
                "chapter": section_keyword,
                "blocks": all_blocks,
                "page_count": len(pages)
            }

        except Exception as e:
            print(f"âš ï¸ ìˆœì°¨ì  ì„¹ì…˜ ì¶”ì¶œ ì‹¤íŒ¨ ({section_keyword}): {e}")
            import traceback
            traceback.print_exc()
            return None

    def _parse_sequential_blocks(
        self,
        container,
        current_path: str,
        start_sequence: int
    ) -> Tuple[List[Dict], int]:
        """
        ì»¨í…Œì´ë„ˆ ë‚´ì˜ ìš”ì†Œë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ íŒŒì‹±

        Args:
            container: BeautifulSoup ìš”ì†Œ (body ë˜ëŠ” div)
            current_path: í˜„ì¬ ì„¹ì…˜ ê²½ë¡œ
            start_sequence: ì‹œì‘ ì‹œí€€ìŠ¤ ë²ˆí˜¸

        Returns:
            Tuple[List[Dict], int]: (ë¸”ë¡ ë¦¬ìŠ¤íŠ¸, ë‹¤ìŒ ì‹œí€€ìŠ¤ ë²ˆí˜¸)
        """
        blocks = []
        sequence = start_sequence
        text_buffer = []  # í…ìŠ¤íŠ¸ ëˆ„ì  ë²„í¼

        # í—¤ë” íƒœê·¸ íŒ¨í„´
        header_tags = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']

        def flush_text_buffer():
            """ëˆ„ì ëœ í…ìŠ¤íŠ¸ë¥¼ ë¸”ë¡ìœ¼ë¡œ ì €ì¥"""
            nonlocal sequence
            if text_buffer:
                combined_text = '\n'.join(text_buffer).strip()
                combined_text = self._clean_text(combined_text)

                if len(combined_text) >= CHUNK_CONFIG['min_chunk_size']:
                    # ì²­í¬ í¬ê¸°ê°€ í¬ë©´ ë¶„í• 
                    chunks = self.chunk_text(combined_text)
                    for chunk in chunks:
                        blocks.append({
                            "chunk_type": "text",
                            "section_path": current_path,
                            "content": chunk,
                            "sequence_order": sequence,
                            "table_metadata": None
                        })
                        sequence += 1
                text_buffer.clear()

        def process_element(element):
            """ë‹¨ì¼ ìš”ì†Œ ì²˜ë¦¬"""
            nonlocal current_path, sequence

            if isinstance(element, NavigableString):
                text = str(element).strip()
                if text:
                    text_buffer.append(text)
                return

            if not isinstance(element, Tag):
                return

            tag_name = element.name

            # 1. í—¤ë” íƒœê·¸ -> ê²½ë¡œ ì—…ë°ì´íŠ¸
            if tag_name in header_tags:
                flush_text_buffer()
                header_text = element.get_text(strip=True)
                if header_text:
                    current_path = self._update_section_path(current_path, header_text, tag_name)
                return

            # 2. í…Œì´ë¸” -> 'table' íƒ€ì…ìœ¼ë¡œ ì €ì¥
            if tag_name == 'table':
                flush_text_buffer()
                markdown_table, table_meta = self.convert_table_to_markdown(element)
                if markdown_table:
                    blocks.append({
                        "chunk_type": "table",
                        "section_path": current_path,
                        "content": markdown_table,
                        "sequence_order": sequence,
                        "table_metadata": table_meta
                    })
                    sequence += 1
                return

            # 3. í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•˜ëŠ” ë¸”ë¡ ìš”ì†Œ (p, div, span ë“±)
            if tag_name in ['p', 'li', 'span', 'td', 'th']:
                text = element.get_text(strip=True)
                if text:
                    text_buffer.append(text)
                return

            # 4. ì»¨í…Œì´ë„ˆ ìš”ì†ŒëŠ” ìì‹ ìˆœíšŒ
            if tag_name in ['div', 'section', 'article', 'body', 'tr', 'tbody', 'thead']:
                for child in element.children:
                    process_element(child)
                return

            # 5. ê¸°íƒ€ íƒœê·¸ëŠ” í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text = element.get_text(strip=True)
            if text:
                text_buffer.append(text)

        # ì»¨í…Œì´ë„ˆì˜ ì§ê³„ ìì‹ë“¤ ìˆœíšŒ
        for child in container.children:
            process_element(child)

        # ë‚¨ì€ í…ìŠ¤íŠ¸ ë²„í¼ ì²˜ë¦¬
        flush_text_buffer()

        return blocks, sequence

    def _update_section_path(self, current_path: str, header_text: str, tag_name: str) -> str:
        """
        í—¤ë”ë¥¼ ë§Œë‚¬ì„ ë•Œ ì„¹ì…˜ ê²½ë¡œ ì—…ë°ì´íŠ¸

        Args:
            current_path: í˜„ì¬ ê²½ë¡œ
            header_text: í—¤ë” í…ìŠ¤íŠ¸
            tag_name: í—¤ë” íƒœê·¸ëª… (h1, h2, ...)

        Returns:
            str: ì—…ë°ì´íŠ¸ëœ ê²½ë¡œ
        """
        # í—¤ë” ë ˆë²¨ ì¶”ì¶œ (h1=1, h2=2, ...)
        level = int(tag_name[1])

        # ê²½ë¡œë¥¼ ' > 'ë¡œ ë¶„í• 
        path_parts = current_path.split(' > ') if current_path else []

        # í˜„ì¬ ë ˆë²¨ì— ë§ê²Œ ê²½ë¡œ ì¡°ì •
        # h1ì€ ë£¨íŠ¸, h2ëŠ” ì²« ë²ˆì§¸ í•˜ìœ„, ...
        if level <= len(path_parts):
            path_parts = path_parts[:level-1]

        path_parts.append(header_text)

        return ' > '.join(path_parts)

    def extract_target_sections_sequential(self, report) -> List[Dict]:
        """
        í•µì‹¬ ì„¹ì…˜ë“¤ì„ ìˆœì°¨ì  ë¸”ë¡ ì²˜ë¦¬ ë°©ì‹ìœ¼ë¡œ ì¶”ì¶œ

        Returns:
            List[Dict]: ì¶”ì¶œëœ ì„¹ì…˜ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        extracted = []
        global_sequence = 0  # ì „ì²´ ë¬¸ì„œì—ì„œ ì—°ì†ë˜ëŠ” ì‹œí€€ìŠ¤ ë²ˆí˜¸

        for section_name in TARGET_SECTIONS:
            section_data = self.extract_section_sequential(report, section_name)
            if section_data:
                # ì‹œí€€ìŠ¤ ë²ˆí˜¸ë¥¼ ì „ì—­ì ìœ¼ë¡œ ì¬ì¡°ì •
                blocks = section_data.get('blocks', [])
                for block in blocks:
                    block['sequence_order'] = global_sequence
                    global_sequence += 1

                extracted.append(section_data)
                block_count = len(blocks)
                text_blocks = sum(1 for b in blocks if b['chunk_type'] == 'text')
                table_blocks = sum(1 for b in blocks if b['chunk_type'] == 'table')
                print(f"   âœ… '{section_name}' ì¶”ì¶œ ì™„ë£Œ "
                      f"({section_data['page_count']}í˜ì´ì§€, {block_count}ë¸”ë¡: í…ìŠ¤íŠ¸ {text_blocks}, í…Œì´ë¸” {table_blocks})")
            else:
                print(f"   âš ï¸ '{section_name}' ì„¹ì…˜ ì—†ìŒ")

        return extracted

