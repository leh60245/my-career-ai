"""
í†µí•© ìž„ë² ë”© ì„œë¹„ìŠ¤ (Unified Embedding Service)

AIì™€ Ingestion ì–‘ìª½ì—ì„œ ë™ì¼í•œ ìž„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•˜ë„ë¡ ê°•ì œí•©ë‹ˆë‹¤.
ì´ë¥¼ í†µí•´ DBì— ì €ìž¥ëœ ë²¡í„°ì™€ ê²€ìƒ‰ ì‹œ ìƒì„±í•˜ëŠ” ë²¡í„°ì˜ ì¼ê´€ì„±ì„ ë³´ìž¥í•©ë‹ˆë‹¤.

ì§€ì› í”„ë¡œë°”ì´ë”:
- huggingface: sentence-transformers/paraphrase-multilingual-mpnet-base-v2 (768ì°¨ì›, ê¸°ë³¸ê°’)
- openai: text-embedding-3-small (1536ì°¨ì›)

âš ï¸ ì¤‘ìš”: DBì— ì´ë¯¸ ì €ìž¥ëœ ìž„ë² ë”©ê³¼ ë™ì¼í•œ ëª¨ë¸ì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤!
í”„ë¡œë°”ì´ë”ë¥¼ ë³€ê²½í•˜ë©´ ê¸°ì¡´ ë°ì´í„° ìž¬ìž„ë² ë”©ì´ í•„ìš”í•©ë‹ˆë‹¤.

ì‚¬ìš© ì˜ˆì‹œ:
    # ê¸°ë³¸ ì‚¬ìš© (configì—ì„œ provider ìžë™ ê²°ì •)
    service = EmbeddingService()
    embedding = service.embed_text("ì‚¼ì„±ì „ìž ë§¤ì¶œ í˜„í™©")

    # ëª…ì‹œì  í”„ë¡œë°”ì´ë” ì§€ì •
    service = EmbeddingService(provider="huggingface")
    embeddings = service.embed_texts(["í…ìŠ¤íŠ¸1", "í…ìŠ¤íŠ¸2"])
"""
import os
import logging
from typing import List, Union, Optional, Literal
from abc import ABC, abstractmethod

import numpy as np

from .config import EMBEDDING_CONFIG

logger = logging.getLogger(__name__)


def get_optimal_device() -> str:
    """Return the best available accelerator in priority order: cuda > mps > cpu."""
    import torch

    if torch.cuda.is_available():
        return "cuda"
    if torch.backends.mps.is_available():
        return "mps"
    return "cpu"


class BaseEmbedder(ABC):
    """ìž„ë² ë”© ìƒì„±ê¸° ê¸°ë³¸ í´ëž˜ìŠ¤"""

    @abstractmethod
    def embed_text(self, text: str) -> List[float]:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ ìž„ë² ë”©"""
        pass

    @abstractmethod
    def embed_texts(self, texts: List[str]) -> List[List[float]]:
        """ë°°ì¹˜ í…ìŠ¤íŠ¸ ìž„ë² ë”©"""
        pass

    @abstractmethod
    def get_dimension(self) -> int:
        """ìž„ë² ë”© ì°¨ì› ìˆ˜ ë°˜í™˜"""
        pass


class HuggingFaceEmbedder(BaseEmbedder):
    """
    HuggingFace ê¸°ë°˜ ìž„ë² ë”© ìƒì„±ê¸°

    sentence-transformers ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ 768ì°¨ì› ìž„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.
    GPUê°€ ìžˆìœ¼ë©´ ìžë™ìœ¼ë¡œ CUDAë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """

    def __init__(
        self,
        model_name: str = None,
        device: str = None,
        batch_size: int = None,
    ):
        self.model_name = model_name or EMBEDDING_CONFIG["hf_model"]
        self.batch_size = batch_size or EMBEDDING_CONFIG["batch_size"]

        # Lazy import (transformersê°€ ë¬´ê±°ìš°ë¯€ë¡œ)
        import torch
        from transformers import AutoTokenizer, AutoModel

        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if device is None:
            self.device = get_optimal_device()
        else:
            self.device = device
            if self.device.startswith("cuda") and not torch.cuda.is_available():
                logger.warning("Requested CUDA device but CUDA is unavailable. Falling back to CPU.")
                self.device = "cpu"
            elif self.device == "mps" and not torch.backends.mps.is_available():
                logger.warning("Requested MPS device but MPS is unavailable. Falling back to CPU.")
                self.device = "cpu"

        logger.info(f"ðŸ”„ Loading HuggingFace embedding model: {self.model_name}")
        logger.info(f"ðŸš€ [System] Embedding Model loaded on: {self.device.upper()}")

        # ëª¨ë¸ ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModel.from_pretrained(self.model_name)
        self.model.to(self.device)
        self.model.eval()

        self._dimension = self.model.config.hidden_size
        logger.info(f"âœ… Model loaded (dimension: {self._dimension})")

    def get_dimension(self) -> int:
        return self._dimension

    def _mean_pooling(self, model_output, attention_mask):
        """Mean Pooling - attention maskë¥¼ ê³ ë ¤í•œ í‰ê· """
        import torch

        token_embeddings = model_output[0]
        input_mask_expanded = (
            attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        )
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(
            input_mask_expanded.sum(1), min=1e-9
        )

    def embed_text(self, text: str) -> List[float]:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ ìž„ë² ë”©"""
        return self.embed_texts([text])[0]

    def embed_texts(self, texts: List[str]) -> List[List[float]]:
        """ë°°ì¹˜ í…ìŠ¤íŠ¸ ìž„ë² ë”©"""
        import torch

        all_embeddings = []

        for i in range(0, len(texts), self.batch_size):
            batch_texts = texts[i : i + self.batch_size]

            # í† í°í™”
            encoded_input = self.tokenizer(
                batch_texts,
                padding=True,
                truncation=True,
                max_length=EMBEDDING_CONFIG["max_length"],
                return_tensors="pt",
            )
            encoded_input = {k: v.to(self.device) for k, v in encoded_input.items()}

            # ìž„ë² ë”© ìƒì„±
            with torch.no_grad():
                model_output = self.model(**encoded_input)

            # Mean pooling
            embeddings = self._mean_pooling(
                model_output, encoded_input["attention_mask"]
            )

            # ì •ê·œí™” (ìœ ì‚¬ë„ ê²€ìƒ‰ì— ìœ ìš©)
            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)

            # CPUë¡œ ì´ë™ í›„ ë¦¬ìŠ¤íŠ¸ ë³€í™˜
            embeddings = embeddings.cpu().tolist()
            all_embeddings.extend(embeddings)

        return all_embeddings


class OpenAIEmbedder(BaseEmbedder):
    """
    OpenAI API ê¸°ë°˜ ìž„ë² ë”© ìƒì„±ê¸°

    text-embedding-3-small ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ 1536ì°¨ì› ìž„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.
    LiteLLMì„ í†µí•´ ìºì‹±ê³¼ ìž¬ì‹œë„ ë¡œì§ì„ ì§€ì›í•©ë‹ˆë‹¤.
    """

    def __init__(
        self,
        model_name: str = None,
        api_key: str = None,
        max_workers: int = 5,
    ):
        self.model_name = model_name or EMBEDDING_CONFIG["openai_model"]
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        self.max_workers = max_workers
        self._dimension = EMBEDDING_CONFIG["openai_dimension"]
        self.total_token_usage = 0

        if not self.api_key:
            raise RuntimeError(
                "OPENAI_API_KEY is required for OpenAI embeddings. "
                "Please set it in .env or as environment variable."
            )

        # LiteLLM ì„¤ì • (ìºì‹±)
        self._setup_litellm()

        logger.info(f"âœ… OpenAI Embedder initialized: {self.model_name}")

    def _setup_litellm(self):
        """LiteLLM ìºì‹œ ì„¤ì •"""
        import warnings
        from pathlib import Path

        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", category=UserWarning)
            if "LITELLM_LOCAL_MODEL_COST_MAP" not in os.environ:
                os.environ["LITELLM_LOCAL_MODEL_COST_MAP"] = "True"

            import litellm
            from litellm.caching.caching import Cache

            litellm.drop_params = True
            litellm.telemetry = False

            disk_cache_dir = os.path.join(Path.home(), ".storm_local_cache")
            litellm.cache = Cache(disk_cache_dir=disk_cache_dir, type="disk")

            self._litellm = litellm

    def get_dimension(self) -> int:
        return self._dimension

    def _get_single_embedding(self, text: str):
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ ìž„ë² ë”© (ë‚´ë¶€ìš©)"""
        response = self._litellm.embedding(
            model=self.model_name,
            input=text,
            caching=True,
            api_key=self.api_key,
        )
        embedding = response.data[0]["embedding"]
        token_usage = response.get("usage", {}).get("total_tokens", 0)
        return text, embedding, token_usage

    def embed_text(self, text: str) -> List[float]:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ ìž„ë² ë”©"""
        _, embedding, tokens = self._get_single_embedding(text)
        self.total_token_usage += tokens
        return embedding

    def embed_texts(self, texts: List[str]) -> List[List[float]]:
        """ë°°ì¹˜ í…ìŠ¤íŠ¸ ìž„ë² ë”© (ë³‘ë ¬ ì²˜ë¦¬)"""
        from concurrent.futures import ThreadPoolExecutor, as_completed

        if len(texts) == 1:
            return [self.embed_text(texts[0])]

        embeddings = []
        total_tokens = 0

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {
                executor.submit(self._get_single_embedding, text): text
                for text in texts
            }

            for future in as_completed(futures):
                try:
                    text, embedding, tokens = future.result()
                    embeddings.append((text, embedding, tokens))
                    total_tokens += tokens
                except Exception as e:
                    logger.error(f"Embedding error for text: {futures[future][:50]}...")
                    logger.error(e)
                    # ì—ëŸ¬ ì‹œ ë¹ˆ ë²¡í„° ì¶”ê°€ (ì°¨ì› ìœ ì§€)
                    embeddings.append((futures[future], [0.0] * self._dimension, 0))

        # ì›ë³¸ ìˆœì„œëŒ€ë¡œ ì •ë ¬
        embeddings.sort(key=lambda x: texts.index(x[0]))
        self.total_token_usage += total_tokens

        return [e[1] for e in embeddings]

    def get_token_usage(self, reset: bool = False) -> int:
        """í† í° ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        usage = self.total_token_usage
        if reset:
            self.total_token_usage = 0
        return usage


class EmbeddingService:
    """
    í†µí•© ìž„ë² ë”© ì„œë¹„ìŠ¤

    configì—ì„œ ì§€ì •í•œ providerì— ë”°ë¼ ì ì ˆí•œ ìž„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    AIì™€ Ingestion ì–‘ìª½ì—ì„œ ì´ í´ëž˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¼ê´€ì„±ì„ ë³´ìž¥í•©ë‹ˆë‹¤.

    âš ï¸ ì¤‘ìš”: ë°˜ë“œì‹œ ë™ì¼í•œ providerë¥¼ ì‚¬ìš©í•´ì•¼ ë²¡í„° ê²€ìƒ‰ì´ ì •í™•í•©ë‹ˆë‹¤!
    """

    _instance: Optional["EmbeddingService"] = None
    _embedder: Optional[BaseEmbedder] = None

    def __new__(cls, provider: str = None, **kwargs):
        """ì‹±ê¸€í†¤ íŒ¨í„´ (ë™ì¼ providerì¼ ê²½ìš°)"""
        target_provider = provider or EMBEDDING_CONFIG["provider"]

        if cls._instance is None or cls._instance._provider != target_provider:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False

        return cls._instance

    def __init__(
        self,
        provider: Literal["huggingface", "openai"] = None,
        validate_dimension: bool = True,
        **kwargs,
    ):
        if self._initialized:
            return

        self._provider = provider or EMBEDDING_CONFIG["provider"]

        logger.info(f"ðŸš€ Initializing EmbeddingService with provider: {self._provider}")

        # [ì•ˆì „ìž¥ì¹˜] ì°¨ì› ë¶ˆì¼ì¹˜ ì¡°ê¸° ê°ì§€
        if validate_dimension:
            try:
                from .config import validate_embedding_dimension_compatibility
                validate_embedding_dimension_compatibility()
            except Exception as e:
                logger.error(f"Dimension validation failed: {e}")
                raise

        if self._provider == "huggingface":
            self._embedder = HuggingFaceEmbedder(**kwargs)
        elif self._provider == "openai":
            self._embedder = OpenAIEmbedder(**kwargs)
        else:
            raise ValueError(
                f"Unsupported embedding provider: {self._provider}. "
                "Supported: 'huggingface', 'openai'"
            )

        self._initialized = True

        # ë¡œë“œëœ ëª¨ë¸ ì°¨ì›ê³¼ ì„¤ì • ì°¨ì› í™•ì¸
        actual_dim = self._embedder.get_dimension()
        expected_dim = EMBEDDING_CONFIG["dimension"]
        if actual_dim != expected_dim:
            raise RuntimeError(
                f"Model dimension mismatch: loaded model has {actual_dim}D, "
                f"but config expects {expected_dim}D"
            )

    @property
    def provider(self) -> str:
        """í˜„ìž¬ í”„ë¡œë°”ì´ë”"""
        return self._provider

    @property
    def dimension(self) -> int:
        """ìž„ë² ë”© ì°¨ì›"""
        return self._embedder.get_dimension()

    def embed_text(self, text: str) -> List[float]:
        """
        ë‹¨ì¼ í…ìŠ¤íŠ¸ ìž„ë² ë”©

        Args:
            text: ìž„ë² ë”©í•  í…ìŠ¤íŠ¸

        Returns:
            List[float]: ìž„ë² ë”© ë²¡í„°
        """
        return self._embedder.embed_text(text)

    def embed_texts(self, texts: List[str]) -> List[List[float]]:
        """
        ë°°ì¹˜ í…ìŠ¤íŠ¸ ìž„ë² ë”©

        Args:
            texts: ìž„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            List[List[float]]: ìž„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
        """
        return self._embedder.embed_texts(texts)

    def embed_to_numpy(self, texts: Union[str, List[str]]) -> np.ndarray:
        """
        NumPy ë°°ì—´ë¡œ ìž„ë² ë”© ë°˜í™˜

        Args:
            texts: ë‹¨ì¼ í…ìŠ¤íŠ¸ ë˜ëŠ” í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            np.ndarray: ìž„ë² ë”© ë°°ì—´ (1D ë˜ëŠ” 2D)
        """
        if isinstance(texts, str):
            return np.array(self.embed_text(texts))
        return np.array(self.embed_texts(texts))


# =============================================================================
# íŽ¸ì˜ í•¨ìˆ˜
# =============================================================================

def get_embedding_service(provider: str = None) -> EmbeddingService:
    """
    ìž„ë² ë”© ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (íŽ¸ì˜ í•¨ìˆ˜)

    Args:
        provider: 'huggingface' ë˜ëŠ” 'openai' (Noneì´ë©´ configì—ì„œ ê²°ì •)

    Returns:
        EmbeddingService: ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
    """
    return EmbeddingService(provider=provider)


def embed_text(text: str, provider: str = None) -> List[float]:
    """
    ë‹¨ì¼ í…ìŠ¤íŠ¸ ìž„ë² ë”© (íŽ¸ì˜ í•¨ìˆ˜)

    Args:
        text: ìž„ë² ë”©í•  í…ìŠ¤íŠ¸
        provider: ìž„ë² ë”© í”„ë¡œë°”ì´ë”

    Returns:
        List[float]: ìž„ë² ë”© ë²¡í„°
    """
    service = get_embedding_service(provider)
    return service.embed_text(text)


def embed_texts(texts: List[str], provider: str = None) -> List[List[float]]:
    """
    ë°°ì¹˜ í…ìŠ¤íŠ¸ ìž„ë² ë”© (íŽ¸ì˜ í•¨ìˆ˜)

    Args:
        texts: ìž„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        provider: ìž„ë² ë”© í”„ë¡œë°”ì´ë”

    Returns:
        List[List[float]]: ìž„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
    """
    service = get_embedding_service(provider)
    return service.embed_texts(texts)


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    print("Testing EmbeddingService...")
    print(f"Provider: {EMBEDDING_CONFIG['provider']}")
    print(f"Dimension: {EMBEDDING_CONFIG['dimension']}")

    service = EmbeddingService()

    test_texts = [
        "ì‚¼ì„±ì „ìž 2024ë…„ ë§¤ì¶œ í˜„í™©",
        "SKí•˜ì´ë‹‰ìŠ¤ ë°˜ë„ì²´ ì‚¬ì—… ë¶„ì„",
    ]

    print(f"\nEmbedding {len(test_texts)} texts...")
    embeddings = service.embed_texts(test_texts)

    for i, (text, emb) in enumerate(zip(test_texts, embeddings)):
        print(f"  [{i+1}] '{text[:30]}...' -> [{len(emb)}D] {emb[:3]}...")

    print(f"\nâœ… EmbeddingService test passed!")
    print(f"   Provider: {service.provider}")
    print(f"   Dimension: {service.dimension}")

